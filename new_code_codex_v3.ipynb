{"cells": [{"cell_type": "code", "metadata": {}, "source": ["import ee\n", "\n", "import geemap\n", "\n", "import pandas as pd\n", "\n", "import numpy as np\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "import seaborn as sns\n", "\n", "import os\n", "\n", "from datetime import datetime\n", "\n", "import warnings\n", "\n", "warnings.filterwarnings('ignore')\n", "\n", "\n", "\n", "ee.Authenticate()\n", "\n", "\n", "\n", "ee.Initialize()\n", "\n", "\n", "\n", "# ==================== 1. \u914d\u7f6e\u53c2\u6570 ====================\n", "\n", "# \u65f6\u95f4\u8303\u56f4 - \u6269\u5c55\u52307-9\u6708\n", "\n", "START_DATE = '2022-07-01'\n", "\n", "END_DATE = '2022-10-01'\n", "\n", "\n", "\n", "# \u7a7a\u95f4\u8303\u56f4\n", "\n", "BOUNDARY_PATH = '/Users/hanxu/geemap/bdy.shp'\n", "\n", "bdy = geemap.shp_to_ee(BOUNDARY_PATH)\n", "\n", "\n", "\n", "# \u7edf\u4e00\u7684\u76ee\u6807\u6295\u5f71\u548c\u5206\u8fa8\u7387 - \u4f7f\u752830m\u4ee5\u9002\u914d\u6240\u6709\u6570\u636e\u6e90\n", "\n", "TARGET_SCALE = 30  # 30\u7c73\u5206\u8fa8\u7387\n", "\n", "TARGET_CRS = 'EPSG:4326'  # WGS84\n", "\n", "\n", "\n", "# \u8f93\u51fa\u8def\u5f84\n", "\n", "OUTPUT_DIR = '/Users/hanxu/geemap/out_plots'\n", "\n", "os.makedirs(OUTPUT_DIR, exist_ok=True)\n", "\n", "OUTPUT_CSV = 'multi_source_features_2022_07_09.csv'\n", "\n", "SOIL_DATA_PATH = '/Users/hanxu/geemap/material/soil/soil_2022_08.csv'\n", "\n", "\n", "\n", "print(\"=\"*60)\n", "\n", "print(\"\u591a\u6e90\u9065\u611f\u6570\u636e\u878d\u5408\u7cfb\u7edf - \u76d0\u6e0d\u5316\u76d1\u6d4b\")\n", "\n", "print(\"=\"*60)\n", "\n", "print(f\"\u65f6\u95f4\u8303\u56f4: {START_DATE} \u81f3 {END_DATE}\")\n", "\n", "print(f\"\u7a7a\u95f4\u5206\u8fa8\u7387: {TARGET_SCALE}\u7c73\")\n", "\n", "print(f\"\u8f93\u51fa\u76ee\u5f55: {OUTPUT_DIR}\")\n", "\n", "\n", "\n", "# ==================== 2. Landsat 8 \u5904\u7406\uff08\u5305\u542b\u70ed\u7ea2\u5916\u6ce2\u6bb5\uff09====================\n", "\n", "\n", "\n", "def process_landsat8_comprehensive(start_date, end_date, boundary):\n", "\n", "    \"\"\"\n", "\n", "    \u7efc\u5408\u5904\u7406Landsat 8\u6570\u636e\uff0c\u5305\u542b\u6240\u6709\u76f8\u5173\u6307\u6570\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\u5904\u7406Landsat 8\u6570\u636e...\")\n", "\n", "    \n", "\n", "    def apply_scale_factors(image):\n", "\n", "        \"\"\"\u5e94\u7528\u7f29\u653e\u56e0\u5b50\"\"\"\n", "\n", "        optical = image.select('SR_B.*').multiply(0.0000275).add(-0.2)\n", "\n", "        thermal = image.select('ST_B10').multiply(0.00341802).add(149.0)\n", "\n", "        return image.addBands(optical, None, True).addBands(thermal, None, True)\n", "\n", "    \n", "\n", "    def mask_l8_clouds(image):\n", "\n", "        \"\"\"\u4e91\u63a9\u819c\"\"\"\n", "\n", "        qa = image.select('QA_PIXEL')\n", "\n", "        mask = qa.bitwiseAnd(1 << 3).eq(0).And(qa.bitwiseAnd(1 << 4).eq(0))\n", "\n", "        return image.updateMask(mask)\n", "\n", "    \n", "\n", "    def calculate_comprehensive_indices(image):\n", "\n", "        \"\"\"\u8ba1\u7b97\u5168\u9762\u7684\u5149\u8c31\u6307\u6570\"\"\"\n", "\n", "        # \u57fa\u7840\u6ce2\u6bb5\n", "\n", "        b2 = image.select('SR_B2')  # Blue\n", "\n", "        b3 = image.select('SR_B3')  # Green\n", "\n", "        b4 = image.select('SR_B4')  # Red\n", "\n", "        b5 = image.select('SR_B5')  # NIR\n", "\n", "        b6 = image.select('SR_B6')  # SWIR1\n", "\n", "        b7 = image.select('SR_B7')  # SWIR2\n", "\n", "        b10 = image.select('ST_B10') # Thermal\n", "\n", "        \n", "\n", "        # \u690d\u88ab\u6307\u6570\n", "\n", "        ndvi = b5.subtract(b4).divide(b5.add(b4)).rename('NDVI')\n", "\n", "        evi = b5.subtract(b4).divide(b5.add(b4.multiply(6)).subtract(b2.multiply(7.5)).add(1)).multiply(2.5).rename('EVI')\n", "\n", "        savi = b5.subtract(b4).divide(b5.add(b4).add(0.5)).multiply(1.5).rename('SAVI')\n", "\n", "        msavi = (b5.multiply(2).add(1).subtract(\n", "\n", "            (b5.multiply(2).add(1)).pow(2).subtract(b5.subtract(b4).multiply(8))\n", "\n", "            .sqrt())).divide(2).rename('MSAVI')\n", "\n", "        \n", "\n", "        # \u6c34\u4f53\u6307\u6570\n", "\n", "        ndwi = b3.subtract(b5).divide(b3.add(b5)).rename('NDWI')\n", "\n", "        mndwi = b3.subtract(b6).divide(b3.add(b6)).rename('MNDWI')\n", "\n", "        \n", "\n", "        # \u76d0\u5ea6\u6307\u6570\uff08\u591a\u79cd\uff09\n", "\n", "        si1 = b2.multiply(b4).sqrt().rename('SI1')\n", "\n", "        si2 = b2.pow(2).add(b3.pow(2)).add(b4.pow(2)).sqrt().rename('SI2')\n", "\n", "        si3 = b3.pow(2).add(b4.pow(2)).sqrt().rename('SI3')\n", "\n", "        si4 = b2.subtract(b3).pow(2).add(b3.subtract(b4).pow(2)).sqrt().rename('SI4')\n", "\n", "        \n", "\n", "        # \u76d0\u5ea6\u76f8\u5173\u7684\u5f52\u4e00\u5316\u6307\u6570\n", "\n", "        s1 = b2.divide(b4).rename('S1')\n", "\n", "        s2 = b2.subtract(b4).divide(b2.add(b4)).rename('S2')\n", "\n", "        s3 = b3.multiply(b4).divide(b2).rename('S3')\n", "\n", "        s5 = b2.multiply(b4).divide(b3).rename('S5')\n", "\n", "        s6 = b4.multiply(b5).divide(b3).rename('S6')\n", "\n", "        \n", "\n", "        # SWIR\u76f8\u5173\u6307\u6570\n", "\n", "        ndsi = b6.subtract(b5).divide(b6.add(b5)).rename('NDSI')\n", "\n", "        si_msi = b6.divide(b5).rename('SI_MSI')\n", "\n", "        \n", "\n", "        # \u571f\u58e4\u6307\u6570\n", "\n", "        bsi = ((b6.add(b4)).subtract(b5.add(b2))).divide((b6.add(b4)).add(b5.add(b2))).rename('BSI')\n", "\n", "        bi = b2.pow(2).add(b3.pow(2)).add(b4.pow(2)).sqrt().rename('BI')\n", "\n", "        \n", "\n", "        # \u70ed\u7ea2\u5916\u76f8\u5173\u6307\u6570\uff08\u91cd\u8981\uff01\uff09\n", "\n", "        # \u6e29\u5ea6-\u690d\u88ab\u6307\u6570\n", "\n", "        tvdi = b10.subtract(b10.reduce(ee.Reducer.min())).divide(\n", "\n", "            b10.reduce(ee.Reducer.max()).subtract(b10.reduce(ee.Reducer.min()))\n", "\n", "        ).rename('TVDI')\n", "\n", "        \n", "\n", "        # \u6e29\u5ea6\u4e0eNDVI\u7684\u4ea4\u4e92\n", "\n", "        temp_ndvi_ratio = b10.divide(ndvi.add(1)).rename('Temp_NDVI_ratio')\n", "\n", "        \n", "\n", "        return image.addBands([\n", "\n", "            ndvi, evi, savi, msavi, ndwi, mndwi,\n", "\n", "            si1, si2, si3, si4, s1, s2, s3, s5, s6,\n", "\n", "            ndsi, si_msi, bsi, bi, tvdi, temp_ndvi_ratio\n", "\n", "        ])\n", "\n", "    \n", "\n", "    # \u83b7\u53d6\u7b2c\u4e00\u4e2a\u5f71\u50cf\u7684\u6295\u5f71\n", "\n", "    first_image = ee.Image(ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n", "\n", "                          .filterBounds(boundary).first())\n", "\n", "    target_projection = first_image.select('SR_B2').projection()\n", "\n", "    \n", "\n", "    # \u52a0\u8f7d\u5e76\u5904\u7406Landsat 8\u96c6\u5408\n", "\n", "    l8_collection = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n", "\n", "                     .filterDate(start_date, end_date)\n", "\n", "                     .filterBounds(boundary)\n", "\n", "                     .map(apply_scale_factors)\n", "\n", "                     .map(mask_l8_clouds)\n", "\n", "                     .map(calculate_comprehensive_indices))\n", "\n", "    \n", "\n", "    print(f\"  \u627e\u5230 {l8_collection.size().getInfo()} \u5e45Landsat 8\u5f71\u50cf\")\n", "\n", "    \n", "\n", "    # \u521b\u5efa\u4e2d\u503c\u5408\u6210\n", "\n", "    l8_composite = l8_collection.median().clip(boundary)\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u6240\u6709\u76f8\u5173\u6ce2\u6bb5\n", "\n", "    l8_bands = [\n", "\n", "        # \u539f\u59cb\u6ce2\u6bb5\n", "\n", "        'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'ST_B10',\n", "\n", "        # \u6240\u6709\u8ba1\u7b97\u7684\u6307\u6570\n", "\n", "        'NDVI', 'EVI', 'SAVI', 'MSAVI', 'NDWI', 'MNDWI',\n", "\n", "        'SI1', 'SI2', 'SI3', 'SI4', 'S1', 'S2', 'S3', 'S5', 'S6',\n", "\n", "        'NDSI', 'SI_MSI', 'BSI', 'BI', 'TVDI', 'Temp_NDVI_ratio'\n", "\n", "    ]\n", "\n", "    \n", "\n", "    return l8_composite.select(l8_bands), target_projection\n", "\n", "\n", "\n", "# ==================== 3. Sentinel-1 \u5904\u7406\uff08\u4f18\u5316\u7684\u96f7\u8fbe\u6307\u6570\uff09====================\n", "\n", "\n", "\n", "def process_sentinel1_enhanced(start_date, end_date, boundary, target_projection, target_scale):\n", "\n", "    \"\"\"\n", "\n", "    \u589e\u5f3a\u7684Sentinel-1\u5904\u7406\uff0c\u5305\u542b\u66f4\u591a\u96f7\u8fbe\u6307\u6570\n", "\n", "    \u6ce8\u610f\uff1a\u5728GEE\u4e2d\uff0cVV\u548cVH\u7684\u5355\u4f4d\u662fdB\uff0c\u6240\u4ee5VV-VH\u66f4\u5408\u9002\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\u5904\u7406Sentinel-1\u6570\u636e...\")\n", "\n", "    \n", "\n", "    def preprocess_s1(image):\n", "\n", "        # \u83b7\u53d6\u6781\u5316\u6ce2\u6bb5\n", "\n", "        vv = image.select('VV')\n", "\n", "        vh = image.select('VH')\n", "\n", "        angle = image.select('angle')\n", "\n", "        \n", "\n", "        # \u5e94\u7528\u6539\u8fdb\u7684\u6ee4\u6ce2\uff08\u4f7f\u7528\u66f4\u5927\u7684\u6838\u4ee5\u51cf\u5c11\u6591\u70b9\u566a\u58f0\uff09\n", "\n", "        vv_filtered = vv.focal_median(radius=50, kernelType='circle', units='meters').rename('VV')\n", "\n", "        vh_filtered = vh.focal_median(radius=50, kernelType='circle', units='meters').rename('VH')\n", "\n", "        \n", "\n", "        # \u91cd\u8981\uff1a\u5728dB\u5c3a\u5ea6\u4e0b\uff0c\u4f7f\u7528\u51cf\u6cd5\u8ba1\u7b97\u4ea4\u53c9\u6781\u5316\u5dee\u5f02\n", "\n", "        # VV - VH \u5728dB\u5c3a\u5ea6\u7b49\u540c\u4e8e\u7ebf\u6027\u5c3a\u5ea6\u7684 VV/VH\n", "\n", "        cross_pol_diff = vv_filtered.subtract(vh_filtered).rename('VV_VH_diff')\n", "\n", "        \n", "\n", "        # \u5f52\u4e00\u5316\u5dee\u5f02\u6781\u5316\u6307\u6570\uff08\u8f6c\u6362\u5230\u7ebf\u6027\u5c3a\u5ea6\u8ba1\u7b97\uff09\n", "\n", "        vv_linear = ee.Image(10).pow(vv_filtered.divide(10))\n", "\n", "        vh_linear = ee.Image(10).pow(vh_filtered.divide(10))\n", "\n", "        \n", "\n", "        # \u96f7\u8fbe\u690d\u88ab\u6307\u6570 (RVI) - \u5728\u7ebf\u6027\u5c3a\u5ea6\u8ba1\u7b97\n", "\n", "        rvi = vh_linear.multiply(4).divide(vv_linear.add(vh_linear)).rename('RVI')\n", "\n", "        \n", "\n", "        # \u53cc\u6781\u5316SAR\u690d\u88ab\u6307\u6570 (DPSVI)\n", "\n", "        dpsvi = vv_linear.add(vh_linear).divide(vv_linear).rename('DPSVI')\n", "\n", "        \n", "\n", "        # \u6781\u5316\u6bd4\uff08\u5728dB\u5c3a\u5ea6\u5c31\u662f\u5dee\u503c\uff09\n", "\n", "        pol_ratio = cross_pol_diff.rename('Pol_Ratio')\n", "\n", "        \n", "\n", "        # \u96f7\u8fbe\u571f\u58e4\u6e7f\u5ea6\u6307\u6570\n", "\n", "        # \u4f7f\u7528VH\u4f5c\u4e3a\u571f\u58e4\u6e7f\u5ea6\u7684\u4ee3\u7406\uff08VH\u5bf9\u571f\u58e4\u6e7f\u5ea6\u66f4\u654f\u611f\uff09\n", "\n", "        soil_moisture_index = vh_filtered.multiply(-1).add(5).rename('SMI')  # \u7b80\u5355\u7684\u7ebf\u6027\u53d8\u6362\n", "\n", "        \n", "\n", "        # VV\u548cVH\u7684\u6807\u51c6\u5316\uff080-1\u8303\u56f4\uff09\n", "\n", "        vv_norm = vv_filtered.unitScale(-30, 0).rename('VV_norm')\n", "\n", "        vh_norm = vh_filtered.unitScale(-30, -5).rename('VH_norm')\n", "\n", "        \n", "\n", "        return image.addBands([\n", "\n", "            vv_filtered, vh_filtered, cross_pol_diff, \n", "\n", "            rvi, dpsvi, pol_ratio, soil_moisture_index,\n", "\n", "            vv_norm, vh_norm, angle\n", "\n", "        ])\n", "\n", "    \n", "\n", "    # \u52a0\u8f7dSentinel-1\u96c6\u5408\n", "\n", "    s1_collection = (ee.ImageCollection('COPERNICUS/S1_GRD')\n", "\n", "                     .filterDate(start_date, end_date)\n", "\n", "                     .filterBounds(boundary)\n", "\n", "                     .filter(ee.Filter.eq('instrumentMode', 'IW'))\n", "\n", "                     .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n", "\n", "                     .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n", "\n", "                     .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n", "\n", "                     .map(preprocess_s1))\n", "\n", "    \n", "\n", "    print(f\"  \u627e\u5230 {s1_collection.size().getInfo()} \u5e45Sentinel-1\u5f71\u50cf\")\n", "\n", "    \n", "\n", "    # \u521b\u5efa\u4e2d\u503c\u5408\u6210\n", "\n", "    s1_composite = s1_collection.median()\n", "\n", "    \n", "\n", "    # \u91cd\u6295\u5f71\u5230\u76ee\u6807\u6295\u5f71\u548c\u5206\u8fa8\u7387\n", "\n", "    s1_resampled = s1_composite.reproject(\n", "\n", "        crs=target_projection,\n", "\n", "        scale=target_scale\n", "\n", "    ).clip(boundary)\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u6700\u7ec8\u6ce2\u6bb5\n", "\n", "    s1_bands = [\n", "\n", "        'VV', 'VH', 'VV_VH_diff', 'RVI', 'DPSVI', \n", "\n", "        'Pol_Ratio', 'SMI', 'VV_norm', 'VH_norm', 'angle'\n", "\n", "    ]\n", "\n", "    \n", "\n", "    return s1_resampled.select(s1_bands)\n", "\n", "\n", "\n", "# ==================== 4. \u73af\u5883\u56e0\u5b50\u5904\u7406 ====================\n", "\n", "\n", "\n", "def calculate_TWI(boundary, target_projection, target_scale):\n", "\n", "    \"\"\"\u4f7f\u7528HydroSHEDS\u6570\u636e\u8ba1\u7b97TWI\"\"\"\n", "\n", "    print(\"\u4f7f\u7528HydroSHEDS\u6570\u636e\u8ba1\u7b97TWI...\")\n", "\n", "\n", "\n", "    # \u5bfc\u5165HydroSHEDS\u6570\u636e (Flow Accumulation)\n", "\n", "    fa = ee.Image(\"WWF/HydroSHEDS/15ACC\").clip(boundary).float()\n", "\n", "    dem = ee.Image(\"USGS/SRTMGL1_003\").clip(boundary)\n", "\n", "\n", "\n", "    # \u8ba1\u7b97\u5761\u5ea6\uff08\u5ea6\uff09\u5e76\u8f6c\u6362\u4e3a\u5f27\u5ea6\n", "\n", "    slope_rad = ee.Terrain.slope(dem).multiply(np.pi / 180).rename('slope_rad')\n", "\n", "\n", "\n", "    # \u8ba1\u7b97TWI\n", "\n", "    area = fa.multiply(900).rename('specific_catchment_area')  # \u6bcf\u50cf\u5143\u9762\u79ef\u7ea630m\u00d730m = 900 m\u00b2\n", "\n", "    TWI = (area.add(1)).log().subtract(slope_rad.tan().add(0.001).log()).rename('TWI')\n", "\n", "\n", "\n", "    # \u91cd\u6295\u5f71\u5230\u76ee\u6807\u6295\u5f71\u548c\u5206\u8fa8\u7387\n", "\n", "    TWI_resampled = TWI.reproject(crs=target_projection, scale=target_scale).clip(boundary)\n", "\n", "\n", "\n", "    # \u68c0\u67e5TWI\u7edf\u8ba1\u503c\n", "\n", "    twi_stats = TWI_resampled.reduceRegion(\n", "\n", "        reducer=ee.Reducer.minMax(),\n", "\n", "        geometry=boundary,\n", "\n", "        scale=target_scale,\n", "\n", "        bestEffort=True,\n", "\n", "        maxPixels=1e13\n", "\n", "    ).getInfo()\n", "\n", "\n", "\n", "    print(\"\u2705 TWI\u6ce2\u6bb5\u7edf\u8ba1\u503c\uff1a\", twi_stats)\n", "\n", "\n", "\n", "    if twi_stats['TWI_min'] is None or twi_stats['TWI_max'] is None:\n", "\n", "        raise ValueError(\"\u26a0\ufe0f TWI\u6ce2\u6bb5\u4ecd\u65e0\u6709\u6548\u6570\u636e\uff0c\u8bf7\u518d\u6b21\u68c0\u67e5\")\n", "\n", "    else:\n", "\n", "        print(f\"TWI \u6700\u5c0f\u503c: {twi_stats['TWI_min']:.4f}\")\n", "\n", "        print(f\"TWI \u6700\u5927\u503c: {twi_stats['TWI_max']:.4f}\")\n", "\n", "\n", "\n", "    return TWI_resampled\n", "\n", "\n", "\n", "def process_environmental_factors_enhanced(start_date, end_date, boundary, target_projection, target_scale):\n", "\n", "    \"\"\"\n", "\n", "    \u5b8c\u6574\u73af\u5883\u56e0\u5b50\u5904\u7406\u51fd\u6570\uff08\u4fee\u590d\u5e76\u4f7f\u7528HydroSHEDS\u8ba1\u7b97TWI\uff09\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\u5904\u7406\u73af\u5883\u56e0\u5b50\u6570\u636e...\")\n", "\n", "\n", "\n", "    env_features = ee.Image().select([])\n", "\n", "\n", "\n", "    # 4.1 MODIS ET (\u84b8\u6563\u53d1) - 8\u5929\u4ea7\u54c1\n", "\n", "    try:\n", "\n", "        et_collection = (ee.ImageCollection('MODIS/061/MOD16A2')\n", "\n", "                        .filterDate(start_date, end_date)\n", "\n", "                        .filterBounds(boundary))\n", "\n", "\n", "\n", "        if et_collection.size().gt(0):\n", "\n", "            et_mean = et_collection.select('ET').mean().rename('ET_mean')\n", "\n", "            et_max = et_collection.select('ET').max().rename('ET_max')\n", "\n", "            et_std = et_collection.select('ET').reduce(ee.Reducer.stdDev()).rename('ET_std')\n", "\n", "            pet_mean = et_collection.select('PET').mean().rename('PET_mean')\n", "\n", "\n", "\n", "            for band in [et_mean, et_max, et_std, pet_mean]:\n", "\n", "                band_resampled = band.reproject(crs=target_projection, scale=target_scale).clip(boundary)\n", "\n", "                env_features = env_features.addBands(band_resampled)\n", "\n", "\n", "\n", "            print(\"  \u2713 ET\u6570\u636e\u5904\u7406\u5b8c\u6210\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 ET\u6570\u636e\u5904\u7406\u5931\u8d25: {e}\")\n", "\n", "\n", "\n", "    # 4.2 CHIRPS\u964d\u6c34\u6570\u636e\n", "\n", "    try:\n", "\n", "        precip_collection = (ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n", "\n", "                            .filterDate(start_date, end_date)\n", "\n", "                            .filterBounds(boundary))\n", "\n", "\n", "\n", "        if precip_collection.size().gt(0):\n", "\n", "            precip_sum = precip_collection.sum().rename('precip_total')\n", "\n", "            precip_mean = precip_collection.mean().rename('precip_mean')\n", "\n", "            precip_max = precip_collection.max().rename('precip_max_daily')\n", "\n", "\n", "\n", "            precip_days = precip_collection.map(lambda img: img.gt(1)).sum()\n", "\n", "            total_days = precip_collection.size()\n", "\n", "            precip_frequency = precip_days.divide(total_days).rename('precip_frequency')\n", "\n", "\n", "\n", "            for band in [precip_sum, precip_mean, precip_max, precip_frequency]:\n", "\n", "                band_resampled = band.reproject(crs=target_projection, scale=target_scale).clip(boundary)\n", "\n", "                env_features = env_features.addBands(band_resampled)\n", "\n", "\n", "\n", "            print(\"  \u2713 \u964d\u6c34\u6570\u636e\u5904\u7406\u5b8c\u6210\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 \u964d\u6c34\u6570\u636e\u5904\u7406\u5931\u8d25: {e}\")\n", "\n", "\n", "\n", "    # 4.3 \u5730\u4e0b\u6c34\u6570\u636e\n", "\n", "    try:\n", "\n", "        gw_image = ee.Image('projects/ee-hanxu0223/assets/Soil_Raster')\n", "\n", "        gw_band = gw_image.select([0]).rename('soil_type')\n", "\n", "        gw_resampled = gw_band.reproject(crs=target_projection, scale=target_scale).clip(boundary)\n", "\n", "        env_features = env_features.addBands(gw_resampled)\n", "\n", "        print(\"  \u2713 \u5730\u4e0b\u6c34\u6570\u636e\u5904\u7406\u5b8c\u6210\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 \u5730\u4e0b\u6c34\u6570\u636e\u5904\u7406\u5931\u8d25: {e}\")\n", "\n", "\n", "\n", "    # 4.4 \u5730\u5f62\u6570\u636e\uff08\u660e\u786e\u4f7f\u7528HydroSHEDS\u6570\u636e\u8ba1\u7b97TWI\uff09\n", "\n", "    try:\n", "\n", "        dem = ee.Image('USGS/SRTMGL1_003').clip(boundary)\n", "\n", "        elevation = dem.select('elevation').rename('elevation')\n", "\n", "        slope = ee.Terrain.slope(elevation).rename('slope')\n", "\n", "        aspect = ee.Terrain.aspect(elevation).rename('aspect')\n", "\n", "\n", "\n", "        # \u660e\u786e\u8c03\u7528calculate_TWI\u51fd\u6570\n", "\n", "        TWI_resampled = calculate_TWI(boundary, target_projection, target_scale)\n", "\n", "\n", "\n", "        hillshade = ee.Terrain.hillshade(elevation).rename('hillshade')\n", "\n", "\n", "\n", "        # \u91cd\u6295\u5f71\u6240\u6709\u5730\u5f62\u6570\u636e\uff08\u5305\u542bTWI\uff09\n", "\n", "        for band in [elevation, slope, aspect, TWI_resampled, hillshade]:\n", "\n", "            band_resampled = band.reproject(crs=target_projection, scale=target_scale).clip(boundary)\n", "\n", "            env_features = env_features.addBands(band_resampled)\n", "\n", "\n", "\n", "        print(\"  \u2713 \u5730\u5f62\u6570\u636e\u5904\u7406\u5b8c\u6210\uff08\u542b\u6709\u6548TWI\uff09\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 \u5730\u5f62\u6570\u636e\u5904\u7406\u5931\u8d25: {e}\")\n", "\n", "\n", "\n", "    return env_features\n", "\n", "\n", "\n", "# ==================== 5. \u6570\u636e\u878d\u5408\u548c\u63a9\u819c ====================\n", "\n", "\n", "\n", "def create_comprehensive_landcover_mask(boundary):\n", "\n", "    \"\"\"\u521b\u5efa\u7efc\u5408\u7684\u571f\u5730\u8986\u76d6\u63a9\u819c\"\"\"\n", "\n", "    print(\"\\n\u521b\u5efa\u571f\u5730\u8986\u76d6\u63a9\u819c...\")\n", "\n", "    \n", "\n", "    # ESA WorldCover\n", "\n", "    esa_worldcover = ee.ImageCollection(\"ESA/WorldCover/v200\").first().clip(boundary)\n", "\n", "    landcover_map = esa_worldcover.select('Map')\n", "\n", "    \n", "\n", "    # \u6392\u9664\uff1a\u5efa\u7b51\uff0850\uff09\u3001\u6c34\u4f53\uff0880\uff09\u3001\u6c38\u4e45\u51b0\u96ea\uff0870\uff09\n", "\n", "    valid_mask = (landcover_map.neq(50)\n", "\n", "                  .And(landcover_map.neq(80))\n", "\n", "                  .And(landcover_map.neq(70)))\n", "\n", "    \n", "\n", "    print(\"  \u2713 \u63a9\u819c\u521b\u5efa\u5b8c\u6210\uff08\u6392\u9664\u5efa\u7b51\u3001\u6c34\u4f53\u3001\u51b0\u96ea\uff09\")\n", "\n", "    \n", "\n", "    return valid_mask\n", "\n", "\n", "\n", "# ==================== 6. \u591a\u6e90\u6570\u636e\u878d\u5408\u4e3b\u51fd\u6570 ====================\n", "\n", "\n", "\n", "def fuse_multisource_data_comprehensive():\n", "\n", "    \"\"\"\n", "\n", "    \u7efc\u5408\u878d\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\uff08\u4fee\u590d\u540e\uff0c\u8fd4\u56detarget_projection\uff09\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u5f00\u59cb\u591a\u6e90\u6570\u636e\u878d\u5408...\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # \u521b\u5efa\u5730\u56fe\u5bf9\u8c61\n", "\n", "    Map = geemap.Map()\n", "\n", "    Map.add_basemap('HYBRID')\n", "\n", "    Map.centerObject(bdy, 10)\n", "\n", "    \n", "\n", "    # 1. \u5904\u7406Landsat 8\uff08\u83b7\u53d6\u6295\u5f71\u4fe1\u606f\uff09\n", "\n", "    l8_features, target_projection = process_landsat8_comprehensive(START_DATE, END_DATE, bdy)\n", "\n", "    \n", "\n", "    # 2. \u5904\u7406Sentinel-1\n", "\n", "    s1_features = process_sentinel1_enhanced(START_DATE, END_DATE, bdy, target_projection, TARGET_SCALE)\n", "\n", "    \n", "\n", "    # 3. \u5904\u7406\u73af\u5883\u56e0\u5b50\n", "\n", "    env_features = process_environmental_factors_enhanced(START_DATE, END_DATE, bdy, target_projection, TARGET_SCALE)\n", "\n", "    \n", "\n", "    # 4. \u5408\u5e76\u6240\u6709\u7279\u5f81\n", "\n", "    print(\"\\n\u5408\u5e76\u591a\u6e90\u7279\u5f81...\")\n", "\n", "    all_features = ee.Image.cat([l8_features, s1_features, env_features])\n", "\n", "    \n", "\n", "    # 5. \u5e94\u7528\u571f\u5730\u8986\u76d6\u63a9\u819c\n", "\n", "    landcover_mask = create_comprehensive_landcover_mask(bdy)\n", "\n", "    all_features_masked = all_features.updateMask(landcover_mask)\n", "\n", "    \n", "\n", "    # 6. \u83b7\u53d6\u5e76\u6253\u5370\u6240\u6709\u6ce2\u6bb5\u540d\u79f0\n", "\n", "    band_names = all_features_masked.bandNames().getInfo()\n", "\n", "    print(f\"\\n\u878d\u5408\u540e\u7684\u7279\u5f81\u6ce2\u6bb5\u603b\u6570: {len(band_names)}\")\n", "\n", "    \n", "\n", "    # \u6309\u6570\u636e\u6e90\u5206\u7c7b\u663e\u793a\n", "\n", "    print(\"\\nLandsat 8 \u7279\u5f81:\")\n", "\n", "    l8_bands = [b for b in band_names if b.startswith(('SR_B', 'ST_B', 'ND', 'SI', 'S', 'EVI', 'SAVI', 'MSAVI', 'BSI', 'BI', 'TVDI', 'Temp'))]\n", "\n", "    for i, band in enumerate(l8_bands):\n", "\n", "        print(f\"  {i+1}. {band}\")\n", "\n", "    \n", "\n", "    print(f\"\\nSentinel-1 \u7279\u5f81:\")\n", "\n", "    s1_bands = [b for b in band_names if b.startswith(('VV', 'VH', 'RVI', 'DPSVI', 'Pol', 'SMI', 'angle'))]\n", "\n", "    for i, band in enumerate(s1_bands):\n", "\n", "        print(f\"  {i+1}. {band}\")\n", "\n", "    \n", "\n", "    print(f\"\\n\u73af\u5883\u56e0\u5b50\u7279\u5f81:\")\n", "\n", "    env_bands = [b for b in band_names if b.startswith(('ET', 'PET', 'precip', 'groundwater', 'elevation', 'slope', 'aspect', 'TWI', 'hillshade'))]\n", "\n", "    for i, band in enumerate(env_bands):\n", "\n", "        print(f\"  {i+1}. {band}\")\n", "\n", "    \n", "\n", "    # 7. \u53ef\u89c6\u5316\n", "\n", "    Map.addLayer(bdy, {}, 'Study Area')\n", "\n", "    \n", "\n", "    # Landsat 8 \u771f\u5f69\u8272\n", "\n", "    Map.addLayer(l8_features.select(['SR_B4', 'SR_B3', 'SR_B2']), \n", "\n", "                 {'min': 0, 'max': 0.3}, 'Landsat 8 True Color')\n", "\n", "    \n", "\n", "    # Landsat 8 \u5047\u5f69\u8272\uff08\u7a81\u51fa\u690d\u88ab\uff09\n", "\n", "    Map.addLayer(l8_features.select(['SR_B5', 'SR_B4', 'SR_B3']), \n", "\n", "                 {'min': 0, 'max': 0.4}, 'Landsat 8 False Color')\n", "\n", "    \n", "\n", "    # \u70ed\u7ea2\u5916\u6ce2\u6bb5\n", "\n", "    Map.addLayer(l8_features.select('ST_B10'), \n", "\n", "                 {'min': 290, 'max': 320, 'palette': ['blue', 'white', 'red']}, \n", "\n", "                 'Land Surface Temperature')\n", "\n", "    \n", "\n", "    # Sentinel-1 VV\n", "\n", "    Map.addLayer(s1_features.select('VV'), \n", "\n", "                 {'min': -25, 'max': 0}, 'Sentinel-1 VV')\n", "\n", "    \n", "\n", "    # VV-VH\u5dee\u5f02\uff08\u91cd\u8981\u7684\u76d0\u5ea6\u6307\u6807\uff09\n", "\n", "    Map.addLayer(s1_features.select('VV_VH_diff'), \n", "\n", "                 {'min': 0, 'max': 15, 'palette': ['blue', 'yellow', 'red']}, \n", "\n", "                 'VV-VH Difference')\n", "\n", "    \n", "\n", "    # ET\n", "\n", "    Map.addLayer(env_features.select('ET_mean'), \n", "\n", "                 {'min': 0, 'max': 300, 'palette': ['red', 'yellow', 'green', 'blue']}, \n", "\n", "                 'Evapotranspiration', False)\n", "\n", "    \n", "\n", "    return all_features_masked, Map, band_names, target_projection\n", "\n", "\n", "\n", "# ==================== 7. \u6837\u672c\u6570\u636e\u63d0\u53d6\u548c\u7279\u5f81\u5de5\u7a0b ====================\n", "\n", "\n", "\n", "def extract_and_engineer_features(feature_image, sample_points_path, band_names, projection, scale):\n", "\n", "    \"\"\"\n", "\n", "    \u63d0\u53d6\u6837\u672c\u7279\u5f81\u5e76\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\uff08\u4fee\u590d\u540e\u7248\u672c\uff0c\u589e\u52a0projection\u548cscale\u53c2\u6570\uff09\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\u7279\u5f81\u63d0\u53d6\u548c\u5de5\u7a0b...\")\n", "\n", "    \n", "\n", "    sample_fc = geemap.csv_to_ee(sample_points_path)\n", "\n", "    print(f\"\u52a0\u8f7d\u4e86 {sample_fc.size().getInfo()} \u4e2a\u6837\u672c\u70b9\")\n", "\n", "\n", "\n", "    # \u5173\u952e\u70b9\uff1a\u663e\u5f0f\u6307\u5b9aprojection\u548cscale\n", "\n", "    sampled_data = feature_image.sampleRegions(\n", "\n", "        collection=sample_fc,\n", "\n", "        properties=['salinity'],\n", "\n", "        scale=scale,\n", "\n", "        projection=projection,\n", "\n", "        geometries=True,\n", "\n", "        tileScale=4\n", "\n", "    )\n", "\n", "\n", "\n", "    sample_size = sampled_data.size().getInfo()\n", "\n", "    print(f\"\u63d0\u53d6\u5230\u7684\u6709\u6548\u6837\u672c\u6570: {sample_size}\")\n", "\n", "    \n", "\n", "    if sample_size == 0:\n", "\n", "        raise ValueError(\"\u26a0\ufe0f \u63d0\u53d6\u7279\u5f81\u503c\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\u6837\u672c\u70b9\u4e0e\u5f71\u50cf\u662f\u5426\u5339\u914d\u3002\")\n", "\n", "\n", "\n", "    # \u63d0\u53d6\u6570\u636e\u5230\u672c\u5730\n", "\n", "    features_list = sampled_data.getInfo()['features']\n", "\n", "    data_records = []\n", "\n", "    for feature in features_list:\n", "\n", "        properties = feature['properties']\n", "\n", "        coords = feature['geometry']['coordinates']\n", "\n", "        properties['longitude'] = coords[0]\n", "\n", "        properties['latitude'] = coords[1]\n", "\n", "        data_records.append(properties)\n", "\n", "\n", "\n", "    df = pd.DataFrame(data_records)\n", "\n", "\n", "\n", "    if df.empty:\n", "\n", "        raise ValueError(\"\u26a0\ufe0f \u63d0\u53d6\u7684\u6570\u636e\u6846\u4e3a\u7a7a\uff0c\u8bf7\u91cd\u65b0\u68c0\u67e5\u6570\u636e\u63d0\u53d6\u8fc7\u7a0b\u3002\")\n", "\n", "\n", "\n", "    # \u7279\u5f81\u5de5\u7a0b\uff08\u793a\u4f8b\uff09\n", "\n", "    if 'NDVI' in df.columns and 'ST_B10' in df.columns:\n", "\n", "        df['Temp_NDVI_interaction'] = df['ST_B10'] * df['NDVI']\n", "\n", "\n", "\n", "    # \u4fdd\u5b58\u6570\u636e\n", "\n", "    df.to_csv(OUTPUT_CSV, index=False)\n", "\n", "    print(f\"\u2713 \u7279\u5f81\u6570\u636e\u6210\u529f\u4fdd\u5b58\u81f3: {OUTPUT_CSV}\")\n", "\n", "    print(f\"  \u603b\u6837\u672c\u6570: {len(df)}\")\n", "\n", "    print(f\"  \u603b\u7279\u5f81\u6570: {len(df.columns)-3}\")\n", "\n", "\n", "\n", "    return df\n", "\n", "\n", "\n", "# ==================== 8. \u7efc\u5408\u7279\u5f81\u5206\u6790\u548c\u53ef\u89c6\u5316 ====================\n", "\n", "\n", "\n", "def comprehensive_feature_analysis(df, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u5168\u9762\u7684\u7279\u5f81\u5206\u6790\u548c\u53ef\u89c6\u5316\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7efc\u5408\u7279\u5f81\u5206\u6790...\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # \u51c6\u5907\u6570\u636e\n", "\n", "    feature_cols = [col for col in df.columns if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "    X = df[feature_cols].fillna(0)\n", "\n", "    y = df['salinity']\n", "\n", "    \n", "\n", "    # 1. \u7279\u5f81\u5206\u5e03\u5206\u6790\n", "\n", "    print(\"\\n1. \u751f\u6210\u7279\u5f81\u5206\u5e03\u56fe...\")\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u5173\u952e\u7279\u5f81\u8fdb\u884c\u53ef\u89c6\u5316\n", "\n", "    key_features = ['NDVI', 'ST_B10', 'VV_VH_diff', 'SI1', 'ET_mean', 'groundwater_depth']\n", "\n", "    available_key_features = [f for f in key_features if f in df.columns]\n", "\n", "    \n", "\n", "    if len(available_key_features) >= 6:\n", "\n", "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n", "\n", "        axes = axes.ravel()\n", "\n", "        \n", "\n", "        for idx, feature in enumerate(available_key_features[:6]):\n", "\n", "            ax = axes[idx]\n", "\n", "            \n", "\n", "            # \u6563\u70b9\u56fe\n", "\n", "            scatter = ax.scatter(df[feature], y, alpha=0.6, c=y, cmap='RdYlBu_r')\n", "\n", "            ax.set_xlabel(feature)\n", "\n", "            ax.set_ylabel('Salinity')\n", "\n", "            ax.set_title(f'{feature} vs Salinity')\n", "\n", "            \n", "\n", "            # \u6dfb\u52a0\u8d8b\u52bf\u7ebf\n", "\n", "            z = np.polyfit(df[feature].fillna(0), y, 1)\n", "\n", "            p = np.poly1d(z)\n", "\n", "            ax.plot(df[feature].sort_values(), p(df[feature].sort_values()), \"r--\", alpha=0.8)\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'feature_vs_salinity_scatter.png'), dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "    \n", "\n", "    # 2. \u76f8\u5173\u6027\u5206\u6790\n", "\n", "    print(\"\\n2. \u8ba1\u7b97\u7279\u5f81\u76f8\u5173\u6027...\")\n", "\n", "    \n", "\n", "    # \u8ba1\u7b97\u4e0e\u76d0\u5ea6\u7684\u76f8\u5173\u6027\n", "\n", "    correlations = X.corrwith(y).sort_values(ascending=False)\n", "\n", "    \n", "\n", "    # \u7ed8\u5236\u76f8\u5173\u6027\u6761\u5f62\u56fe\n", "\n", "    plt.figure(figsize=(10, 12))\n", "\n", "    top_n = 30  # \u663e\u793a\u524d30\u4e2a\u7279\u5f81\n", "\n", "    correlations.head(top_n).plot(kind='barh')\n", "\n", "    plt.xlabel('Correlation with Salinity')\n", "\n", "    plt.title('Top Features Correlated with Salinity')\n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_correlation_with_salinity.png'), dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 3. \u7279\u5f81\u95f4\u76f8\u5173\u6027\u70ed\u529b\u56fe\n", "\n", "    print(\"\\n3. \u751f\u6210\u7279\u5f81\u76f8\u5173\u6027\u70ed\u529b\u56fe...\")\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u9ad8\u76f8\u5173\u7279\u5f81\u7684\u5b50\u96c6\n", "\n", "    high_corr_features = correlations.abs().nlargest(20).index.tolist()\n", "\n", "    \n", "\n", "    plt.figure(figsize=(12, 10))\n", "\n", "    corr_matrix = df[high_corr_features + ['salinity']].corr()\n", "\n", "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n", "\n", "    \n", "\n", "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f',\n", "\n", "                cmap='coolwarm', center=0, square=True, linewidths=0.5,\n", "\n", "                cbar_kws={\"shrink\": 0.8})\n", "\n", "    plt.title('Feature Correlation Heatmap (Top 20 Features)', fontsize=14)\n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_correlation_heatmap.png'), dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 4. \u6570\u636e\u6e90\u8d21\u732e\u5206\u6790\n", "\n", "    print(\"\\n4. \u5206\u6790\u4e0d\u540c\u6570\u636e\u6e90\u7684\u8d21\u732e...\")\n", "\n", "    \n", "\n", "    # \u6309\u6570\u636e\u6e90\u5206\u7c7b\u7279\u5f81\n", "\n", "    source_mapping = {\n", "\n", "        'Landsat': ['SR_B', 'ST_B', 'NDVI', 'EVI', 'SAVI', 'MSAVI', 'NDWI', 'MNDWI', \n", "\n", "                   'SI1', 'SI2', 'SI3', 'SI4', 'S1', 'S2', 'S3', 'S5', 'S6', \n", "\n", "                   'NDSI', 'SI_MSI', 'BSI', 'BI', 'TVDI', 'Temp'],\n", "\n", "        'Sentinel-1': ['VV', 'VH', 'RVI', 'DPSVI', 'Pol', 'SMI', 'angle'],\n", "\n", "        'Environmental': ['ET', 'PET', 'precip', 'groundwater', 'elevation', 'slope', \n", "\n", "                         'aspect', 'TWI', 'hillshade'],\n", "\n", "        'Interaction': ['interaction', 'ratio', 'diff', 'Balance', 'Stress', 'Wetness']\n", "\n", "    }\n", "\n", "    \n", "\n", "    source_corr = {}\n", "\n", "    for source, keywords in source_mapping.items():\n", "\n", "        source_features = [f for f in feature_cols if any(k in f for k in keywords)]\n", "\n", "        if source_features:\n", "\n", "            source_corr[source] = X[source_features].corrwith(y).abs().mean()\n", "\n", "    \n", "\n", "    # \u7ed8\u5236\u6570\u636e\u6e90\u8d21\u732e\u56fe\n", "\n", "    plt.figure(figsize=(10, 6))\n", "\n", "    sources = list(source_corr.keys())\n", "\n", "    values = list(source_corr.values())\n", "\n", "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n", "\n", "    \n", "\n", "    bars = plt.bar(sources, values, color=colors[:len(sources)])\n", "\n", "    plt.xlabel('Data Source')\n", "\n", "    plt.ylabel('Average Absolute Correlation with Salinity')\n", "\n", "    plt.title('Contribution of Different Data Sources')\n", "\n", "    \n", "\n", "    # \u6dfb\u52a0\u6570\u503c\u6807\u7b7e\n", "\n", "    for bar, value in zip(bars, values):\n", "\n", "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n", "\n", "                f'{value:.3f}', ha='center', va='bottom')\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'data_source_contribution.png'), dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 5. \u7a7a\u95f4\u5206\u5e03\u5206\u6790\n", "\n", "    if 'longitude' in df.columns and 'latitude' in df.columns:\n", "\n", "        print(\"\\n5. \u751f\u6210\u7a7a\u95f4\u5206\u5e03\u56fe...\")\n", "\n", "        \n", "\n", "        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n", "\n", "        \n", "\n", "        # \u76d0\u5ea6\u7a7a\u95f4\u5206\u5e03\n", "\n", "        scatter1 = axes[0, 0].scatter(df['longitude'], df['latitude'], \n", "\n", "                                     c=df['salinity'], s=50, cmap='RdYlBu_r',\n", "\n", "                                     edgecolors='black', linewidth=0.5)\n", "\n", "        axes[0, 0].set_title('Salinity Spatial Distribution')\n", "\n", "        axes[0, 0].set_xlabel('Longitude')\n", "\n", "        axes[0, 0].set_ylabel('Latitude')\n", "\n", "        plt.colorbar(scatter1, ax=axes[0, 0], label='Salinity')\n", "\n", "        \n", "\n", "        # NDVI\u7a7a\u95f4\u5206\u5e03\n", "\n", "        if 'NDVI' in df.columns:\n", "\n", "            scatter2 = axes[0, 1].scatter(df['longitude'], df['latitude'], \n", "\n", "                                         c=df['NDVI'], s=50, cmap='RdYlGn',\n", "\n", "                                         edgecolors='black', linewidth=0.5)\n", "\n", "            axes[0, 1].set_title('NDVI Spatial Distribution')\n", "\n", "            axes[0, 1].set_xlabel('Longitude')\n", "\n", "            axes[0, 1].set_ylabel('Latitude')\n", "\n", "            plt.colorbar(scatter2, ax=axes[0, 1], label='NDVI')\n", "\n", "        \n", "\n", "        # \u6e29\u5ea6\u7a7a\u95f4\u5206\u5e03\n", "\n", "        if 'ST_B10' in df.columns:\n", "\n", "            scatter3 = axes[1, 0].scatter(df['longitude'], df['latitude'], \n", "\n", "                                         c=df['ST_B10'], s=50, cmap='hot',\n", "\n", "                                         edgecolors='black', linewidth=0.5)\n", "\n", "            axes[1, 0].set_title('Land Surface Temperature Distribution')\n", "\n", "            axes[1, 0].set_xlabel('Longitude')\n", "\n", "            axes[1, 0].set_ylabel('Latitude')\n", "\n", "            plt.colorbar(scatter3, ax=axes[1, 0], label='Temperature (K)')\n", "\n", "        \n", "\n", "        # VV-VH\u5dee\u5f02\u7a7a\u95f4\u5206\u5e03\n", "\n", "        if 'VV_VH_diff' in df.columns:\n", "\n", "            scatter4 = axes[1, 1].scatter(df['longitude'], df['latitude'], \n", "\n", "                                         c=df['VV_VH_diff'], s=50, cmap='viridis',\n", "\n", "                                         edgecolors='black', linewidth=0.5)\n", "\n", "            axes[1, 1].set_title('VV-VH Difference Spatial Distribution')\n", "\n", "            axes[1, 1].set_xlabel('Longitude')\n", "\n", "            axes[1, 1].set_ylabel('Latitude')\n", "\n", "            plt.colorbar(scatter4, ax=axes[1, 1], label='VV-VH (dB)')\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'spatial_distribution_analysis.png'), dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "    \n", "\n", "    return correlations\n", "\n", "\n", "\n", "# ==================== 9. \u589e\u5f3a\u7684\u7279\u5f81\u9009\u62e9\u6d41\u7a0b ====================\n", "\n", "\n", "\n", "def enhanced_feature_selection_multisource(df, output_dir, min_features=15, max_features=30):\n", "\n", "    \"\"\"\n", "\n", "    \u9488\u5bf9\u591a\u6e90\u6570\u636e\u7684\u589e\u5f3a\u7279\u5f81\u9009\u62e9\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u589e\u5f3a\u7279\u5f81\u9009\u62e9\u6d41\u7a0b...\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    from sklearn.preprocessing import StandardScaler\n", "\n", "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n", "\n", "    from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n", "\n", "    from sklearn.linear_model import LassoCV, ElasticNetCV\n", "\n", "    from sklearn.model_selection import cross_val_score\n", "\n", "    import warnings\n", "\n", "    warnings.filterwarnings('ignore')\n", "\n", "    \n", "\n", "    # \u51c6\u5907\u7279\u5f81\u548c\u76ee\u6807\n", "\n", "    feature_cols = [col for col in df.columns if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "    X = df[feature_cols].fillna(0)\n", "\n", "    y = df['salinity']\n", "\n", "    \n", "\n", "    # \u6807\u51c6\u5316\n", "\n", "    scaler = StandardScaler()\n", "\n", "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols)\n", "\n", "    \n", "\n", "    print(f\"\u7279\u5f81\u603b\u6570: {len(feature_cols)}\")\n", "\n", "    print(f\"\u6837\u672c\u603b\u6570: {len(X)}\")\n", "\n", "    \n", "\n", "    # 1. \u591a\u65b9\u6cd5\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\n", "\n", "    print(\"\\n1. \u591a\u65b9\u6cd5\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30...\")\n", "\n", "    \n", "\n", "    # Random Forest\n", "\n", "    rf = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n", "\n", "    rf.fit(X_scaled, y)\n", "\n", "    rf_importance = pd.Series(rf.feature_importances_, index=feature_cols, name='RF_importance')\n", "\n", "    \n", "\n", "    # Gradient Boosting\n", "\n", "    gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n", "\n", "    gb.fit(X_scaled, y)\n", "\n", "    gb_importance = pd.Series(gb.feature_importances_, index=feature_cols, name='GB_importance')\n", "\n", "    \n", "\n", "    # F-statistic\n", "\n", "    f_selector = SelectKBest(score_func=f_regression, k='all')\n", "\n", "    f_selector.fit(X_scaled, y)\n", "\n", "    f_importance = pd.Series(f_selector.scores_, index=feature_cols, name='F_score')\n", "\n", "    \n", "\n", "    # Mutual Information\n", "\n", "    mi_scores = mutual_info_regression(X_scaled, y, random_state=42, n_neighbors=5)\n", "\n", "    mi_importance = pd.Series(mi_scores, index=feature_cols, name='MI_score')\n", "\n", "    \n", "\n", "    # LASSO\n", "\n", "    lasso = LassoCV(cv=5, random_state=42, max_iter=1000)\n", "\n", "    lasso.fit(X_scaled, y)\n", "\n", "    lasso_importance = pd.Series(np.abs(lasso.coef_), index=feature_cols, name='LASSO_coef')\n", "\n", "    \n", "\n", "    # ElasticNet\n", "\n", "    elastic = ElasticNetCV(cv=5, random_state=42, max_iter=1000)\n", "\n", "    elastic.fit(X_scaled, y)\n", "\n", "    elastic_importance = pd.Series(np.abs(elastic.coef_), index=feature_cols, name='ElasticNet_coef')\n", "\n", "    \n", "\n", "    # Correlation\n", "\n", "    corr_importance = X_scaled.corrwith(y).abs()\n", "\n", "    corr_importance.name = 'Correlation'\n", "\n", "    \n", "\n", "    # \u5408\u5e76\u6240\u6709\u91cd\u8981\u6027\u6307\u6807\n", "\n", "    importance_df = pd.concat([\n", "\n", "        rf_importance, gb_importance, f_importance, mi_importance,\n", "\n", "        lasso_importance, elastic_importance, corr_importance\n", "\n", "    ], axis=1)\n", "\n", "    \n", "\n", "    # \u6807\u51c6\u5316\n", "\n", "    for col in importance_df.columns:\n", "\n", "        if importance_df[col].max() > 0:\n", "\n", "            importance_df[col] = importance_df[col] / importance_df[col].max()\n", "\n", "    \n", "\n", "    # \u8ba1\u7b97\u52a0\u6743\u7efc\u5408\u5f97\u5206\n", "\n", "    weights = {\n", "\n", "        'RF_importance': 0.20,\n", "\n", "        'GB_importance': 0.15,\n", "\n", "        'F_score': 0.15,\n", "\n", "        'MI_score': 0.15,\n", "\n", "        'LASSO_coef': 0.10,\n", "\n", "        'ElasticNet_coef': 0.10,\n", "\n", "        'Correlation': 0.15\n", "\n", "    }\n", "\n", "    \n", "\n", "    importance_df['Weighted_score'] = sum(importance_df[col] * weight \n", "\n", "                                         for col, weight in weights.items())\n", "\n", "    \n", "\n", "    # 2. \u6570\u636e\u6e90\u5e73\u8861\u9009\u62e9\n", "\n", "    print(\"\\n2. \u57fa\u4e8e\u6570\u636e\u6e90\u7684\u5e73\u8861\u9009\u62e9...\")\n", "\n", "    \n", "\n", "    # \u8bc6\u522b\u7279\u5f81\u6765\u6e90\n", "\n", "    def identify_source(feature_name):\n", "\n", "        if any(x in feature_name for x in ['SR_B', 'ST_B', 'NDVI', 'EVI', 'SAVI', 'MSAVI', \n", "\n", "                                           'NDWI', 'MNDWI', 'SI', 'S1', 'S2', 'S3', 'S5', 'S6',\n", "\n", "                                           'NDSI', 'BSI', 'BI', 'TVDI', 'Temp']):\n", "\n", "            return 'Landsat'\n", "\n", "        elif any(x in feature_name for x in ['VV', 'VH', 'RVI', 'DPSVI', 'Pol', 'SMI', 'angle']):\n", "\n", "            return 'Sentinel-1'\n", "\n", "        elif any(x in feature_name for x in ['ET', 'PET', 'precip', 'groundwater', \n", "\n", "                                             'elevation', 'slope', 'aspect', 'TWI', 'hillshade']):\n", "\n", "            return 'Environmental'\n", "\n", "        else:\n", "\n", "            return 'Interaction'\n", "\n", "    \n", "\n", "    importance_df['Source'] = importance_df.index.map(identify_source)\n", "\n", "    \n", "\n", "    # \u6bcf\u4e2a\u6570\u636e\u6e90\u7684\u6700\u5c0f\u7279\u5f81\u6570\n", "\n", "    min_per_source = {\n", "\n", "        'Landsat': 5,\n", "\n", "        'Sentinel-1': 3,\n", "\n", "        'Environmental': 3,\n", "\n", "        'Interaction': 2\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u7279\u5f81\n", "\n", "    selected_features = []\n", "\n", "    for source, min_count in min_per_source.items():\n", "\n", "        source_features = importance_df[importance_df['Source'] == source].nlargest(\n", "\n", "            min_count, 'Weighted_score'\n", "\n", "        ).index.tolist()\n", "\n", "        selected_features.extend(source_features)\n", "\n", "        print(f\"  {source}: \u9009\u62e9\u4e86 {len(source_features)} \u4e2a\u7279\u5f81\")\n", "\n", "    \n", "\n", "    # \u8865\u5145\u9ad8\u5206\u7279\u5f81\n", "\n", "    remaining_budget = max_features - len(selected_features)\n", "\n", "    remaining_features = importance_df[~importance_df.index.isin(selected_features)].nlargest(\n", "\n", "        remaining_budget, 'Weighted_score'\n", "\n", "    ).index.tolist()\n", "\n", "    selected_features.extend(remaining_features)\n", "\n", "    \n", "\n", "    # 3. \u76f8\u5173\u6027\u4f18\u5316\n", "\n", "    print(\"\\n3. \u4f18\u5316\u7279\u5f81\u76f8\u5173\u6027...\")\n", "\n", "    \n", "\n", "    def optimize_by_correlation(features, X, threshold=0.95):\n", "\n", "        corr_matrix = X[features].corr().abs()\n", "\n", "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n", "\n", "        \n", "\n", "        to_drop = []\n", "\n", "        for column in upper.columns:\n", "\n", "            if column in to_drop:\n", "\n", "                continue\n", "\n", "            correlated = upper.index[upper[column] > threshold].tolist()\n", "\n", "            if correlated:\n", "\n", "                all_corr_features = [column] + correlated\n", "\n", "                importance_scores = importance_df.loc[all_corr_features, 'Weighted_score']\n", "\n", "                keep_feature = importance_scores.idxmax()\n", "\n", "                drop_features = [f for f in all_corr_features if f != keep_feature]\n", "\n", "                to_drop.extend(drop_features)\n", "\n", "        \n", "\n", "        return [f for f in features if f not in to_drop]\n", "\n", "    \n", "\n", "    selected_features = optimize_by_correlation(selected_features, X_scaled, threshold=0.95)\n", "\n", "    print(f\"  \u76f8\u5173\u6027\u4f18\u5316\u540e\u5269\u4f59\u7279\u5f81\u6570: {len(selected_features)}\")\n", "\n", "    \n", "\n", "    # 4. \u4ea4\u53c9\u9a8c\u8bc1\u9009\u62e9\u6700\u4f18\u7279\u5f81\u6570\n", "\n", "    print(\"\\n4. \u4ea4\u53c9\u9a8c\u8bc1\u786e\u5b9a\u6700\u4f18\u7279\u5f81\u6570...\")\n", "\n", "    \n", "\n", "    # \u6309\u91cd\u8981\u6027\u6392\u5e8f\n", "\n", "    selected_features_sorted = importance_df.loc[selected_features].sort_values(\n", "\n", "        'Weighted_score', ascending=False\n", "\n", "    ).index.tolist()\n", "\n", "    \n", "\n", "    cv_scores = []\n", "\n", "    feature_range = range(min_features, min(len(selected_features_sorted) + 1, max_features + 1))\n", "\n", "    \n", "\n", "    for n_features in feature_range:\n", "\n", "        features_subset = selected_features_sorted[:n_features]\n", "\n", "        X_subset = X_scaled[features_subset]\n", "\n", "        \n", "\n", "        # \u4f7f\u7528\u8f83\u5feb\u7684\u6a21\u578b\u8fdb\u884cCV\n", "\n", "        rf_cv = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)\n", "\n", "        scores = cross_val_score(rf_cv, X_subset, y, cv=5, scoring='r2')\n", "\n", "        \n", "\n", "        cv_scores.append({\n", "\n", "            'n_features': n_features,\n", "\n", "            'mean_r2': scores.mean(),\n", "\n", "            'std_r2': scores.std()\n", "\n", "        })\n", "\n", "        \n", "\n", "        print(f\"  {n_features} \u7279\u5f81: R\u00b2 = {scores.mean():.4f} \u00b1 {scores.std():.4f}\")\n", "\n", "    \n", "\n", "    # \u627e\u5230\u6700\u4f18\u7279\u5f81\u6570\n", "\n", "    cv_scores_df = pd.DataFrame(cv_scores)\n", "\n", "    \n", "\n", "    # \u4f7f\u7528\"\u4e00\u4e2a\u6807\u51c6\u5dee\"\u89c4\u5219\n", "\n", "    best_idx = cv_scores_df['mean_r2'].idxmax()\n", "\n", "    best_score = cv_scores_df.loc[best_idx, 'mean_r2']\n", "\n", "    best_std = cv_scores_df.loc[best_idx, 'std_r2']\n", "\n", "    \n", "\n", "    # \u627e\u5230\u5728\u4e00\u4e2a\u6807\u51c6\u5dee\u5185\u7684\u6700\u5c11\u7279\u5f81\u6570\n", "\n", "    threshold = best_score - best_std\n", "\n", "    optimal_idx = cv_scores_df[cv_scores_df['mean_r2'] >= threshold]['n_features'].idxmin()\n", "\n", "    optimal_n = cv_scores_df.loc[optimal_idx, 'n_features']\n", "\n", "    \n", "\n", "    print(f\"\\n\u6700\u4f18\u7279\u5f81\u6570: {optimal_n}\")\n", "\n", "    print(f\"\u5bf9\u5e94R\u00b2: {cv_scores_df.loc[optimal_idx, 'mean_r2']:.4f}\")\n", "\n", "    \n", "\n", "    # \u6700\u7ec8\u7279\u5f81\u96c6\n", "\n", "    final_features = selected_features_sorted[:optimal_n]\n", "\n", "    \n", "\n", "    # 5. \u53ef\u89c6\u5316\u7ed3\u679c\n", "\n", "    print(\"\\n5. \u751f\u6210\u7279\u5f81\u9009\u62e9\u53ef\u89c6\u5316...\")\n", "\n", "    \n", "\n", "    # 5.1 \u7279\u5f81\u91cd\u8981\u6027\u70ed\u529b\u56fe\n", "\n", "    plt.figure(figsize=(14, 10))\n", "\n", "    importance_matrix = importance_df.loc[final_features, list(weights.keys())]\n", "\n", "    \n", "\n", "    sns.heatmap(importance_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n", "\n", "                cbar_kws={'label': 'Normalized Importance'})\n", "\n", "    plt.title('Multi-Method Feature Importance Heatmap', fontsize=14)\n", "\n", "    plt.xlabel('Method')\n", "\n", "    plt.ylabel('Feature')\n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_importance_heatmap.png'), dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 5.2 \u7279\u5f81\u9009\u62e9\u66f2\u7ebf\n", "\n", "    plt.figure(figsize=(10, 6))\n", "\n", "    plt.plot(cv_scores_df['n_features'], cv_scores_df['mean_r2'], 'b-', marker='o', label='Mean R\u00b2')\n", "\n", "    plt.fill_between(cv_scores_df['n_features'],\n", "\n", "                     cv_scores_df['mean_r2'] - cv_scores_df['std_r2'],\n", "\n", "                     cv_scores_df['mean_r2'] + cv_scores_df['std_r2'],\n", "\n", "                     alpha=0.2, color='blue', label='\u00b11 std')\n", "\n", "    plt.axvline(optimal_n, color='red', linestyle='--', label=f'Optimal ({optimal_n} features)')\n", "\n", "    plt.axhline(threshold, color='gray', linestyle=':', label='One std rule threshold')\n", "\n", "    plt.xlabel('Number of Features')\n", "\n", "    plt.ylabel('Cross-validation R\u00b2')\n", "\n", "    plt.title('Feature Selection Performance Curve')\n", "\n", "    plt.legend()\n", "\n", "    plt.grid(True, alpha=0.3)\n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_selection_curve.png'), dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 5.3 \u6570\u636e\u6e90\u5206\u5e03\u997c\u56fe\n", "\n", "    source_dist = importance_df.loc[final_features, 'Source'].value_counts()\n", "\n", "    \n", "\n", "    plt.figure(figsize=(8, 8))\n", "\n", "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n", "\n", "    plt.pie(source_dist.values, labels=source_dist.index, autopct='%1.1f%%',\n", "\n", "            colors=colors[:len(source_dist)], startangle=90)\n", "\n", "    plt.title('Distribution of Selected Features by Data Source')\n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_source_distribution.png'), dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 6. \u4fdd\u5b58\u7ed3\u679c\n", "\n", "    print(\"\\n6. \u4fdd\u5b58\u7279\u5f81\u9009\u62e9\u7ed3\u679c...\")\n", "\n", "    \n", "\n", "    # \u7279\u5f81\u91cd\u8981\u6027\u62a5\u544a\n", "\n", "    feature_report = importance_df.loc[final_features].copy()\n", "\n", "    feature_report = feature_report.sort_values('Weighted_score', ascending=False)\n", "\n", "    feature_report.to_csv(os.path.join(output_dir, 'feature_importance_report.csv'))\n", "\n", "    \n", "\n", "    # \u6253\u5370\u6700\u7ec8\u9009\u62e9\u7684\u7279\u5f81\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u6700\u7ec8\u9009\u62e9\u7684\u7279\u5f81:\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    for source in ['Landsat', 'Sentinel-1', 'Environmental', 'Interaction']:\n", "\n", "        source_final = feature_report[feature_report['Source'] == source]\n", "\n", "        if len(source_final) > 0:\n", "\n", "            print(f\"\\n{source} ({len(source_final)}\u4e2a):\")\n", "\n", "            for feat, score in source_final['Weighted_score'].items():\n", "\n", "                print(f\"  - {feat}: {score:.4f}\")\n", "\n", "    \n", "\n", "    return final_features, importance_df, cv_scores_df\n", "\n", "\n", "\n", "# ==================== 10. \u4e3b\u6267\u884c\u51fd\u6570 ====================\n", "\n", "\n", "\n", "def main():\n", "\n", "    \"\"\"\n", "\n", "    \u4e3b\u6267\u884c\u51fd\u6570\n", "\n", "    \"\"\"\n", "\n", "    try:\n", "\n", "        # 1. \u591a\u6e90\u6570\u636e\u878d\u5408\n", "\n", "        fused_image, map_obj, band_names, target_projection = fuse_multisource_data_comprehensive()\n", "\n", "        \n", "\n", "        # 2. \u7279\u5f81\u63d0\u53d6\u548c\u5de5\u7a0b\n", "\n", "        df = extract_and_engineer_features(\n", "\n", "            fused_image,\n", "\n", "            SOIL_DATA_PATH,\n", "\n", "            band_names,\n", "\n", "            projection=target_projection,  # \u73b0\u5728\u5df2\u5b9a\u4e49\n", "\n", "            scale=TARGET_SCALE\n", "\n", "        )\n", "\n", "\n", "\n", "        \n", "\n", "        # 3. \u7efc\u5408\u7279\u5f81\u5206\u6790\n", "\n", "        correlations = comprehensive_feature_analysis(df, OUTPUT_DIR)\n", "\n", "        \n", "\n", "        # 4. \u7279\u5f81\u9009\u62e9\n", "\n", "        final_features, importance_df, cv_scores = enhanced_feature_selection_multisource(\n", "\n", "            df, OUTPUT_DIR, min_features=15, max_features=35\n", "\n", "        )\n", "\n", "        \n", "\n", "        # 5. \u4fdd\u5b58\u5904\u7406\u540e\u7684\u6570\u636e\n", "\n", "        df_final = df[final_features + ['salinity', 'longitude', 'latitude']]\n", "\n", "        final_csv_path = os.path.join(OUTPUT_DIR, 'final_selected_features.csv')\n", "\n", "        df_final.to_csv(final_csv_path, index=False)\n", "\n", "        \n", "\n", "        print(\"\\n\" + \"=\"*60)\n", "\n", "        print(\"\u5904\u7406\u5b8c\u6210\uff01\")\n", "\n", "        print(\"=\"*60)\n", "\n", "        print(f\"\u2713 \u539f\u59cb\u7279\u5f81\u6570\u636e: {OUTPUT_CSV}\")\n", "\n", "        print(f\"\u2713 \u6700\u7ec8\u7279\u5f81\u6570\u636e: {final_csv_path}\")\n", "\n", "        print(f\"\u2713 \u6240\u6709\u56fe\u8868\u4fdd\u5b58\u5728: {OUTPUT_DIR}\")\n", "\n", "        print(f\"\u2713 \u6700\u7ec8\u7279\u5f81\u6570: {len(final_features)}\")\n", "\n", "        \n", "\n", "        return map_obj, df_final, final_features\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"\\n\u9519\u8bef: {e}\")\n", "\n", "        import traceback\n", "\n", "        traceback.print_exc()\n", "\n", "        return None, None, None\n", "\n", "\n", "\n", "# \u6267\u884c\n", "\n", "if __name__ == \"__main__\":\n", "\n", "    map_result, final_data, selected_features = main()\n", "\n", "    if map_result:\n", "\n", "        map_result  # \u663e\u793a\u5730\u56fe\n", "# ==================== \u589e\u5f3a\u7248\u76d0\u6e0d\u5316\u5efa\u6a21\u7cfb\u7edf v3.0 ====================\n", "\n", "# \u57fa\u4e8eGPT\u5efa\u8bae\u7684\u9488\u5bf9\u6027\u4f18\u5316\n", "\n", "\n", "\n", "import ee\n", "\n", "import geemap\n", "\n", "import pandas as pd\n", "\n", "import numpy as np\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "import seaborn as sns\n", "\n", "import os\n", "\n", "from datetime import datetime\n", "\n", "import warnings\n", "\n", "warnings.filterwarnings('ignore')\n", "\n", "\n", "\n", "from sklearn.preprocessing import PowerTransformer, StandardScaler\n", "\n", "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n", "\n", "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n", "\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "from sklearn.linear_model import Ridge, ElasticNet\n", "\n", "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n", "\n", "from sklearn.decomposition import PCA\n", "\n", "from sklearn.cluster import KMeans\n", "\n", "\n", "\n", "import xgboost as xgb\n", "\n", "from scipy import stats\n", "\n", "from scipy.stats import yeojohnson\n", "\n", "from statsmodels.stats.outliers_influence import variance_inflation_factor\n", "\n", "import shap\n", "\n", "\n", "\n", "# ==================== \u7b2c\u4e00\u6b65\uff1aYeo-Johnson\u76ee\u6807\u53d8\u91cf\u53d8\u6362\u4f18\u5316 ====================\n", "\n", "\n", "\n", "def advanced_target_transformation(y_original, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u9ad8\u7ea7\u76ee\u6807\u53d8\u91cf\u53d8\u6362 - \u91cd\u70b9\u4f7f\u7528Yeo-Johnson\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e00\u6b65\uff1a\u9ad8\u7ea7\u76ee\u6807\u53d8\u91cf\u53d8\u6362\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    y = y_original.copy()\n", "\n", "    \n", "\n", "    # 1. \u5f02\u5e38\u503c\u68c0\u6d4b\u548c\u5904\u7406\n", "\n", "    print(\"\\n1. \u5f02\u5e38\u503c\u68c0\u6d4b\u548c\u5904\u7406...\")\n", "\n", "    \n", "\n", "    # IQR\u65b9\u6cd5\u68c0\u6d4b\u5f02\u5e38\u503c\n", "\n", "    Q1 = y.quantile(0.25)\n", "\n", "    Q3 = y.quantile(0.75)\n", "\n", "    IQR = Q3 - Q1\n", "\n", "    lower_bound = Q1 - 1.5 * IQR\n", "\n", "    upper_bound = Q3 + 1.5 * IQR\n", "\n", "    \n", "\n", "    outliers_mask = (y < lower_bound) | (y > upper_bound)\n", "\n", "    outliers_count = outliers_mask.sum()\n", "\n", "    \n", "\n", "    print(f\"  \u68c0\u6d4b\u5230\u5f02\u5e38\u503c: {outliers_count} \u4e2a ({outliers_count/len(y)*100:.1f}%)\")\n", "\n", "    print(f\"  \u6b63\u5e38\u8303\u56f4: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n", "\n", "    \n", "\n", "    # \u8bb0\u5f55\u5f02\u5e38\u503c\n", "\n", "    if outliers_count > 0:\n", "\n", "        outlier_values = y[outliers_mask].values\n", "\n", "        print(f\"  \u5f02\u5e38\u503c: {outlier_values}\")\n", "\n", "        \n", "\n", "        # \u6e29\u548c\u5904\u7406\uff1acap\u5f02\u5e38\u503c\u800c\u4e0d\u662f\u5220\u9664\n", "\n", "        y_capped = y.copy()\n", "\n", "        y_capped[y < lower_bound] = lower_bound\n", "\n", "        y_capped[y > upper_bound] = upper_bound\n", "\n", "        \n", "\n", "        print(f\"  \u5df2\u5c06\u5f02\u5e38\u503c\u8c03\u6574\u5230\u8fb9\u754c\u503c\")\n", "\n", "    else:\n", "\n", "        y_capped = y.copy()\n", "\n", "    \n", "\n", "    # 2. \u591a\u79cd\u53d8\u6362\u65b9\u6cd5\u6bd4\u8f83\n", "\n", "    print(\"\\n2. \u6bd4\u8f83\u591a\u79cd\u53d8\u6362\u65b9\u6cd5...\")\n", "\n", "    \n", "\n", "    transformations = {}\n", "\n", "    \n", "\n", "    # \u539f\u59cb\u6570\u636e\n", "\n", "    transformations['original'] = y_capped\n", "\n", "    \n", "\n", "    # Log\u53d8\u6362 (\u6dfb\u52a0\u5c0f\u5e38\u6570\u907f\u514d0\u503c)\n", "\n", "    transformations['log'] = np.log1p(y_capped)\n", "\n", "    \n", "\n", "    # \u5e73\u65b9\u6839\u53d8\u6362\n", "\n", "    transformations['sqrt'] = np.sqrt(y_capped)\n", "\n", "    \n", "\n", "    # Box-Cox\u53d8\u6362\n", "\n", "    try:\n", "\n", "        from scipy.stats import boxcox\n", "\n", "        y_boxcox, lambda_bc = boxcox(y_capped + 1)  # +1\u907f\u514d0\u503c\n", "\n", "        transformations['boxcox'] = y_boxcox\n", "\n", "        print(f\"  Box-Cox \u03bb: {lambda_bc:.4f}\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  Box-Cox\u53d8\u6362\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # Yeo-Johnson\u53d8\u6362 (\u91cd\u70b9\u63a8\u8350)\n", "\n", "    try:\n", "\n", "        y_yj, lambda_yj = yeojohnson(y_capped)\n", "\n", "        transformations['yeo_johnson'] = y_yj\n", "\n", "        print(f\"  Yeo-Johnson \u03bb: {lambda_yj:.4f}\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  Yeo-Johnson\u53d8\u6362\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # PowerTransformer (Yeo-Johnson)\n", "\n", "    try:\n", "\n", "        pt = PowerTransformer(method='yeo-johnson', standardize=False)\n", "\n", "        y_pt = pt.fit_transform(y_capped.values.reshape(-1, 1)).flatten()\n", "\n", "        transformations['power_transformer'] = y_pt\n", "\n", "        print(f\"  PowerTransformer \u03bb: {pt.lambdas_[0]:.4f}\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  PowerTransformer\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # 3. \u8bc4\u4f30\u53d8\u6362\u6548\u679c\n", "\n", "    print(\"\\n3. \u8bc4\u4f30\u53d8\u6362\u6548\u679c...\")\n", "\n", "    \n", "\n", "    # \u8ba1\u7b97\u504f\u5ea6\u3001\u5cf0\u5ea6\u548c\u6b63\u6001\u6027\u68c0\u9a8c\n", "\n", "    evaluation_results = {}\n", "\n", "    \n", "\n", "    for name, transformed_y in transformations.items():\n", "\n", "        if transformed_y is not None:\n", "\n", "            # \u504f\u5ea6\n", "\n", "            skewness = stats.skew(transformed_y)\n", "\n", "            # \u5cf0\u5ea6\n", "\n", "            kurtosis = stats.kurtosis(transformed_y)\n", "\n", "            # Shapiro-Wilk\u6b63\u6001\u6027\u68c0\u9a8c\n", "\n", "            try:\n", "\n", "                if len(transformed_y) <= 5000:  # Shapiro-Wilk\u5bf9\u5927\u6837\u672c\u4e0d\u9002\u7528\n", "\n", "                    shapiro_stat, shapiro_p = stats.shapiro(transformed_y)\n", "\n", "                else:\n", "\n", "                    shapiro_stat, shapiro_p = np.nan, np.nan\n", "\n", "            except:\n", "\n", "                shapiro_stat, shapiro_p = np.nan, np.nan\n", "\n", "            \n", "\n", "            # Anderson-Darling\u6b63\u6001\u6027\u68c0\u9a8c\n", "\n", "            try:\n", "\n", "                ad_stat, ad_critical, ad_significance = stats.anderson(transformed_y, dist='norm')\n", "\n", "                ad_p = 1 - stats.norm.cdf(ad_stat)  # \u8fd1\u4f3cp\u503c\n", "\n", "            except:\n", "\n", "                ad_stat, ad_p = np.nan, np.nan\n", "\n", "            \n", "\n", "            evaluation_results[name] = {\n", "\n", "                'skewness': abs(skewness),\n", "\n", "                'kurtosis': abs(kurtosis),\n", "\n", "                'shapiro_p': shapiro_p,\n", "\n", "                'anderson_p': ad_p,\n", "\n", "                'composite_score': abs(skewness) + abs(kurtosis)/3  # \u590d\u5408\u5f97\u5206\n", "\n", "            }\n", "\n", "            \n", "\n", "            print(f\"  {name:15s}: \u504f\u5ea6={skewness:6.3f}, \u5cf0\u5ea6={kurtosis:6.3f}, Shapiro p={shapiro_p:.4f}\")\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u6700\u4f73\u53d8\u6362\n", "\n", "    best_transform = min(evaluation_results.keys(), \n", "\n", "                        key=lambda x: evaluation_results[x]['composite_score'])\n", "\n", "    \n", "\n", "    print(f\"\\n\u6700\u4f73\u53d8\u6362\u65b9\u6cd5: {best_transform}\")\n", "\n", "    print(f\"  \u590d\u5408\u5f97\u5206: {evaluation_results[best_transform]['composite_score']:.4f}\")\n", "\n", "    \n", "\n", "    best_y = transformations[best_transform]\n", "\n", "    \n", "\n", "    # 4. \u53ef\u89c6\u5316\u53d8\u6362\u6548\u679c\n", "\n", "    print(\"\\n4. \u53ef\u89c6\u5316\u53d8\u6362\u6548\u679c...\")\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n", "\n", "    axes = axes.ravel()\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u524d6\u4e2a\u53d8\u6362\u8fdb\u884c\u53ef\u89c6\u5316\n", "\n", "    transform_names = list(transformations.keys())[:6]\n", "\n", "    \n", "\n", "    for i, name in enumerate(transform_names):\n", "\n", "        if i < 6 and transformations[name] is not None:\n", "\n", "            transformed_data = transformations[name]\n", "\n", "            \n", "\n", "            # \u76f4\u65b9\u56fe\n", "\n", "            axes[i].hist(transformed_data, bins=25, alpha=0.7, edgecolor='black', density=True)\n", "\n", "            \n", "\n", "            # \u6dfb\u52a0\u6b63\u6001\u5206\u5e03\u66f2\u7ebf\n", "\n", "            mu, sigma = np.mean(transformed_data), np.std(transformed_data)\n", "\n", "            x = np.linspace(transformed_data.min(), transformed_data.max(), 100)\n", "\n", "            axes[i].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2, label='Normal fit')\n", "\n", "            \n", "\n", "            # \u6807\u9898\u548c\u7edf\u8ba1\u4fe1\u606f\n", "\n", "            skew_val = stats.skew(transformed_data)\n", "\n", "            axes[i].set_title(f'{name.replace(\"_\", \" \").title()}\\nSkew: {skew_val:.3f}')\n", "\n", "            axes[i].legend()\n", "\n", "            axes[i].grid(True, alpha=0.3)\n", "\n", "            \n", "\n", "            # \u9ad8\u4eae\u6700\u4f73\u53d8\u6362\n", "\n", "            if name == best_transform:\n", "\n", "                axes[i].patch.set_facecolor('lightgreen')\n", "\n", "                axes[i].patch.set_alpha(0.3)\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'target_transformation_comparison.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 5. \u4fdd\u5b58\u53d8\u6362\u53c2\u6570\n", "\n", "    transform_params = {\n", "\n", "        'method': best_transform,\n", "\n", "        'original_stats': {\n", "\n", "            'mean': float(y_original.mean()),\n", "\n", "            'std': float(y_original.std()),\n", "\n", "            'min': float(y_original.min()),\n", "\n", "            'max': float(y_original.max()),\n", "\n", "            'skew': float(stats.skew(y_original))\n", "\n", "        },\n", "\n", "        'transformed_stats': {\n", "\n", "            'mean': float(best_y.mean()),\n", "\n", "            'std': float(best_y.std()),\n", "\n", "            'min': float(best_y.min()),\n", "\n", "            'max': float(best_y.max()),\n", "\n", "            'skew': float(stats.skew(best_y))\n", "\n", "        }\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u7279\u5b9a\u53d8\u6362\u7684\u53c2\u6570\n", "\n", "    if best_transform == 'yeo_johnson':\n", "\n", "        transform_params['lambda'] = lambda_yj\n", "\n", "    elif best_transform == 'boxcox':\n", "\n", "        transform_params['lambda'] = lambda_bc\n", "\n", "    elif best_transform == 'power_transformer':\n", "\n", "        transform_params['transformer'] = pt\n", "\n", "    \n", "\n", "    print(f\"\\n\u2713 \u6700\u4f73\u53d8\u6362\u53c2\u6570\u5df2\u4fdd\u5b58\")\n", "\n", "    \n", "\n", "    return best_y, transform_params, outliers_mask\n", "\n", "\n", "\n", "# ==================== \u7b2c\u4e8c\u6b65\uff1a\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790 ====================\n", "\n", "\n", "\n", "def spatial_autocorrelation_analysis(df, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790 - \u5c40\u90e8Moran's I\u548c\u7a7a\u95f4\u6a21\u5f0f\u5206\u6790\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e8c\u6b65\uff1a\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    try:\n", "\n", "        import libpysal as lps\n", "\n", "        from esda.moran import Moran, Moran_Local\n", "\n", "        from splot.esda import moran_scatterplot, lisa_cluster\n", "\n", "    except ImportError:\n", "\n", "        print(\"\u26a0\ufe0f \u9700\u8981\u5b89\u88c5 pysal: pip install libpysal esda splot\")\n", "\n", "        return None, None\n", "\n", "    \n", "\n", "    # 1. \u51c6\u5907\u7a7a\u95f4\u6570\u636e\n", "\n", "    print(\"\\n1. \u51c6\u5907\u7a7a\u95f4\u6570\u636e...\")\n", "\n", "    \n", "\n", "    # \u83b7\u53d6\u5750\u6807\u548c\u76d0\u5ea6\u6570\u636e\n", "\n", "    coords = df[['longitude', 'latitude']].values\n", "\n", "    salinity = df['salinity'].values\n", "\n", "    \n", "\n", "    print(f\"  \u6837\u672c\u70b9\u6570\u91cf: {len(coords)}\")\n", "\n", "    print(f\"  \u5750\u6807\u8303\u56f4: \u7ecf\u5ea6[{coords[:, 0].min():.4f}, {coords[:, 0].max():.4f}]\")\n", "\n", "    print(f\"            \u7eac\u5ea6[{coords[:, 1].min():.4f}, {coords[:, 1].max():.4f}]\")\n", "\n", "    \n", "\n", "    # 2. \u6784\u5efa\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\n", "\n", "    print(\"\\n2. \u6784\u5efa\u7a7a\u95f4\u6743\u91cd\u77e9\u9635...\")\n", "\n", "    \n", "\n", "    # \u5c1d\u8bd5\u591a\u79cd\u6743\u91cd\u77e9\u9635\n", "\n", "    weight_methods = {}\n", "\n", "    \n", "\n", "    # KNN\u6743\u91cd\n", "\n", "    try:\n", "\n", "        w_knn = lps.weights.KNN.from_array(coords, k=min(8, len(coords)-1))\n", "\n", "        w_knn.transform = 'r'  # \u884c\u6807\u51c6\u5316\n", "\n", "        weight_methods['KNN'] = w_knn\n", "\n", "        print(f\"  \u2713 KNN\u6743\u91cd\u77e9\u9635 (k={min(8, len(coords)-1)})\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 KNN\u6743\u91cd\u77e9\u9635\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # \u8ddd\u79bb\u6743\u91cd\n", "\n", "    try:\n", "\n", "        # \u8ba1\u7b97\u5408\u9002\u7684\u8ddd\u79bb\u9608\u503c\n", "\n", "        from sklearn.neighbors import NearestNeighbors\n", "\n", "        nbrs = NearestNeighbors(n_neighbors=min(6, len(coords)-1)).fit(coords)\n", "\n", "        distances, indices = nbrs.kneighbors(coords)\n", "\n", "        max_distance = np.percentile(distances[:, -1], 90)  # 90%\u5206\u4f4d\u6570\u8ddd\u79bb\n", "\n", "        \n", "\n", "        w_dist = lps.weights.DistanceBand.from_array(coords, threshold=max_distance)\n", "\n", "        if w_dist.n_components == 1:  # \u786e\u4fdd\u8fde\u901a\u6027\n", "\n", "            w_dist.transform = 'r'\n", "\n", "            weight_methods['Distance'] = w_dist\n", "\n", "            print(f\"  \u2713 \u8ddd\u79bb\u6743\u91cd\u77e9\u9635 (\u9608\u503c={max_distance:.6f})\")\n", "\n", "        else:\n", "\n", "            print(f\"  \u2717 \u8ddd\u79bb\u6743\u91cd\u77e9\u9635\u4e0d\u8fde\u901a ({w_dist.n_components}\u4e2a\u5206\u91cf)\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 \u8ddd\u79bb\u6743\u91cd\u77e9\u9635\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    if not weight_methods:\n", "\n", "        print(\"\u26a0\ufe0f \u65e0\u6cd5\u6784\u5efa\u6709\u6548\u7684\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\")\n", "\n", "        return None, None\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u6700\u4f73\u6743\u91cd\u77e9\u9635\n", "\n", "    best_weight_name = list(weight_methods.keys())[0]\n", "\n", "    w = weight_methods[best_weight_name]\n", "\n", "    print(f\"  \u4f7f\u7528\u6743\u91cd\u77e9\u9635: {best_weight_name}\")\n", "\n", "    \n", "\n", "    # 3. \u5168\u5c40Moran's I\u5206\u6790\n", "\n", "    print(\"\\n3. \u5168\u5c40Moran's I\u5206\u6790...\")\n", "\n", "    \n", "\n", "    moran_global = Moran(salinity, w)\n", "\n", "    \n", "\n", "    print(f\"  Moran's I: {moran_global.I:.4f}\")\n", "\n", "    print(f\"  \u671f\u671b\u503c: {moran_global.EI:.4f}\")\n", "\n", "    print(f\"  \u65b9\u5dee: {moran_global.VI:.6f}\")\n", "\n", "    print(f\"  Z-score: {moran_global.z_norm:.4f}\")\n", "\n", "    print(f\"  p-value: {moran_global.p_norm:.6f}\")\n", "\n", "    \n", "\n", "    # \u5224\u65ad\u7a7a\u95f4\u81ea\u76f8\u5173\u6027\n", "\n", "    if moran_global.p_norm < 0.05:\n", "\n", "        if moran_global.I > moran_global.EI:\n", "\n", "            spatial_pattern = \"\u6b63\u5411\u7a7a\u95f4\u81ea\u76f8\u5173\uff08\u805a\u96c6\u6a21\u5f0f\uff09\"\n", "\n", "        else:\n", "\n", "            spatial_pattern = \"\u8d1f\u5411\u7a7a\u95f4\u81ea\u76f8\u5173\uff08\u68cb\u76d8\u6a21\u5f0f\uff09\"\n", "\n", "    else:\n", "\n", "        spatial_pattern = \"\u65e0\u663e\u8457\u7a7a\u95f4\u81ea\u76f8\u5173\uff08\u968f\u673a\u6a21\u5f0f\uff09\"\n", "\n", "    \n", "\n", "    print(f\"  \u7a7a\u95f4\u6a21\u5f0f: {spatial_pattern}\")\n", "\n", "    \n", "\n", "    # 4. \u5c40\u90e8Moran's I\u5206\u6790\n", "\n", "    print(\"\\n4. \u5c40\u90e8Moran's I\u5206\u6790...\")\n", "\n", "    \n", "\n", "    moran_local = Moran_Local(salinity, w)\n", "\n", "    \n", "\n", "    # \u5206\u7c7b\u5c40\u90e8\u7a7a\u95f4\u5173\u8054\n", "\n", "    lisa_categories = []\n", "\n", "    for i in range(len(salinity)):\n", "\n", "        if moran_local.p_sim[i] < 0.05:  # \u663e\u8457\u6027\u6c34\u5e73\n", "\n", "            if moran_local.Is[i] > 0:\n", "\n", "                if salinity[i] > np.mean(salinity):\n", "\n", "                    lisa_categories.append('HH')  # \u9ad8-\u9ad8\u805a\u96c6\n", "\n", "                else:\n", "\n", "                    lisa_categories.append('LL')  # \u4f4e-\u4f4e\u805a\u96c6\n", "\n", "            else:\n", "\n", "                if salinity[i] > np.mean(salinity):\n", "\n", "                    lisa_categories.append('HL')  # \u9ad8-\u4f4e\u5f02\u5e38\n", "\n", "                else:\n", "\n", "                    lisa_categories.append('LH')  # \u4f4e-\u9ad8\u5f02\u5e38\n", "\n", "        else:\n", "\n", "            lisa_categories.append('NS')  # \u4e0d\u663e\u8457\n", "\n", "    \n", "\n", "    lisa_counts = pd.Series(lisa_categories).value_counts()\n", "\n", "    print(f\"  LISA\u5206\u7c7b\u7edf\u8ba1:\")\n", "\n", "    for category, count in lisa_counts.items():\n", "\n", "        category_names = {\n", "\n", "            'HH': '\u9ad8-\u9ad8\u805a\u96c6', 'LL': '\u4f4e-\u4f4e\u805a\u96c6',\n", "\n", "            'HL': '\u9ad8-\u4f4e\u5f02\u5e38', 'LH': '\u4f4e-\u9ad8\u5f02\u5e38', 'NS': '\u4e0d\u663e\u8457'\n", "\n", "        }\n", "\n", "        print(f\"    {category_names.get(category, category)}: {count} \u4e2a\u70b9 ({count/len(salinity)*100:.1f}%)\")\n", "\n", "    \n", "\n", "    # 5. \u53ef\u89c6\u5316\u7a7a\u95f4\u81ea\u76f8\u5173\n", "\n", "    print(\"\\n5. \u53ef\u89c6\u5316\u7a7a\u95f4\u81ea\u76f8\u5173...\")\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n", "\n", "    \n", "\n", "    # 5.1 Moran\u6563\u70b9\u56fe\n", "\n", "    try:\n", "\n", "        moran_scatterplot(moran_global, ax=axes[0, 0])\n", "\n", "        axes[0, 0].set_title(f'Global Moran\\'s I = {moran_global.I:.4f}\\np-value = {moran_global.p_norm:.4f}')\n", "\n", "    except:\n", "\n", "        # \u624b\u52a8\u7ed8\u5236Moran\u6563\u70b9\u56fe\n", "\n", "        lag_salinity = lps.weights.lag_spatial(w, salinity)\n", "\n", "        axes[0, 0].scatter(salinity, lag_salinity, alpha=0.6)\n", "\n", "        axes[0, 0].axhline(np.mean(lag_salinity), color='r', linestyle='--')\n", "\n", "        axes[0, 0].axvline(np.mean(salinity), color='r', linestyle='--')\n", "\n", "        axes[0, 0].set_xlabel('Salinity')\n", "\n", "        axes[0, 0].set_ylabel('Spatial Lag of Salinity')\n", "\n", "        axes[0, 0].set_title(f'Moran Scatterplot (I={moran_global.I:.4f})')\n", "\n", "    \n", "\n", "    # 5.2 \u7a7a\u95f4\u5206\u5e03\u56fe\n", "\n", "    scatter = axes[0, 1].scatter(coords[:, 0], coords[:, 1], c=salinity, \n", "\n", "                                s=50, cmap='RdYlBu_r', edgecolors='black', linewidth=0.5)\n", "\n", "    axes[0, 1].set_xlabel('Longitude')\n", "\n", "    axes[0, 1].set_ylabel('Latitude')\n", "\n", "    axes[0, 1].set_title('Salinity Spatial Distribution')\n", "\n", "    plt.colorbar(scatter, ax=axes[0, 1], label='Salinity')\n", "\n", "    \n", "\n", "    # 5.3 LISA\u805a\u7c7b\u56fe\n", "\n", "    lisa_colors = {'HH': 'red', 'LL': 'blue', 'HL': 'lightpink', 'LH': 'lightblue', 'NS': 'lightgray'}\n", "\n", "    for category in lisa_colors:\n", "\n", "        mask = np.array(lisa_categories) == category\n", "\n", "        if mask.any():\n", "\n", "            axes[1, 0].scatter(coords[mask, 0], coords[mask, 1], \n", "\n", "                             c=lisa_colors[category], label=category, s=50, alpha=0.7)\n", "\n", "    \n", "\n", "    axes[1, 0].set_xlabel('Longitude')\n", "\n", "    axes[1, 0].set_ylabel('Latitude')\n", "\n", "    axes[1, 0].set_title('LISA Cluster Map')\n", "\n", "    axes[1, 0].legend()\n", "\n", "    \n", "\n", "    # 5.4 \u5c40\u90e8Moran's I\u503c\u5206\u5e03\n", "\n", "    axes[1, 1].hist(moran_local.Is, bins=20, alpha=0.7, edgecolor='black')\n", "\n", "    axes[1, 1].axvline(0, color='red', linestyle='--', label='Expected Value')\n", "\n", "    axes[1, 1].set_xlabel('Local Moran\\'s I')\n", "\n", "    axes[1, 1].set_ylabel('Frequency')\n", "\n", "    axes[1, 1].set_title('Distribution of Local Moran\\'s I')\n", "\n", "    axes[1, 1].legend()\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'spatial_autocorrelation_analysis.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 6. \u751f\u6210\u7a7a\u95f4\u7279\u5f81\n", "\n", "    print(\"\\n6. \u751f\u6210\u7a7a\u95f4\u7279\u5f81...\")\n", "\n", "    \n", "\n", "    spatial_features = pd.DataFrame({\n", "\n", "        'spatial_lag': lps.weights.lag_spatial(w, salinity),\n", "\n", "        'local_moran_i': moran_local.Is,\n", "\n", "        'local_moran_p': moran_local.p_sim,\n", "\n", "        'lisa_category': lisa_categories\n", "\n", "    })\n", "\n", "    \n", "\n", "    # \u6dfb\u52a0\u7a7a\u95f4\u5e73\u6ed1\u7279\u5f81\n", "\n", "    # k\u8fd1\u90bb\u5e73\u5747\n", "\n", "    for k in [3, 5, 8]:\n", "\n", "        if k < len(coords):\n", "\n", "            nbrs = NearestNeighbors(n_neighbors=k).fit(coords)\n", "\n", "            distances, indices = nbrs.kneighbors(coords)\n", "\n", "            \n", "\n", "            knn_mean = []\n", "\n", "            for i in range(len(coords)):\n", "\n", "                neighbor_salinity = salinity[indices[i][1:]]  # \u6392\u9664\u81ea\u8eab\n", "\n", "                knn_mean.append(np.mean(neighbor_salinity))\n", "\n", "            \n", "\n", "            spatial_features[f'knn_mean_{k}'] = knn_mean\n", "\n", "    \n", "\n", "    print(f\"  \u751f\u6210\u7a7a\u95f4\u7279\u5f81\u6570: {spatial_features.shape[1]}\")\n", "\n", "    \n", "\n", "    # 7. \u4fdd\u5b58\u7a7a\u95f4\u5206\u6790\u7ed3\u679c\n", "\n", "    spatial_results = {\n", "\n", "        'global_moran_i': moran_global.I,\n", "\n", "        'global_moran_p': moran_global.p_norm,\n", "\n", "        'global_moran_z': moran_global.z_norm,\n", "\n", "        'spatial_pattern': spatial_pattern,\n", "\n", "        'lisa_counts': lisa_counts.to_dict(),\n", "\n", "        'weight_method': best_weight_name\n", "\n", "    }\n", "\n", "    \n", "\n", "    print(f\"\\n\u2713 \u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790\u5b8c\u6210\")\n", "\n", "    \n", "\n", "    return spatial_features, spatial_results\n", "\n", "\n", "\n", "# ==================== \u7b2c\u4e09\u6b65\uff1aXGBoost\u8d85\u53c2\u6570\u4f18\u5316 ====================\n", "\n", "\n", "\n", "def optimized_xgboost_modeling(X, y, spatial_features, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    XGBoost\u8d85\u53c2\u6570\u4f18\u5316\u5efa\u6a21\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e09\u6b65\uff1aXGBoost\u8d85\u53c2\u6570\u4f18\u5316\u5efa\u6a21\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # 1. \u6570\u636e\u51c6\u5907\n", "\n", "    print(\"\\n1. \u6570\u636e\u51c6\u5907...\")\n", "\n", "    \n", "\n", "    # \u5408\u5e76\u7a7a\u95f4\u7279\u5f81\n", "\n", "    if spatial_features is not None:\n", "\n", "        # \u53ea\u9009\u62e9\u6570\u503c\u578b\u7a7a\u95f4\u7279\u5f81\n", "\n", "        numeric_spatial = spatial_features.select_dtypes(include=[np.number])\n", "\n", "        X_with_spatial = pd.concat([X, numeric_spatial], axis=1)\n", "\n", "        print(f\"  \u6dfb\u52a0\u7a7a\u95f4\u7279\u5f81: {numeric_spatial.shape[1]} \u4e2a\")\n", "\n", "    else:\n", "\n", "        X_with_spatial = X.copy()\n", "\n", "    \n", "\n", "    print(f\"  \u603b\u7279\u5f81\u6570: {X_with_spatial.shape[1]}\")\n", "\n", "    print(f\"  \u6837\u672c\u6570: {X_with_spatial.shape[0]}\")\n", "\n", "    \n", "\n", "    # \u5904\u7406\u7f3a\u5931\u503c\n", "\n", "    X_with_spatial = X_with_spatial.fillna(X_with_spatial.median())\n", "\n", "    \n", "\n", "    # 2. \u7279\u5f81\u9009\u62e9\u548cVIF\u8fc7\u6ee4\n", "\n", "    print(\"\\n2. \u7279\u5f81\u9009\u62e9\u548cVIF\u8fc7\u6ee4...\")\n", "\n", "    \n", "\n", "    # 2.1 \u57fa\u4e8e\u65b9\u5dee\u7684\u521d\u6b65\u7b5b\u9009\n", "\n", "    from sklearn.feature_selection import VarianceThreshold\n", "\n", "    var_selector = VarianceThreshold(threshold=0.01)\n", "\n", "    X_var_filtered = var_selector.fit_transform(X_with_spatial)\n", "\n", "    selected_features = X_with_spatial.columns[var_selector.get_support()].tolist()\n", "\n", "    \n", "\n", "    print(f\"  \u65b9\u5dee\u7b5b\u9009\u540e\u7279\u5f81\u6570: {len(selected_features)}\")\n", "\n", "    \n", "\n", "    # 2.2 \u76f8\u5173\u6027\u7b5b\u9009\n", "\n", "    X_corr = pd.DataFrame(X_var_filtered, columns=selected_features)\n", "\n", "    corr_matrix = X_corr.corr().abs()\n", "\n", "    \n", "\n", "    # \u627e\u5230\u9ad8\u76f8\u5173\u7279\u5f81\u5bf9\n", "\n", "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n", "\n", "    high_corr_pairs = []\n", "\n", "    \n", "\n", "    for column in upper_tri.columns:\n", "\n", "        high_corr_features = upper_tri.index[upper_tri[column] > 0.9].tolist()\n", "\n", "        if high_corr_features:\n", "\n", "            for feature in high_corr_features:\n", "\n", "                high_corr_pairs.append((column, feature, upper_tri.loc[feature, column]))\n", "\n", "    \n", "\n", "    print(f\"  \u53d1\u73b0\u9ad8\u76f8\u5173\u7279\u5f81\u5bf9: {len(high_corr_pairs)} \u5bf9\")\n", "\n", "    \n", "\n", "    # \u79fb\u9664\u9ad8\u76f8\u5173\u7279\u5f81\uff08\u4fdd\u7559\u4e0e\u76ee\u6807\u53d8\u91cf\u76f8\u5173\u6027\u66f4\u9ad8\u7684\uff09\n", "\n", "    features_to_remove = set()\n", "\n", "    target_corr = X_corr.corrwith(y).abs()\n", "\n", "    \n", "\n", "    for feat1, feat2, corr_val in high_corr_pairs:\n", "\n", "        if target_corr[feat1] > target_corr[feat2]:\n", "\n", "            features_to_remove.add(feat2)\n", "\n", "        else:\n", "\n", "            features_to_remove.add(feat1)\n", "\n", "    \n", "\n", "    final_features = [f for f in selected_features if f not in features_to_remove]\n", "\n", "    X_final = X_corr[final_features]\n", "\n", "    \n", "\n", "    print(f\"  \u76f8\u5173\u6027\u7b5b\u9009\u540e\u7279\u5f81\u6570: {len(final_features)}\")\n", "\n", "    \n", "\n", "    # 2.3 VIF\u8fc7\u6ee4\uff08\u66f4\u4e25\u683c\u7684\u9608\u503c\uff09\n", "\n", "    def calculate_vif(df):\n", "\n", "        vif_data = pd.DataFrame()\n", "\n", "        vif_data[\"Feature\"] = df.columns\n", "\n", "        vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) \n", "\n", "                          for i in range(len(df.columns))]\n", "\n", "        return vif_data\n", "\n", "    \n", "\n", "    # \u8fed\u4ee3\u79fb\u9664\u9ad8VIF\u7279\u5f81\n", "\n", "    max_vif_threshold = 5.0  # \u66f4\u4e25\u683c\u7684\u9608\u503c\n", "\n", "    iteration = 0\n", "\n", "    \n", "\n", "    while True:\n", "\n", "        try:\n", "\n", "            vif_df = calculate_vif(X_final)\n", "\n", "            max_vif = vif_df['VIF'].max()\n", "\n", "            \n", "\n", "            if max_vif <= max_vif_threshold or len(X_final.columns) <= 10:\n", "\n", "                break\n", "\n", "            \n", "\n", "            # \u79fb\u9664VIF\u6700\u9ad8\u7684\u7279\u5f81\n", "\n", "            worst_feature = vif_df.loc[vif_df['VIF'].idxmax(), 'Feature']\n", "\n", "            X_final = X_final.drop(columns=[worst_feature])\n", "\n", "            \n", "\n", "            iteration += 1\n", "\n", "            print(f\"    \u8fed\u4ee3{iteration}: \u79fb\u9664 {worst_feature} (VIF={max_vif:.2f})\")\n", "\n", "            \n", "\n", "            if iteration >= 20:  # \u9632\u6b62\u65e0\u9650\u5faa\u73af\n", "\n", "                break\n", "\n", "                \n", "\n", "        except Exception as e:\n", "\n", "            print(f\"    VIF\u8ba1\u7b97\u51fa\u9519: {e}\")\n", "\n", "            break\n", "\n", "    \n", "\n", "    print(f\"  VIF\u7b5b\u9009\u540e\u7279\u5f81\u6570: {len(X_final.columns)}\")\n", "\n", "    print(f\"  \u6700\u7ec8\u7279\u5f81: {list(X_final.columns)}\")\n", "\n", "    \n", "\n", "    # 3. \u6570\u636e\u5206\u5272\uff08\u5206\u5c42\u62bd\u6837\uff09\n", "\n", "    print(\"\\n3. \u6570\u636e\u5206\u5272...\")\n", "\n", "    \n", "\n", "    # \u57fa\u4e8e\u76ee\u6807\u53d8\u91cf\u5206\u4f4d\u6570\u8fdb\u884c\u5206\u5c42\n", "\n", "    y_bins = pd.qcut(y, q=4, labels=False, duplicates='drop')\n", "\n", "    \n", "\n", "    try:\n", "\n", "        X_train, X_test, y_train, y_test = train_test_split(\n", "\n", "            X_final, y, test_size=0.2, random_state=42, stratify=y_bins\n", "\n", "        )\n", "\n", "        print(\"  \u4f7f\u7528\u5206\u5c42\u62bd\u6837\")\n", "\n", "    except:\n", "\n", "        X_train, X_test, y_train, y_test = train_test_split(\n", "\n", "            X_final, y, test_size=0.2, random_state=42\n", "\n", "        )\n", "\n", "        print(\"  \u4f7f\u7528\u968f\u673a\u62bd\u6837\")\n", "\n", "    \n", "\n", "    print(f\"  \u8bad\u7ec3\u96c6: {X_train.shape[0]} \u6837\u672c\")\n", "\n", "    print(f\"  \u6d4b\u8bd5\u96c6: {X_test.shape[0]} \u6837\u672c\")\n", "\n", "    \n", "\n", "    # 4. XGBoost\u8d85\u53c2\u6570\u4f18\u5316\n", "\n", "    print(\"\\n4. XGBoost\u8d85\u53c2\u6570\u4f18\u5316...\")\n", "\n", "    \n", "\n", "    # \u9488\u5bf9\u5c0f\u6837\u672c\u7684\u53c2\u6570\u7f51\u683c\n", "\n", "    param_grid = {\n", "\n", "        'n_estimators': [50, 100, 200],\n", "\n", "        'max_depth': [3, 5, 7],\n", "\n", "        'learning_rate': [0.05, 0.1, 0.15],\n", "\n", "        'subsample': [0.8, 0.9, 1.0],\n", "\n", "        'colsample_bytree': [0.8, 0.9, 1.0],\n", "\n", "        'min_child_weight': [1, 3, 5],\n", "\n", "        'reg_alpha': [0, 0.1, 0.5],\n", "\n", "        'reg_lambda': [0.1, 1, 2]\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u57fa\u7840\u6a21\u578b\n", "\n", "    xgb_base = xgb.XGBRegressor(\n", "\n", "        random_state=42,\n", "\n", "        n_jobs=-1,\n", "\n", "        eval_metric='rmse'\n", "\n", "    )\n", "\n", "    \n", "\n", "    # \u4f7f\u7528RandomizedSearchCV\u8fdb\u884c\u8d85\u53c2\u6570\u641c\u7d22\uff08\u66f4\u9002\u5408\u5927\u53c2\u6570\u7a7a\u95f4\uff09\n", "\n", "    from sklearn.model_selection import RandomizedSearchCV\n", "\n", "    \n", "\n", "    random_search = RandomizedSearchCV(\n", "\n", "        xgb_base,\n", "\n", "        param_grid,\n", "\n", "        n_iter=50,  # \u5c1d\u8bd550\u4e2a\u53c2\u6570\u7ec4\u5408\n", "\n", "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n", "\n", "        scoring='r2',\n", "\n", "        n_jobs=-1,\n", "\n", "        random_state=42,\n", "\n", "        verbose=1\n", "\n", "    )\n", "\n", "    \n", "\n", "    print(\"  \u5f00\u59cb\u8d85\u53c2\u6570\u641c\u7d22...\")\n", "\n", "    random_search.fit(X_train, y_train)\n", "\n", "    \n", "\n", "    best_xgb = random_search.best_estimator_\n", "\n", "    \n", "\n", "    print(f\"  \u6700\u4f73\u53c2\u6570: {random_search.best_params_}\")\n", "\n", "    print(f\"  \u6700\u4f73CV R\u00b2: {random_search.best_score_:.4f}\")\n", "\n", "    \n", "\n", "    # 5. \u6a21\u578b\u8bc4\u4f30\n", "\n", "    print(\"\\n5. \u6a21\u578b\u8bc4\u4f30...\")\n", "\n", "    \n", "\n", "    # \u8bad\u7ec3\u96c6\u9884\u6d4b\n", "\n", "    y_train_pred = best_xgb.predict(X_train)\n", "\n", "    train_r2 = r2_score(y_train, y_train_pred)\n", "\n", "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n", "\n", "    train_mae = mean_absolute_error(y_train, y_train_pred)\n", "\n", "    \n", "\n", "    # \u6d4b\u8bd5\u96c6\u9884\u6d4b\n", "\n", "    y_test_pred = best_xgb.predict(X_test)\n", "\n", "    test_r2 = r2_score(y_test, y_test_pred)\n", "\n", "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n", "\n", "    test_mae = mean_absolute_error(y_test, y_test_pred)\n", "\n", "    \n", "\n", "    print(f\"  \u8bad\u7ec3\u96c6 - R\u00b2: {train_r2:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n", "\n", "    print(f\"  \u6d4b\u8bd5\u96c6 - R\u00b2: {test_r2:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n", "\n", "    \n", "\n", "    # \u68c0\u67e5\u8fc7\u62df\u5408\n", "\n", "    overfit_indicator = train_r2 - test_r2\n", "\n", "    print(f\"  \u8fc7\u62df\u5408\u6307\u6807: {overfit_indicator:.4f}\")\n", "\n", "    \n", "\n", "    if overfit_indicator > 0.2:\n", "\n", "        print(\"  \u26a0\ufe0f \u53ef\u80fd\u5b58\u5728\u8fc7\u62df\u5408\")\n", "\n", "    elif test_r2 < 0:\n", "\n", "        print(\"  \u26a0\ufe0f \u6a21\u578b\u6027\u80fd\u5f88\u5dee\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\")\n", "\n", "    else:\n", "\n", "        print(\"  \u2713 \u6a21\u578b\u6027\u80fd\u5408\u7406\")\n", "\n", "    \n", "\n", "    # 6. SHAP\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\n", "\n", "    print(\"\\n6. SHAP\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        # \u521b\u5efaSHAP\u89e3\u91ca\u5668\n", "\n", "        explainer = shap.TreeExplainer(best_xgb)\n", "\n", "        shap_values = explainer.shap_values(X_test)\n", "\n", "        \n", "\n", "        # \u8ba1\u7b97\u7279\u5f81\u91cd\u8981\u6027\n", "\n", "        feature_importance = np.abs(shap_values).mean(0)\n", "\n", "        importance_df = pd.DataFrame({\n", "\n", "            'feature': X_final.columns,\n", "\n", "            'importance': feature_importance\n", "\n", "        }).sort_values('importance', ascending=False)\n", "\n", "        \n", "\n", "        print(\"  SHAP\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f:\")\n", "\n", "        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n", "\n", "            print(f\"    {i+1:2d}. {row['feature']:20s}: {row['importance']:.4f}\")\n", "\n", "    \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  SHAP\u5206\u6790\u5931\u8d25: {e}\")\n", "\n", "        # \u4f7f\u7528XGBoost\u5185\u7f6e\u7279\u5f81\u91cd\u8981\u6027\n", "\n", "        feature_importance = best_xgb.feature_importances_\n", "\n", "        importance_df = pd.DataFrame({\n", "\n", "            'feature': X_final.columns,\n", "\n", "            'importance': feature_importance\n", "\n", "        }).sort_values('importance', ascending=False)\n", "\n", "        \n", "\n", "        print(\"  XGBoost\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f:\")\n", "\n", "        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n", "\n", "            print(f\"    {i+1:2d}. {row['feature']:20s}: {row['importance']:.4f}\")\n", "\n", "    \n", "\n", "    # 7. \u53ef\u89c6\u5316\u7ed3\u679c\n", "\n", "    print(\"\\n7. \u751f\u6210\u53ef\u89c6\u5316...\")\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n", "\n", "    \n", "\n", "    # 7.1 \u9884\u6d4bvs\u5b9e\u9645\n", "\n", "    axes[0, 0].scatter(y_train, y_train_pred, alpha=0.6, label=f'Train (R\u00b2={train_r2:.3f})', color='blue')\n", "\n", "    axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, label=f'Test (R\u00b2={test_r2:.3f})', color='red')\n", "\n", "    \n", "\n", "    # \u5b8c\u7f8e\u9884\u6d4b\u7ebf\n", "\n", "    min_val = min(y.min(), min(y_train_pred.min(), y_test_pred.min()))\n", "\n", "    max_val = max(y.max(), max(y_train_pred.max(), y_test_pred.max()))\n", "\n", "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, alpha=0.8)\n", "\n", "    \n", "\n", "    axes[0, 0].set_xlabel('Actual Salinity')\n", "\n", "    axes[0, 0].set_ylabel('Predicted Salinity')\n", "\n", "    axes[0, 0].set_title('Predicted vs Actual')\n", "\n", "    axes[0, 0].legend()\n", "\n", "    axes[0, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.2 \u6b8b\u5dee\u56fe\n", "\n", "    train_residuals = y_train - y_train_pred\n", "\n", "    test_residuals = y_test - y_test_pred\n", "\n", "    \n", "\n", "    axes[0, 1].scatter(y_train_pred, train_residuals, alpha=0.6, label='Train', color='blue')\n", "\n", "    axes[0, 1].scatter(y_test_pred, test_residuals, alpha=0.6, label='Test', color='red')\n", "\n", "    axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.8)\n", "\n", "    \n", "\n", "    axes[0, 1].set_xlabel('Predicted Salinity')\n", "\n", "    axes[0, 1].set_ylabel('Residuals')\n", "\n", "    axes[0, 1].set_title('Residuals Plot')\n", "\n", "    axes[0, 1].legend()\n", "\n", "    axes[0, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.3 \u7279\u5f81\u91cd\u8981\u6027\n", "\n", "    top_features = importance_df.head(15)\n", "\n", "    axes[0, 2].barh(range(len(top_features)), top_features['importance'])\n", "\n", "    axes[0, 2].set_yticks(range(len(top_features)))\n", "\n", "    axes[0, 2].set_yticklabels(top_features['feature'])\n", "\n", "    axes[0, 2].set_xlabel('Feature Importance')\n", "\n", "    axes[0, 2].set_title('Top 15 Feature Importances')\n", "\n", "    axes[0, 2].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.4 \u4ea4\u53c9\u9a8c\u8bc1\u5206\u6570\u5206\u5e03\n", "\n", "    cv_scores = []\n", "\n", "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n", "\n", "    for train_idx, val_idx in kfold.split(X_train):\n", "\n", "        X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n", "\n", "        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n", "\n", "        \n", "\n", "        best_xgb.fit(X_cv_train, y_cv_train)\n", "\n", "        y_cv_pred = best_xgb.predict(X_cv_val)\n", "\n", "        cv_scores.append(r2_score(y_cv_val, y_cv_pred))\n", "\n", "    \n", "\n", "    axes[1, 0].boxplot(cv_scores)\n", "\n", "    axes[1, 0].set_ylabel('R\u00b2 Score')\n", "\n", "    axes[1, 0].set_title(f'Cross-Validation Scores\\nMean: {np.mean(cv_scores):.4f} \u00b1 {np.std(cv_scores):.4f}')\n", "\n", "    axes[1, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.5 \u5b66\u4e60\u66f2\u7ebf\n", "\n", "    from sklearn.model_selection import learning_curve\n", "\n", "    \n", "\n", "    train_sizes, train_scores, val_scores = learning_curve(\n", "\n", "        best_xgb, X_train, y_train, cv=5, \n", "\n", "        train_sizes=np.linspace(0.3, 1.0, 8),\n", "\n", "        scoring='r2', random_state=42\n", "\n", "    )\n", "\n", "    \n", "\n", "    axes[1, 1].plot(train_sizes, np.mean(train_scores, axis=1), 'b-', label='Training Score')\n", "\n", "    axes[1, 1].fill_between(train_sizes, \n", "\n", "                           np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),\n", "\n", "                           np.mean(train_scores, axis=1) + np.std(train_scores, axis=1),\n", "\n", "                           alpha=0.2, color='blue')\n", "\n", "    \n", "\n", "    axes[1, 1].plot(train_sizes, np.mean(val_scores, axis=1), 'r-', label='Validation Score')\n", "\n", "    axes[1, 1].fill_between(train_sizes,\n", "\n", "                           np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),\n", "\n", "                           np.mean(val_scores, axis=1) + np.std(val_scores, axis=1),\n", "\n", "                           alpha=0.2, color='red')\n", "\n", "    \n", "\n", "    axes[1, 1].set_xlabel('Training Set Size')\n", "\n", "    axes[1, 1].set_ylabel('R\u00b2 Score')\n", "\n", "    axes[1, 1].set_title('Learning Curve')\n", "\n", "    axes[1, 1].legend()\n", "\n", "    axes[1, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.6 \u9884\u6d4b\u5206\u5e03\u5bf9\u6bd4\n", "\n", "    axes[1, 2].hist(y_train, bins=20, alpha=0.5, label='Actual Train', color='blue', density=True)\n", "\n", "    axes[1, 2].hist(y_train_pred, bins=20, alpha=0.5, label='Predicted Train', color='lightblue', density=True)\n", "\n", "    axes[1, 2].hist(y_test, bins=20, alpha=0.5, label='Actual Test', color='red', density=True)\n", "\n", "    axes[1, 2].hist(y_test_pred, bins=20, alpha=0.5, label='Predicted Test', color='lightcoral', density=True)\n", "\n", "    \n", "\n", "    axes[1, 2].set_xlabel('Salinity Value')\n", "\n", "    axes[1, 2].set_ylabel('Density')\n", "\n", "    axes[1, 2].set_title('Distribution Comparison')\n", "\n", "    axes[1, 2].legend()\n", "\n", "    axes[1, 2].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'xgboost_modeling_results.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 8. \u4fdd\u5b58\u6a21\u578b\u548c\u7ed3\u679c\n", "\n", "    print(\"\\n8. \u4fdd\u5b58\u6a21\u578b\u548c\u7ed3\u679c...\")\n", "\n", "    \n", "\n", "    import pickle\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u6a21\u578b\n", "\n", "    model_path = os.path.join(output_dir, 'optimized_xgboost_model.pkl')\n", "\n", "    with open(model_path, 'wb') as f:\n", "\n", "        pickle.dump(best_xgb, f)\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u7ed3\u679c\n", "\n", "    results = {\n", "\n", "        'model_type': 'XGBoost',\n", "\n", "        'best_params': random_search.best_params_,\n", "\n", "        'cv_r2': random_search.best_score_,\n", "\n", "        'train_r2': train_r2,\n", "\n", "        'test_r2': test_r2,\n", "\n", "        'train_rmse': train_rmse,\n", "\n", "        'test_rmse': test_rmse,\n", "\n", "        'train_mae': train_mae,\n", "\n", "        'test_mae': test_mae,\n", "\n", "        'overfit_indicator': overfit_indicator,\n", "\n", "        'n_features': len(X_final.columns),\n", "\n", "        'feature_names': list(X_final.columns),\n", "\n", "        'cv_scores': cv_scores,\n", "\n", "        'cv_mean': np.mean(cv_scores),\n", "\n", "        'cv_std': np.std(cv_scores)\n", "\n", "    }\n", "\n", "    \n", "\n", "    results_df = pd.DataFrame([results])\n", "\n", "    results_df.to_csv(os.path.join(output_dir, 'xgboost_results.csv'), index=False)\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u7279\u5f81\u91cd\u8981\u6027\n", "\n", "    importance_df.to_csv(os.path.join(output_dir, 'feature_importance_xgboost.csv'), index=False)\n", "\n", "    \n", "\n", "    print(f\"\u2713 \u6a21\u578b\u4fdd\u5b58\u81f3: {model_path}\")\n", "\n", "    print(f\"\u2713 \u7ed3\u679c\u4fdd\u5b58\u5b8c\u6210\")\n", "\n", "    \n", "\n", "    return best_xgb, X_final.columns.tolist(), results\n", "\n", "\n", "\n", "# ==================== \u7b2c\u56db\u6b65\uff1a\u4ea4\u4e92\u7279\u5f81\u5de5\u7a0b ====================\n", "\n", "\n", "\n", "def enhanced_interaction_features(X, selected_features, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u589e\u5f3a\u7684\u4ea4\u4e92\u7279\u5f81\u5de5\u7a0b\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u56db\u6b65\uff1a\u589e\u5f3a\u7684\u4ea4\u4e92\u7279\u5f81\u5de5\u7a0b\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    X_base = X[selected_features].copy()\n", "\n", "    \n", "\n", "    # 1. \u5b9a\u4e49\u7279\u5f81\u5206\u7ec4\n", "\n", "    print(\"\\n1. \u5b9a\u4e49\u7279\u5f81\u5206\u7ec4...\")\n", "\n", "    \n", "\n", "    feature_groups = {\n", "\n", "        'optical': [f for f in selected_features if any(x in f for x in ['SR_B', 'NDVI', 'EVI', 'SAVI', 'NDWI', 'MNDWI'])],\n", "\n", "        'thermal': [f for f in selected_features if any(x in f for x in ['ST_B', 'Temp'])],\n", "\n", "        'radar': [f for f in selected_features if any(x in f for x in ['VV', 'VH', 'Pol', 'RVI', 'DPSVI'])],\n", "\n", "        'salinity': [f for f in selected_features if any(x in f for x in ['SI', 'S1', 'S2', 'S3', 'S5', 'S6', 'NDSI'])],\n", "\n", "        'environmental': [f for f in selected_features if any(x in f for x in ['ET', 'PET', 'precip', 'elevation', 'slope', 'TWI'])],\n", "\n", "        'spatial': [f for f in selected_features if any(x in f for x in ['spatial', 'knn', 'moran'])]\n", "\n", "    }\n", "\n", "    \n", "\n", "    for group, features in feature_groups.items():\n", "\n", "        print(f\"  {group:12s}: {len(features)} \u7279\u5f81\")\n", "\n", "    \n", "\n", "    # 2. \u751f\u6210\u4ea4\u4e92\u7279\u5f81\n", "\n", "    print(\"\\n2. \u751f\u6210\u4ea4\u4e92\u7279\u5f81...\")\n", "\n", "    \n", "\n", "    interaction_features = []\n", "\n", "    interaction_names = []\n", "\n", "    \n", "\n", "    # 2.1 \u5173\u952e\u7279\u5f81\u5bf9\u7684\u4e58\u79ef\u4ea4\u4e92\n", "\n", "    key_interactions = [\n", "\n", "        ('optical', 'thermal'),    # \u5149\u5b66\u00d7\u70ed\u7ea2\u5916\n", "\n", "        ('optical', 'radar'),      # \u5149\u5b66\u00d7\u96f7\u8fbe\n", "\n", "        ('radar', 'thermal'),      # \u96f7\u8fbe\u00d7\u70ed\u7ea2\u5916\n", "\n", "        ('salinity', 'environmental'),  # \u76d0\u5ea6\u00d7\u73af\u5883\n", "\n", "        ('optical', 'environmental'),   # \u5149\u5b66\u00d7\u73af\u5883\n", "\n", "        ('thermal', 'environmental')    # \u70ed\u7ea2\u5916\u00d7\u73af\u5883\n", "\n", "    ]\n", "\n", "    \n", "\n", "    for group1, group2 in key_interactions:\n", "\n", "        features1 = feature_groups[group1]\n", "\n", "        features2 = feature_groups[group2]\n", "\n", "        \n", "\n", "        if features1 and features2:\n", "\n", "            # \u9009\u62e9\u6bcf\u7ec4\u6700\u91cd\u8981\u7684\u7279\u5f81\u8fdb\u884c\u4ea4\u4e92\n", "\n", "            for f1 in features1[:2]:  # \u6bcf\u7ec4\u6700\u591a2\u4e2a\u7279\u5f81\n", "\n", "                for f2 in features2[:2]:\n", "\n", "                    if f1 != f2:\n", "\n", "                        # \u4e58\u79ef\u4ea4\u4e92\n", "\n", "                        interaction = X_base[f1] * X_base[f2]\n", "\n", "                        interaction_features.append(interaction)\n", "\n", "                        interaction_names.append(f'{f1}_x_{f2}')\n", "\n", "                        \n", "\n", "                        # \u6bd4\u503c\u4ea4\u4e92\n", "\n", "                        ratio = X_base[f1] / (X_base[f2] + 1e-10)  # \u907f\u514d\u9664\u96f6\n", "\n", "                        interaction_features.append(ratio)\n", "\n", "                        interaction_names.append(f'{f1}_div_{f2}')\n", "\n", "    \n", "\n", "    print(f\"  \u57fa\u7840\u4ea4\u4e92\u7279\u5f81: {len(interaction_features)}\")\n", "\n", "    \n", "\n", "    # 2.2 \u7279\u5b9a\u7684\u7269\u7406\u610f\u4e49\u4ea4\u4e92\n", "\n", "    physical_interactions = []\n", "\n", "    physical_names = []\n", "\n", "    \n", "\n", "    # \u690d\u88ab-\u6e29\u5ea6\u5e72\u65f1\u6307\u6570\u589e\u5f3a\u7248\n", "\n", "    if 'NDVI' in X_base.columns and 'ST_B10' in X_base.columns:\n", "\n", "        tvdi_enhanced = (X_base['ST_B10'] - X_base['ST_B10'].min()) / (X_base['NDVI'] + 0.1)\n", "\n", "        physical_interactions.append(tvdi_enhanced)\n", "\n", "        physical_names.append('TVDI_enhanced')\n", "\n", "    \n", "\n", "    # \u76d0\u5ea6\u690d\u88ab\u80c1\u8feb\u6307\u6570\n", "\n", "    salinity_features = [f for f in X_base.columns if 'SI' in f or 'S1' in f or 'S2' in f or 'S3' in f]\n", "\n", "    vegetation_features = [f for f in X_base.columns if any(x in f for x in ['NDVI', 'EVI', 'SAVI'])]\n", "\n", "    \n", "\n", "    if salinity_features and vegetation_features:\n", "\n", "        for sf in salinity_features[:2]:\n", "\n", "            for vf in vegetation_features[:2]:\n", "\n", "                stress_index = X_base[sf] / (X_base[vf] + 0.1)\n", "\n", "                physical_interactions.append(stress_index)\n", "\n", "                physical_names.append(f'stress_{sf}_{vf}')\n", "\n", "    \n", "\n", "    # \u96f7\u8fbe-\u5149\u5b66\u7ec4\u5408\u6307\u6570\n", "\n", "    if 'VV_VH_diff' in X_base.columns and 'NDVI' in X_base.columns:\n", "\n", "        radar_optical = X_base['VV_VH_diff'] * (1 - X_base['NDVI'])\n", "\n", "        physical_interactions.append(radar_optical)\n", "\n", "        physical_names.append('radar_optical_combined')\n", "\n", "    \n", "\n", "    # \u6c34\u76d0\u5e73\u8861\u6307\u6570\n", "\n", "    water_features = [f for f in X_base.columns if any(x in f for x in ['NDWI', 'MNDWI', 'ET', 'precip'])]\n", "\n", "    if len(water_features) >= 2 and salinity_features:\n", "\n", "        water_balance = X_base[water_features[0]] - X_base[water_features[1]] if len(water_features) >= 2 else X_base[water_features[0]]\n", "\n", "        for sf in salinity_features[:1]:\n", "\n", "            salt_water_balance = water_balance / (X_base[sf] + 0.1)\n", "\n", "            physical_interactions.append(salt_water_balance)\n", "\n", "            physical_names.append(f'salt_water_balance_{sf}')\n", "\n", "    \n", "\n", "    print(f\"  \u7269\u7406\u610f\u4e49\u4ea4\u4e92\u7279\u5f81: {len(physical_interactions)}\")\n", "\n", "    \n", "\n", "    # 2.3 \u591a\u9879\u5f0f\u7279\u5f81\uff08\u5173\u952e\u7279\u5f81\u7684\u5e73\u65b9\u548c\u7acb\u65b9\uff09\n", "\n", "    polynomial_features = []\n", "\n", "    polynomial_names = []\n", "\n", "    \n", "\n", "    # \u57fa\u4e8e\u91cd\u8981\u6027\u9009\u62e9\u5173\u952e\u7279\u5f81\n", "\n", "    key_features = []\n", "\n", "    for group, features in feature_groups.items():\n", "\n", "        if features:\n", "\n", "            key_features.extend(features[:1])  # \u6bcf\u7ec4\u90091\u4e2a\u6700\u91cd\u8981\u7684\n", "\n", "    \n", "\n", "    for feature in key_features[:5]:  # \u6700\u591a5\u4e2a\u7279\u5f81\n", "\n", "        if feature in X_base.columns:\n", "\n", "            # \u5e73\u65b9\u9879\n", "\n", "            squared = X_base[feature] ** 2\n", "\n", "            polynomial_features.append(squared)\n", "\n", "            polynomial_names.append(f'{feature}_squared')\n", "\n", "            \n", "\n", "            # \u7acb\u65b9\u9879\uff08\u53ea\u5bf9\u5173\u952e\u7279\u5f81\uff09\n", "\n", "            if feature in ['NDVI', 'ST_B10', 'VV_VH_diff']:\n", "\n", "                cubed = X_base[feature] ** 3\n", "\n", "                polynomial_features.append(cubed)\n", "\n", "                polynomial_names.append(f'{feature}_cubed')\n", "\n", "    \n", "\n", "    print(f\"  \u591a\u9879\u5f0f\u7279\u5f81: {len(polynomial_features)}\")\n", "\n", "    \n", "\n", "    # 3. \u5408\u5e76\u6240\u6709\u65b0\u7279\u5f81\n", "\n", "    print(\"\\n3. \u5408\u5e76\u6240\u6709\u65b0\u7279\u5f81...\")\n", "\n", "    \n", "\n", "    all_new_features = interaction_features + physical_interactions + polynomial_features\n", "\n", "    all_new_names = interaction_names + physical_names + polynomial_names\n", "\n", "    \n", "\n", "    if all_new_features:\n", "\n", "        # \u521b\u5efa\u65b0\u7279\u5f81DataFrame\n", "\n", "        new_features_df = pd.DataFrame(\n", "\n", "            np.column_stack(all_new_features),\n", "\n", "            columns=all_new_names,\n", "\n", "            index=X_base.index\n", "\n", "        )\n", "\n", "        \n", "\n", "        # \u5904\u7406\u65e0\u7a77\u5927\u548cNaN\u503c\n", "\n", "        new_features_df = new_features_df.replace([np.inf, -np.inf], np.nan)\n", "\n", "        new_features_df = new_features_df.fillna(new_features_df.median())\n", "\n", "        \n", "\n", "        # \u5408\u5e76\u539f\u59cb\u7279\u5f81\u548c\u65b0\u7279\u5f81\n", "\n", "        X_enhanced = pd.concat([X_base, new_features_df], axis=1)\n", "\n", "        \n", "\n", "        print(f\"  \u539f\u59cb\u7279\u5f81\u6570: {X_base.shape[1]}\")\n", "\n", "        print(f\"  \u65b0\u589e\u7279\u5f81\u6570: {new_features_df.shape[1]}\")\n", "\n", "        print(f\"  \u603b\u7279\u5f81\u6570: {X_enhanced.shape[1]}\")\n", "\n", "        \n", "\n", "    else:\n", "\n", "        X_enhanced = X_base.copy()\n", "\n", "        print(\"  \u672a\u751f\u6210\u65b0\u7279\u5f81\")\n", "\n", "    \n", "\n", "    # 4. \u7279\u5f81\u7b5b\u9009\n", "\n", "    print(\"\\n4. \u65b0\u7279\u5f81\u7b5b\u9009...\")\n", "\n", "    \n", "\n", "    # \u65b9\u5dee\u7b5b\u9009\n", "\n", "    from sklearn.feature_selection import VarianceThreshold\n", "\n", "    var_selector = VarianceThreshold(threshold=0.01)\n", "\n", "    \n", "\n", "    try:\n", "\n", "        X_var_filtered = var_selector.fit_transform(X_enhanced)\n", "\n", "        selected_feature_mask = var_selector.get_support()\n", "\n", "        filtered_feature_names = X_enhanced.columns[selected_feature_mask].tolist()\n", "\n", "        \n", "\n", "        X_final = pd.DataFrame(X_var_filtered, columns=filtered_feature_names, index=X_enhanced.index)\n", "\n", "        \n", "\n", "        print(f\"  \u65b9\u5dee\u7b5b\u9009\u540e: {X_final.shape[1]} \u7279\u5f81\")\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u65b9\u5dee\u7b5b\u9009\u5931\u8d25: {e}\")\n", "\n", "        X_final = X_enhanced.copy()\n", "\n", "    \n", "\n", "    # 5. \u53ef\u89c6\u5316\u65b0\u7279\u5f81\u5206\u5e03\n", "\n", "    print(\"\\n5. \u53ef\u89c6\u5316\u65b0\u7279\u5f81\u5206\u5e03...\")\n", "\n", "    \n", "\n", "    if len(all_new_names) > 0:\n", "\n", "        # \u9009\u62e9\u524d8\u4e2a\u65b0\u7279\u5f81\u8fdb\u884c\u53ef\u89c6\u5316\n", "\n", "        features_to_plot = all_new_names[:8]\n", "\n", "        \n", "\n", "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n", "\n", "        axes = axes.ravel()\n", "\n", "        \n", "\n", "        for i, feature_name in enumerate(features_to_plot):\n", "\n", "            if i < 8 and feature_name in X_final.columns:\n", "\n", "                feature_data = X_final[feature_name]\n", "\n", "                \n", "\n", "                axes[i].hist(feature_data, bins=20, alpha=0.7, edgecolor='black')\n", "\n", "                axes[i].set_title(f'{feature_name}')\n", "\n", "                axes[i].set_xlabel('Value')\n", "\n", "                axes[i].set_ylabel('Frequency')\n", "\n", "                axes[i].grid(True, alpha=0.3)\n", "\n", "                \n", "\n", "                # \u6dfb\u52a0\u7edf\u8ba1\u4fe1\u606f\n", "\n", "                mean_val = feature_data.mean()\n", "\n", "                std_val = feature_data.std()\n", "\n", "                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.3f}')\n", "\n", "                axes[i].legend()\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'interaction_features_distribution.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "    \n", "\n", "    # 6. \u4fdd\u5b58\u7279\u5f81\u5de5\u7a0b\u62a5\u544a\n", "\n", "    print(\"\\n6. \u4fdd\u5b58\u7279\u5f81\u5de5\u7a0b\u62a5\u544a...\")\n", "\n", "    \n", "\n", "    feature_report = {\n", "\n", "        'original_features': len(selected_features),\n", "\n", "        'interaction_features': len(interaction_features),\n", "\n", "        'physical_features': len(physical_interactions),\n", "\n", "        'polynomial_features': len(polynomial_features),\n", "\n", "        'total_new_features': len(all_new_features),\n", "\n", "        'final_features': X_final.shape[1],\n", "\n", "        'feature_groups': {k: len(v) for k, v in feature_groups.items()},\n", "\n", "        'new_feature_names': all_new_names\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u5230JSON\u6587\u4ef6\n", "\n", "    import json\n", "\n", "    with open(os.path.join(output_dir, 'interaction_features_report.json'), 'w', encoding='utf-8') as f:\n", "\n", "        json.dump(feature_report, f, indent=2, ensure_ascii=False)\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u6700\u7ec8\u7279\u5f81\u5217\u8868\n", "\n", "    final_features_df = pd.DataFrame({\n", "\n", "        'feature_name': X_final.columns,\n", "\n", "        'feature_type': ['interaction' if f in all_new_names else 'original' for f in X_final.columns]\n", "\n", "    })\n", "\n", "    final_features_df.to_csv(os.path.join(output_dir, 'final_enhanced_features.csv'), index=False)\n", "\n", "    \n", "\n", "    print(f\"\u2713 \u4ea4\u4e92\u7279\u5f81\u5de5\u7a0b\u5b8c\u6210\")\n", "\n", "    print(f\"\u2713 \u6700\u7ec8\u7279\u5f81\u6570: {X_final.shape[1]}\")\n", "\n", "    \n", "\n", "    return X_final, all_new_names\n", "\n", "\n", "\n", "# ==================== \u8865\u5145\uff1a\u5b8c\u6574\u7684\u53ef\u89c6\u5316\u5206\u6790\u6a21\u5757 ====================\n", "\n", "\n", "\n", "def comprehensive_data_visualization(df, y_transformed, selected_features, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u7efc\u5408\u6570\u636e\u53ef\u89c6\u5316\u5206\u6790 - \u8865\u5145\u65e9\u671f\u7248\u672c\u7684\u8be6\u7ec6\u5206\u6790\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7efc\u5408\u6570\u636e\u53ef\u89c6\u5316\u5206\u6790\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # 1. \u7279\u5f81\u4e0e\u76d0\u5ea6\u5173\u7cfb\u6563\u70b9\u56fe\u77e9\u9635\n", "\n", "    print(\"\\n1. \u751f\u6210\u7279\u5f81-\u76d0\u5ea6\u5173\u7cfb\u5206\u6790...\")\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u5173\u952e\u7279\u5f81\u8fdb\u884c\u53ef\u89c6\u5316\n", "\n", "    key_features = ['NDVI', 'ST_B10', 'VV_VH_diff', 'S3', 'Pol_Ratio', 'PET_mean', 'ET_mean', 'slope']\n", "\n", "    available_key_features = [f for f in key_features if f in df.columns][:6]\n", "\n", "    \n", "\n", "    if len(available_key_features) >= 4:\n", "\n", "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n", "\n", "        axes = axes.ravel()\n", "\n", "        \n", "\n", "        for idx, feature in enumerate(available_key_features):\n", "\n", "            if idx < 6:\n", "\n", "                ax = axes[idx]\n", "\n", "                \n", "\n", "                # \u6563\u70b9\u56fe\n", "\n", "                scatter = ax.scatter(df[feature], df['salinity'], alpha=0.6, \n", "\n", "                                   c=df['salinity'], cmap='RdYlBu_r', s=40, edgecolors='black', linewidth=0.3)\n", "\n", "                \n", "\n", "                # \u8ba1\u7b97\u76f8\u5173\u6027\n", "\n", "                corr = df[feature].corr(df['salinity'])\n", "\n", "                \n", "\n", "                # \u6dfb\u52a0\u8d8b\u52bf\u7ebf\n", "\n", "                z = np.polyfit(df[feature].fillna(df[feature].median()), df['salinity'], 1)\n", "\n", "                p = np.poly1d(z)\n", "\n", "                x_trend = np.linspace(df[feature].min(), df[feature].max(), 100)\n", "\n", "                ax.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n", "\n", "                \n", "\n", "                ax.set_xlabel(feature, fontsize=12)\n", "\n", "                ax.set_ylabel('Salinity', fontsize=12)\n", "\n", "                ax.set_title(f'{feature} vs Salinity\\nCorr = {corr:.3f}', fontsize=11)\n", "\n", "                ax.grid(True, alpha=0.3)\n", "\n", "                \n", "\n", "                # \u6dfb\u52a0\u989c\u8272\u6761\n", "\n", "                if idx == 5:  # \u53ea\u5728\u6700\u540e\u4e00\u4e2a\u5b50\u56fe\u6dfb\u52a0\u989c\u8272\u6761\n", "\n", "                    plt.colorbar(scatter, ax=ax, label='Salinity')\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'feature_salinity_relationships.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "    \n", "\n", "    # 2. \u7279\u5f81\u76f8\u5173\u6027\u70ed\u529b\u56fe\n", "\n", "    print(\"\\n2. \u751f\u6210\u7279\u5f81\u76f8\u5173\u6027\u5206\u6790...\")\n", "\n", "    \n", "\n", "    # \u8ba1\u7b97\u4e0e\u76d0\u5ea6\u7684\u76f8\u5173\u6027\n", "\n", "    feature_cols = [col for col in df.columns if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "    correlations = df[feature_cols].corrwith(df['salinity']).sort_values(ascending=False)\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u9ad8\u76f8\u5173\u7279\u5f81\u7684\u5b50\u96c6\n", "\n", "    high_corr_features = correlations.abs().nlargest(20).index.tolist()\n", "\n", "    \n", "\n", "    plt.figure(figsize=(14, 12))\n", "\n", "    corr_matrix = df[high_corr_features + ['salinity']].corr()\n", "\n", "    \n", "\n", "    # \u521b\u5efa\u63a9\u819c\uff08\u53ea\u663e\u793a\u4e0a\u4e09\u89d2\uff09\n", "\n", "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n", "\n", "    \n", "\n", "    # \u7ed8\u5236\u70ed\u529b\u56fe\n", "\n", "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f',\n", "\n", "                cmap='coolwarm', center=0, square=True, linewidths=0.5,\n", "\n", "                cbar_kws={\"shrink\": 0.8}, annot_kws={'size': 8})\n", "\n", "    \n", "\n", "    plt.title('Feature Correlation Heatmap (Top 20 Features)', fontsize=16, pad=20)\n", "\n", "    plt.xticks(rotation=45, ha='right')\n", "\n", "    plt.yticks(rotation=0)\n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_correlation_heatmap.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 3. \u6570\u636e\u6e90\u8d21\u732e\u5206\u6790\n", "\n", "    print(\"\\n3. \u5206\u6790\u4e0d\u540c\u6570\u636e\u6e90\u8d21\u732e...\")\n", "\n", "    \n", "\n", "    # \u6309\u6570\u636e\u6e90\u5206\u7c7b\u7279\u5f81\n", "\n", "    source_mapping = {\n", "\n", "        'Landsat': ['SR_B', 'ST_B', 'NDVI', 'EVI', 'SAVI', 'MSAVI', 'NDWI', 'MNDWI', \n", "\n", "                   'SI1', 'SI2', 'SI3', 'SI4', 'S1', 'S2', 'S3', 'S5', 'S6', \n", "\n", "                   'NDSI', 'SI_MSI', 'BSI', 'BI', 'TVDI', 'Temp'],\n", "\n", "        'Sentinel-1': ['VV', 'VH', 'RVI', 'DPSVI', 'Pol', 'SMI', 'angle'],\n", "\n", "        'Environmental': ['ET', 'PET', 'precip', 'groundwater', 'elevation', 'slope', \n", "\n", "                         'aspect', 'TWI', 'hillshade'],\n", "\n", "        'Interaction': ['interaction', 'ratio', 'div', 'squared', 'cubed', 'enhanced', 'combined', 'balance']\n", "\n", "    }\n", "\n", "    \n", "\n", "    source_corr = {}\n", "\n", "    source_counts = {}\n", "\n", "    \n", "\n", "    for source, keywords in source_mapping.items():\n", "\n", "        source_features = [f for f in feature_cols if any(k in f for k in keywords)]\n", "\n", "        if source_features:\n", "\n", "            source_corr[source] = df[source_features].corrwith(df['salinity']).abs().mean()\n", "\n", "            source_counts[source] = len(source_features)\n", "\n", "        else:\n", "\n", "            source_corr[source] = 0\n", "\n", "            source_counts[source] = 0\n", "\n", "    \n", "\n", "    # \u7ed8\u5236\u6570\u636e\u6e90\u8d21\u732e\u56fe\n", "\n", "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n", "\n", "    \n", "\n", "    # \u8d21\u732e\u5ea6\u67f1\u72b6\u56fe\n", "\n", "    sources = list(source_corr.keys())\n", "\n", "    values = list(source_corr.values())\n", "\n", "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n", "\n", "    \n", "\n", "    bars = ax1.bar(sources, values, color=colors[:len(sources)], alpha=0.8, edgecolor='black')\n", "\n", "    ax1.set_xlabel('Data Source', fontsize=12)\n", "\n", "    ax1.set_ylabel('Average Absolute Correlation with Salinity', fontsize=12)\n", "\n", "    ax1.set_title('Data Source Contribution to Salinity Prediction', fontsize=14)\n", "\n", "    ax1.grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u6dfb\u52a0\u6570\u503c\u6807\u7b7e\n", "\n", "    for bar, value in zip(bars, values):\n", "\n", "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n", "\n", "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n", "\n", "    \n", "\n", "    # \u7279\u5f81\u6570\u91cf\u5206\u5e03\u997c\u56fe\n", "\n", "    sizes = [source_counts[source] for source in sources]\n", "\n", "    ax2.pie(sizes, labels=[f'{source}\\n({count} features)' for source, count in zip(sources, sizes)], \n", "\n", "            autopct='%1.1f%%', colors=colors[:len(sources)], startangle=90)\n", "\n", "    ax2.set_title('Feature Count Distribution by Data Source', fontsize=14)\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'data_source_analysis.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 4. \u7a7a\u95f4\u5206\u5e03\u591a\u53d8\u91cf\u5206\u6790\n", "\n", "    print(\"\\n4. \u751f\u6210\u7a7a\u95f4\u5206\u5e03\u5206\u6790...\")\n", "\n", "    \n", "\n", "    if 'longitude' in df.columns and 'latitude' in df.columns:\n", "\n", "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n", "\n", "        \n", "\n", "        # \u53d8\u91cf\u5217\u8868\n", "\n", "        spatial_vars = ['salinity', 'NDVI', 'ST_B10', 'VV_VH_diff', 'ET_mean', 'slope']\n", "\n", "        spatial_vars = [var for var in spatial_vars if var in df.columns][:6]\n", "\n", "        \n", "\n", "        for i, var in enumerate(spatial_vars):\n", "\n", "            row, col = i // 3, i % 3\n", "\n", "            ax = axes[row, col]\n", "\n", "            \n", "\n", "            # \u6839\u636e\u53d8\u91cf\u9009\u62e9\u989c\u8272\u56fe\n", "\n", "            if var == 'salinity':\n", "\n", "                cmap = 'RdYlBu_r'\n", "\n", "                label = 'Salinity'\n", "\n", "            elif 'NDVI' in var:\n", "\n", "                cmap = 'RdYlGn'\n", "\n", "                label = 'NDVI'\n", "\n", "            elif 'ST_B10' in var or 'Temp' in var:\n", "\n", "                cmap = 'hot'\n", "\n", "                label = 'Temperature (K)'\n", "\n", "            elif 'VV' in var:\n", "\n", "                cmap = 'viridis'\n", "\n", "                label = 'VV-VH (dB)'\n", "\n", "            elif 'ET' in var:\n", "\n", "                cmap = 'Blues'\n", "\n", "                label = 'ET'\n", "\n", "            else:\n", "\n", "                cmap = 'plasma'\n", "\n", "                label = var\n", "\n", "            \n", "\n", "            scatter = ax.scatter(df['longitude'], df['latitude'], \n", "\n", "                               c=df[var], s=50, cmap=cmap,\n", "\n", "                               edgecolors='black', linewidth=0.5, alpha=0.8)\n", "\n", "            \n", "\n", "            ax.set_xlabel('Longitude', fontsize=10)\n", "\n", "            ax.set_ylabel('Latitude', fontsize=10)\n", "\n", "            ax.set_title(f'{var} Spatial Distribution', fontsize=12)\n", "\n", "            ax.grid(True, alpha=0.3)\n", "\n", "            \n", "\n", "            # \u6dfb\u52a0\u989c\u8272\u6761\n", "\n", "            cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n", "\n", "            cbar.set_label(label, fontsize=9)\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'comprehensive_spatial_analysis.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "    \n", "\n", "    # 5. \u7279\u5f81\u5206\u5e03\u7edf\u8ba1\u5206\u6790\n", "\n", "    print(\"\\n5. \u751f\u6210\u7279\u5f81\u5206\u5e03\u7edf\u8ba1...\")\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u4ee3\u8868\u6027\u7279\u5f81\n", "\n", "    representative_features = correlations.abs().nlargest(8).index.tolist()\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 4, figsize=(16, 10))\n", "\n", "    axes = axes.ravel()\n", "\n", "    \n", "\n", "    for i, feature in enumerate(representative_features):\n", "\n", "        if i < 8:\n", "\n", "            ax = axes[i]\n", "\n", "            \n", "\n", "            # \u76f4\u65b9\u56fe\n", "\n", "            n, bins, patches = ax.hist(df[feature].dropna(), bins=25, alpha=0.7, \n", "\n", "                                      edgecolor='black', density=True)\n", "\n", "            \n", "\n", "            # \u6dfb\u52a0\u6838\u5bc6\u5ea6\u4f30\u8ba1\n", "\n", "            try:\n", "\n", "                from scipy.stats import gaussian_kde\n", "\n", "                data = df[feature].dropna()\n", "\n", "                kde = gaussian_kde(data)\n", "\n", "                x_range = np.linspace(data.min(), data.max(), 100)\n", "\n", "                ax.plot(x_range, kde(x_range), 'r-', lw=2, label='KDE')\n", "\n", "            except:\n", "\n", "                pass\n", "\n", "            \n", "\n", "            # \u7edf\u8ba1\u4fe1\u606f\n", "\n", "            mean_val = df[feature].mean()\n", "\n", "            median_val = df[feature].median()\n", "\n", "            std_val = df[feature].std()\n", "\n", "            skew_val = df[feature].skew()\n", "\n", "            \n", "\n", "            ax.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.3f}')\n", "\n", "            ax.axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'Median: {median_val:.3f}')\n", "\n", "            \n", "\n", "            ax.set_title(f'{feature}\\nSkew: {skew_val:.3f}, Std: {std_val:.3f}', fontsize=10)\n", "\n", "            ax.set_xlabel('Value', fontsize=9)\n", "\n", "            ax.set_ylabel('Density', fontsize=9)\n", "\n", "            ax.legend(fontsize=8)\n", "\n", "            ax.grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_distribution_analysis.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    return correlations, source_corr\n", "\n", "\n", "\n", "def enhanced_model_diagnostics(model, X_train, X_test, y_train, y_test, feature_names, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u589e\u5f3a\u7684\u6a21\u578b\u8bca\u65ad\u53ef\u89c6\u5316\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u589e\u5f3a\u6a21\u578b\u8bca\u65ad\u5206\u6790\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # 1. SHAP\u5206\u6790\n", "\n", "    print(\"\\n1. SHAP\u7279\u5f81\u89e3\u91ca\u5206\u6790...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        import shap\n", "\n", "        \n", "\n", "        # \u521b\u5efa\u89e3\u91ca\u5668\n", "\n", "        explainer = shap.TreeExplainer(model)\n", "\n", "        shap_values = explainer.shap_values(X_test)\n", "\n", "        \n", "\n", "        # SHAP\u6c47\u603b\u56fe\n", "\n", "        plt.figure(figsize=(12, 8))\n", "\n", "        shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n", "\n", "        plt.title('SHAP Feature Importance Summary', fontsize=16, pad=20)\n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'shap_summary_plot.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "        \n", "\n", "        # SHAP\u7011\u5e03\u56fe\uff08\u9009\u62e9\u4e00\u4e2a\u4ee3\u8868\u6027\u6837\u672c\uff09\n", "\n", "        plt.figure(figsize=(12, 8))\n", "\n", "        sample_idx = len(X_test) // 2  # \u9009\u62e9\u4e2d\u95f4\u7684\u6837\u672c\n", "\n", "        shap.waterfall_plot(explainer.expected_value, shap_values[sample_idx], \n", "\n", "                           X_test.iloc[sample_idx], feature_names=feature_names, show=False)\n", "\n", "        plt.title(f'SHAP Waterfall Plot - Sample {sample_idx}', fontsize=16)\n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'shap_waterfall_plot.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "        \n", "\n", "        # SHAP\u7279\u5f81\u91cd\u8981\u6027\u6761\u5f62\u56fe\n", "\n", "        feature_importance = np.abs(shap_values).mean(0)\n", "\n", "        importance_df = pd.DataFrame({\n", "\n", "            'feature': feature_names,\n", "\n", "            'importance': feature_importance\n", "\n", "        }).sort_values('importance', ascending=True)\n", "\n", "        \n", "\n", "        plt.figure(figsize=(10, 12))\n", "\n", "        plt.barh(range(len(importance_df)), importance_df['importance'])\n", "\n", "        plt.yticks(range(len(importance_df)), importance_df['feature'])\n", "\n", "        plt.xlabel('Mean |SHAP Value|', fontsize=12)\n", "\n", "        plt.title('SHAP Feature Importance', fontsize=16)\n", "\n", "        plt.grid(True, alpha=0.3)\n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'shap_feature_importance.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"SHAP\u5206\u6790\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # 2. \u6b8b\u5dee\u8be6\u7ec6\u5206\u6790\n", "\n", "    print(\"\\n2. \u6b8b\u5dee\u8be6\u7ec6\u5206\u6790...\")\n", "\n", "    \n", "\n", "    y_train_pred = model.predict(X_train)\n", "\n", "    y_test_pred = model.predict(X_test)\n", "\n", "    \n", "\n", "    train_residuals = y_train - y_train_pred\n", "\n", "    test_residuals = y_test - y_test_pred\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n", "\n", "    \n", "\n", "    # \u6b8b\u5deevs\u9884\u6d4b\u503c\n", "\n", "    axes[0, 0].scatter(y_train_pred, train_residuals, alpha=0.6, color='blue', label='Train')\n", "\n", "    axes[0, 0].scatter(y_test_pred, test_residuals, alpha=0.6, color='red', label='Test')\n", "\n", "    axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.8)\n", "\n", "    axes[0, 0].set_xlabel('Predicted Values')\n", "\n", "    axes[0, 0].set_ylabel('Residuals')\n", "\n", "    axes[0, 0].set_title('Residuals vs Predicted')\n", "\n", "    axes[0, 0].legend()\n", "\n", "    axes[0, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u6b8b\u5dee\u76f4\u65b9\u56fe\n", "\n", "    axes[0, 1].hist(train_residuals, bins=20, alpha=0.6, label='Train', color='blue', density=True)\n", "\n", "    axes[0, 1].hist(test_residuals, bins=20, alpha=0.6, label='Test', color='red', density=True)\n", "\n", "    axes[0, 1].set_xlabel('Residuals')\n", "\n", "    axes[0, 1].set_ylabel('Density')\n", "\n", "    axes[0, 1].set_title('Residuals Distribution')\n", "\n", "    axes[0, 1].legend()\n", "\n", "    axes[0, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # Q-Q\u56fe\n", "\n", "    from scipy import stats\n", "\n", "    stats.probplot(test_residuals, dist=\"norm\", plot=axes[0, 2])\n", "\n", "    axes[0, 2].set_title('Q-Q Plot (Test Residuals)')\n", "\n", "    axes[0, 2].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u6b8b\u5deevs\u7279\u5f81\uff08\u9009\u62e9\u6700\u91cd\u8981\u7684\u7279\u5f81\uff09\n", "\n", "    if hasattr(model, 'feature_importances_'):\n", "\n", "        top_feature_idx = np.argmax(model.feature_importances_)\n", "\n", "        top_feature = feature_names[top_feature_idx]\n", "\n", "        \n", "\n", "        axes[1, 0].scatter(X_train.iloc[:, top_feature_idx], train_residuals, alpha=0.6, color='blue', label='Train')\n", "\n", "        axes[1, 0].scatter(X_test.iloc[:, top_feature_idx], test_residuals, alpha=0.6, color='red', label='Test')\n", "\n", "        axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.8)\n", "\n", "        axes[1, 0].set_xlabel(f'{top_feature} (Most Important Feature)')\n", "\n", "        axes[1, 0].set_ylabel('Residuals')\n", "\n", "        axes[1, 0].set_title('Residuals vs Top Feature')\n", "\n", "        axes[1, 0].legend()\n", "\n", "        axes[1, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u9884\u6d4b\u8bef\u5dee\u5206\u6790\n", "\n", "    train_errors = np.abs(train_residuals)\n", "\n", "    test_errors = np.abs(test_residuals)\n", "\n", "    \n", "\n", "    error_percentiles = [50, 75, 90, 95, 99]\n", "\n", "    train_percentiles = [np.percentile(train_errors, p) for p in error_percentiles]\n", "\n", "    test_percentiles = [np.percentile(test_errors, p) for p in error_percentiles]\n", "\n", "    \n", "\n", "    x = np.arange(len(error_percentiles))\n", "\n", "    width = 0.35\n", "\n", "    \n", "\n", "    axes[1, 1].bar(x - width/2, train_percentiles, width, label='Train', alpha=0.8, color='blue')\n", "\n", "    axes[1, 1].bar(x + width/2, test_percentiles, width, label='Test', alpha=0.8, color='red')\n", "\n", "    axes[1, 1].set_xlabel('Percentile')\n", "\n", "    axes[1, 1].set_ylabel('Absolute Error')\n", "\n", "    axes[1, 1].set_title('Error Percentiles')\n", "\n", "    axes[1, 1].set_xticks(x)\n", "\n", "    axes[1, 1].set_xticklabels([f'{p}%' for p in error_percentiles])\n", "\n", "    axes[1, 1].legend()\n", "\n", "    axes[1, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u9884\u6d4b\u7f6e\u4fe1\u533a\u95f4\n", "\n", "    sorted_indices = np.argsort(y_test_pred)\n", "\n", "    sorted_pred = y_test_pred[sorted_indices]\n", "\n", "    sorted_actual = y_test.iloc[sorted_indices]\n", "\n", "    sorted_residuals = test_residuals[sorted_indices]\n", "\n", "    \n", "\n", "    # \u8ba1\u7b97\u79fb\u52a8\u5e73\u5747\u7684\u7f6e\u4fe1\u533a\u95f4\n", "\n", "    window_size = max(5, len(sorted_pred) // 10)\n", "\n", "    rolling_std = pd.Series(sorted_residuals).rolling(window=window_size, center=True).std()\n", "\n", "    \n", "\n", "    axes[1, 2].scatter(sorted_pred, sorted_actual, alpha=0.6, color='blue', s=30)\n", "\n", "    axes[1, 2].plot(sorted_pred, sorted_pred, 'r--', lw=2, label='Perfect Prediction')\n", "\n", "    \n", "\n", "    # \u6dfb\u52a0\u7f6e\u4fe1\u533a\u95f4\n", "\n", "    upper_bound = sorted_pred + 1.96 * rolling_std\n", "\n", "    lower_bound = sorted_pred - 1.96 * rolling_std\n", "\n", "    \n", "\n", "    axes[1, 2].fill_between(sorted_pred, lower_bound, upper_bound, \n", "\n", "                           alpha=0.2, color='gray', label='95% Confidence')\n", "\n", "    \n", "\n", "    axes[1, 2].set_xlabel('Predicted Salinity')\n", "\n", "    axes[1, 2].set_ylabel('Actual Salinity')\n", "\n", "    axes[1, 2].set_title('Prediction with Confidence Interval')\n", "\n", "    axes[1, 2].legend()\n", "\n", "    axes[1, 2].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'enhanced_model_diagnostics.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "\n", "\n", "def feature_selection_visualization(importance_scores, cv_scores_history, selected_features, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u7279\u5f81\u9009\u62e9\u8fc7\u7a0b\u53ef\u89c6\u5316\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7279\u5f81\u9009\u62e9\u8fc7\u7a0b\u53ef\u89c6\u5316\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n", "\n", "    \n", "\n", "    # 1. \u7279\u5f81\u91cd\u8981\u6027\u5206\u5e03\n", "\n", "    axes[0, 0].hist(importance_scores, bins=30, alpha=0.7, edgecolor='black')\n", "\n", "    axes[0, 0].axvline(np.mean(importance_scores), color='red', linestyle='--', \n", "\n", "                      label=f'Mean: {np.mean(importance_scores):.4f}')\n", "\n", "    axes[0, 0].axvline(np.median(importance_scores), color='green', linestyle='--', \n", "\n", "                      label=f'Median: {np.median(importance_scores):.4f}')\n", "\n", "    axes[0, 0].set_xlabel('Feature Importance Score')\n", "\n", "    axes[0, 0].set_ylabel('Frequency')\n", "\n", "    axes[0, 0].set_title('Distribution of Feature Importance Scores')\n", "\n", "    axes[0, 0].legend()\n", "\n", "    axes[0, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 2. \u7279\u5f81\u9009\u62e9\u66f2\u7ebf\uff08\u5982\u679c\u6709\u5386\u53f2\u6570\u636e\uff09\n", "\n", "    if cv_scores_history is not None and len(cv_scores_history) > 1:\n", "\n", "        feature_counts = range(1, len(cv_scores_history) + 1)\n", "\n", "        axes[0, 1].plot(feature_counts, cv_scores_history, 'b-o', linewidth=2, markersize=6)\n", "\n", "        \n", "\n", "        # \u6807\u8bb0\u6700\u4f73\u70b9\n", "\n", "        best_idx = np.argmax(cv_scores_history)\n", "\n", "        axes[0, 1].scatter(best_idx + 1, cv_scores_history[best_idx], \n", "\n", "                          color='red', s=100, zorder=5, label=f'Best: {len(selected_features)} features')\n", "\n", "        \n", "\n", "        axes[0, 1].set_xlabel('Number of Features')\n", "\n", "        axes[0, 1].set_ylabel('Cross-Validation R\u00b2')\n", "\n", "        axes[0, 1].set_title('Feature Selection Performance Curve')\n", "\n", "        axes[0, 1].legend()\n", "\n", "        axes[0, 1].grid(True, alpha=0.3)\n", "\n", "    else:\n", "\n", "        axes[0, 1].text(0.5, 0.5, 'No CV History Available', \n", "\n", "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n", "\n", "        axes[0, 1].set_title('Feature Selection History')\n", "\n", "    \n", "\n", "    # 3. \u9009\u4e2d\u7279\u5f81\u7684\u91cd\u8981\u6027\u6392\u5e8f\n", "\n", "    if len(selected_features) > 0:\n", "\n", "        # \u5047\u8bbe\u6211\u4eec\u6709\u9009\u4e2d\u7279\u5f81\u7684\u91cd\u8981\u6027\u5206\u6570\n", "\n", "        selected_importance = importance_scores[-len(selected_features):]  # \u53d6\u6700\u540eN\u4e2a\u4f5c\u4e3a\u9009\u4e2d\u7684\n", "\n", "        \n", "\n", "        sorted_indices = np.argsort(selected_importance)[::-1]\n", "\n", "        sorted_features = [selected_features[i] for i in sorted_indices]\n", "\n", "        sorted_scores = [selected_importance[i] for i in sorted_indices]\n", "\n", "        \n", "\n", "        # \u53ea\u663e\u793a\u524d15\u4e2a\n", "\n", "        display_count = min(15, len(sorted_features))\n", "\n", "        \n", "\n", "        axes[1, 0].barh(range(display_count), sorted_scores[:display_count])\n", "\n", "        axes[1, 0].set_yticks(range(display_count))\n", "\n", "        axes[1, 0].set_yticklabels(sorted_features[:display_count], fontsize=9)\n", "\n", "        axes[1, 0].set_xlabel('Importance Score')\n", "\n", "        axes[1, 0].set_title(f'Top {display_count} Selected Features')\n", "\n", "        axes[1, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 4. \u6570\u636e\u6e90\u5206\u5e03\n", "\n", "    source_counts = {'Landsat': 0, 'Sentinel-1': 0, 'Environmental': 0, 'Interaction': 0}\n", "\n", "    \n", "\n", "    for feature in selected_features:\n", "\n", "        if any(x in feature for x in ['SR_B', 'ST_B', 'NDVI', 'EVI', 'SAVI', 'SI', 'S1', 'S2', 'S3']):\n", "\n", "            source_counts['Landsat'] += 1\n", "\n", "        elif any(x in feature for x in ['VV', 'VH', 'Pol', 'RVI', 'DPSVI', 'angle']):\n", "\n", "            source_counts['Sentinel-1'] += 1\n", "\n", "        elif any(x in feature for x in ['ET', 'PET', 'precip', 'elevation', 'slope', 'TWI']):\n", "\n", "            source_counts['Environmental'] += 1\n", "\n", "        else:\n", "\n", "            source_counts['Interaction'] += 1\n", "\n", "    \n", "\n", "    # \u997c\u56fe\n", "\n", "    sizes = [count for count in source_counts.values() if count > 0]\n", "\n", "    labels = [source for source, count in source_counts.items() if count > 0]\n", "\n", "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D'][:len(labels)]\n", "\n", "    \n", "\n", "    axes[1, 1].pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n", "\n", "    axes[1, 1].set_title('Selected Features by Data Source')\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'feature_selection_visualization.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "\n", "\n", "def run_enhanced_pipeline_v3(df_original, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u8fd0\u884c\u5b8c\u6574\u7684\u589e\u5f3a\u6d41\u7a0b v3.0\n", "\n", "    \"\"\"\n", "\n", "    print(\"\ud83d\ude80 \u5f00\u59cb\u589e\u5f3a\u7248\u76d0\u6e0d\u5316\u5efa\u6a21\u6d41\u7a0b v3.0\")\n", "\n", "    print(\"=\"*80)\n", "\n", "    \n", "\n", "    # \u786e\u4fdd\u8f93\u51fa\u76ee\u5f55\u5b58\u5728\n", "\n", "    os.makedirs(output_dir, exist_ok=True)\n", "\n", "    \n", "\n", "    try:\n", "\n", "        # \u7b2c\u4e00\u6b65\uff1a\u76ee\u6807\u53d8\u91cf\u53d8\u6362\n", "\n", "        y_transformed, transform_params, outliers_mask = advanced_target_transformation(\n", "\n", "            df_original['salinity'], output_dir\n", "\n", "        )\n", "\n", "        \n", "\n", "        # \u79fb\u9664\u5f02\u5e38\u503c\u6837\u672c\n", "\n", "        if outliers_mask.any():\n", "\n", "            clean_mask = ~outliers_mask\n", "\n", "            df_clean = df_original[clean_mask].copy()\n", "\n", "            y_clean = y_transformed[clean_mask]\n", "\n", "            print(f\"\\n\u79fb\u9664\u5f02\u5e38\u503c\u540e\u6837\u672c\u6570: {len(df_clean)}\")\n", "\n", "        else:\n", "\n", "            df_clean = df_original.copy()\n", "\n", "            y_clean = y_transformed\n", "\n", "        \n", "\n", "        # \u51c6\u5907\u7279\u5f81\u6570\u636e\n", "\n", "        feature_cols = [col for col in df_clean.columns \n", "\n", "                       if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "        X_clean = df_clean[feature_cols].fillna(0)\n", "\n", "        \n", "\n", "        # \u7b2c\u4e8c\u6b65\uff1a\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790\n", "\n", "        spatial_features, spatial_results = spatial_autocorrelation_analysis(df_clean, output_dir)\n", "\n", "        \n", "\n", "        # \u7b2c\u4e09\u6b65\uff1aXGBoost\u5efa\u6a21\uff08\u521d\u6b65\uff09\n", "\n", "        best_model_v1, selected_features_v1, results_v1 = optimized_xgboost_modeling(\n", "\n", "            X_clean, y_clean, spatial_features, output_dir\n", "\n", "        )\n", "\n", "        \n", "\n", "        print(f\"\\n\u7b2c\u4e00\u8f6e\u5efa\u6a21\u7ed3\u679c - R\u00b2: {results_v1['test_r2']:.4f}\")\n", "\n", "        \n", "\n", "        # \u7b2c\u56db\u6b65\uff1a\u4ea4\u4e92\u7279\u5f81\u5de5\u7a0b\n", "\n", "        X_enhanced, new_feature_names = enhanced_interaction_features(\n", "\n", "            X_clean, selected_features_v1, output_dir\n", "\n", "        )\n", "\n", "        \n", "\n", "        # \u7b2c\u4e94\u6b65\uff1a\u6700\u7ec8XGBoost\u5efa\u6a21\n", "\n", "        print(\"\\n\" + \"=\"*60)\n", "\n", "        print(\"\u7b2c\u4e94\u6b65\uff1a\u57fa\u4e8e\u589e\u5f3a\u7279\u5f81\u7684\u6700\u7ec8\u5efa\u6a21\")\n", "\n", "        print(\"=\"*60)\n", "\n", "        \n", "\n", "        best_model_final, selected_features_final, results_final = optimized_xgboost_modeling(\n", "\n", "            X_enhanced, y_clean, None, output_dir  # \u7a7a\u95f4\u7279\u5f81\u5df2\u5305\u542b\u5728X_enhanced\u4e2d\n", "\n", "        )\n", "\n", "        \n", "\n", "        # \u6027\u80fd\u6bd4\u8f83\n", "\n", "        print(\"\\n\" + \"=\"*60)\n", "\n", "        print(\"\u6027\u80fd\u5bf9\u6bd4\")\n", "\n", "        print(\"=\"*60)\n", "\n", "        print(f\"\u7b2c\u4e00\u8f6e\u5efa\u6a21 R\u00b2: {results_v1['test_r2']:.4f}\")\n", "\n", "        print(f\"\u6700\u7ec8\u5efa\u6a21 R\u00b2: {results_final['test_r2']:.4f}\")\n", "\n", "        print(f\"\u6539\u8fdb\u5e45\u5ea6: {results_final['test_r2'] - results_v1['test_r2']:.4f}\")\n", "\n", "        \n", "\n", "        # \u4fdd\u5b58\u5b8c\u6574\u7ed3\u679c\n", "\n", "        complete_results = {\n", "\n", "            'pipeline_version': '3.0',\n", "\n", "            'transform_method': transform_params['method'],\n", "\n", "            'spatial_analysis': spatial_results,\n", "\n", "            'v1_results': results_v1,\n", "\n", "            'final_results': results_final,\n", "\n", "            'improvement': results_final['test_r2'] - results_v1['test_r2'],\n", "\n", "            'final_features': selected_features_final,\n", "\n", "            'new_features': new_feature_names\n", "\n", "        }\n", "\n", "        \n", "\n", "        # \u4fdd\u5b58\u5230JSON\n", "\n", "        import json\n", "\n", "        with open(os.path.join(output_dir, 'complete_pipeline_results_v3.json'), 'w', encoding='utf-8') as f:\n", "\n", "            json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)\n", "\n", "        \n", "\n", "        print(\"\\n\" + \"=\"*80)\n", "\n", "        print(\"\u2705 \u589e\u5f3a\u6d41\u7a0b v3.0 \u5b8c\u6210\uff01\")\n", "\n", "        print(\"=\"*80)\n", "\n", "        print(f\"\u2713 \u6700\u7ec8\u6a21\u578b R\u00b2: {results_final['test_r2']:.4f}\")\n", "\n", "        print(f\"\u2713 \u6700\u7ec8\u7279\u5f81\u6570: {len(selected_features_final)}\")\n", "\n", "        print(f\"\u2713 \u5904\u7406\u6837\u672c\u6570: {len(df_clean)}\")\n", "\n", "        print(f\"\u2713 \u6240\u6709\u7ed3\u679c\u4fdd\u5b58\u5728: {output_dir}\")\n", "\n", "        \n", "\n", "        if results_final['test_r2'] > 0.3:\n", "\n", "            print(\"\ud83c\udf89 \u6a21\u578b\u6027\u80fd\u826f\u597d\uff0c\u53ef\u4ee5\u8fdb\u884c\u76d0\u6e0d\u5316\u53cd\u6f14\uff01\")\n", "\n", "        elif results_final['test_r2'] > 0:\n", "\n", "            print(\"\u26a0\ufe0f \u6a21\u578b\u6027\u80fd\u4e00\u822c\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u4f18\u5316\u6216\u6536\u96c6\u66f4\u591a\u6570\u636e\")\n", "\n", "        else:\n", "\n", "            print(\"\u274c \u6a21\u578b\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u91cd\u65b0\u8003\u8651\u5efa\u6a21\u7b56\u7565\")\n", "\n", "        \n", "\n", "        return best_model_final, df_clean, selected_features_final, complete_results\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"\\n\u274c \u6d41\u7a0b\u6267\u884c\u5931\u8d25: {e}\")\n", "\n", "        import traceback\n", "\n", "        traceback.print_exc()\n", "\n", "        return None, None, None, None\n", "\n", "\n", "\n", "# ==================== \u4f7f\u7528\u793a\u4f8b ====================\n", "\n", "\n", "\n", "# \u8fd0\u884c\u5b8c\u6574\u6d41\u7a0b\n", "\n", "# best_model, clean_data, final_features, results = run_enhanced_pipeline_v3(df, OUTPUT_DIR)\n", "\n", "\n", "\n", "# ==================== \u4fee\u590d\u540e\u7684XGBoost\u5efa\u6a21\u51fd\u6570 ====================\n", "\n", "\n", "\n", "def optimized_xgboost_modeling_fixed(X, y, spatial_features, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u4fee\u590d\u540e\u7684XGBoost\u8d85\u53c2\u6570\u4f18\u5316\u5efa\u6a21\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e09\u6b65\uff1aXGBoost\u8d85\u53c2\u6570\u4f18\u5316\u5efa\u6a21\uff08\u4fee\u590d\u7248\uff09\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # 1. \u6570\u636e\u51c6\u5907\n", "\n", "    print(\"\\n1. \u6570\u636e\u51c6\u5907...\")\n", "\n", "    \n", "\n", "    # \u786e\u4fddX\u662fDataFrame\n", "\n", "    if not isinstance(X, pd.DataFrame):\n", "\n", "        X = pd.DataFrame(X)\n", "\n", "    \n", "\n", "    # \u786e\u4fddy\u662fSeries\n", "\n", "    if not isinstance(y, pd.Series):\n", "\n", "        y = pd.Series(y)\n", "\n", "    \n", "\n", "    # \u5408\u5e76\u7a7a\u95f4\u7279\u5f81\n", "\n", "    if spatial_features is not None:\n", "\n", "        # \u53ea\u9009\u62e9\u6570\u503c\u578b\u7a7a\u95f4\u7279\u5f81\n", "\n", "        numeric_spatial = spatial_features.select_dtypes(include=[np.number])\n", "\n", "        X_with_spatial = pd.concat([X, numeric_spatial], axis=1)\n", "\n", "        print(f\"  \u6dfb\u52a0\u7a7a\u95f4\u7279\u5f81: {numeric_spatial.shape[1]} \u4e2a\")\n", "\n", "    else:\n", "\n", "        X_with_spatial = X.copy()\n", "\n", "        print(\"  \u672a\u6dfb\u52a0\u7a7a\u95f4\u7279\u5f81\uff08spatial_features\u4e3aNone\uff09\")\n", "\n", "    \n", "\n", "    print(f\"  \u603b\u7279\u5f81\u6570: {X_with_spatial.shape[1]}\")\n", "\n", "    print(f\"  \u6837\u672c\u6570: {X_with_spatial.shape[0]}\")\n", "\n", "    \n", "\n", "    # \u5904\u7406\u7f3a\u5931\u503c\n", "\n", "    X_with_spatial = X_with_spatial.fillna(X_with_spatial.median())\n", "\n", "    \n", "\n", "    # 2. \u7279\u5f81\u9009\u62e9\u548cVIF\u8fc7\u6ee4\n", "\n", "    print(\"\\n2. \u7279\u5f81\u9009\u62e9\u548cVIF\u8fc7\u6ee4...\")\n", "\n", "    \n", "\n", "    # 2.1 \u57fa\u4e8e\u65b9\u5dee\u7684\u521d\u6b65\u7b5b\u9009\n", "\n", "    from sklearn.feature_selection import VarianceThreshold\n", "\n", "    var_selector = VarianceThreshold(threshold=0.01)\n", "\n", "    \n", "\n", "    try:\n", "\n", "        X_var_filtered = var_selector.fit_transform(X_with_spatial)\n", "\n", "        selected_features = X_with_spatial.columns[var_selector.get_support()].tolist()\n", "\n", "        print(f\"  \u65b9\u5dee\u7b5b\u9009\u540e\u7279\u5f81\u6570: {len(selected_features)}\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u65b9\u5dee\u7b5b\u9009\u5931\u8d25: {e}\")\n", "\n", "        selected_features = X_with_spatial.columns.tolist()\n", "\n", "        X_var_filtered = X_with_spatial.values\n", "\n", "    \n", "\n", "    # 2.2 \u76f8\u5173\u6027\u7b5b\u9009\n", "\n", "    X_corr = pd.DataFrame(X_var_filtered, columns=selected_features, index=X_with_spatial.index)\n", "\n", "    \n", "\n", "    try:\n", "\n", "        corr_matrix = X_corr.corr().abs()\n", "\n", "        \n", "\n", "        # \u627e\u5230\u9ad8\u76f8\u5173\u7279\u5f81\u5bf9\n", "\n", "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n", "\n", "        high_corr_pairs = []\n", "\n", "        \n", "\n", "        for column in upper_tri.columns:\n", "\n", "            high_corr_features = upper_tri.index[upper_tri[column] > 0.9].tolist()\n", "\n", "            if high_corr_features:\n", "\n", "                for feature in high_corr_features:\n", "\n", "                    high_corr_pairs.append((column, feature, upper_tri.loc[feature, column]))\n", "\n", "        \n", "\n", "        print(f\"  \u53d1\u73b0\u9ad8\u76f8\u5173\u7279\u5f81\u5bf9: {len(high_corr_pairs)} \u5bf9\")\n", "\n", "        \n", "\n", "        # \u79fb\u9664\u9ad8\u76f8\u5173\u7279\u5f81\uff08\u4fdd\u7559\u4e0e\u76ee\u6807\u53d8\u91cf\u76f8\u5173\u6027\u66f4\u9ad8\u7684\uff09\n", "\n", "        features_to_remove = set()\n", "\n", "        target_corr = X_corr.corrwith(y).abs()\n", "\n", "        \n", "\n", "        for feat1, feat2, corr_val in high_corr_pairs:\n", "\n", "            if target_corr[feat1] > target_corr[feat2]:\n", "\n", "                features_to_remove.add(feat2)\n", "\n", "            else:\n", "\n", "                features_to_remove.add(feat1)\n", "\n", "        \n", "\n", "        final_features = [f for f in selected_features if f not in features_to_remove]\n", "\n", "        X_final = X_corr[final_features]\n", "\n", "        \n", "\n", "        print(f\"  \u76f8\u5173\u6027\u7b5b\u9009\u540e\u7279\u5f81\u6570: {len(final_features)}\")\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u76f8\u5173\u6027\u7b5b\u9009\u5931\u8d25: {e}\")\n", "\n", "        X_final = X_corr.copy()\n", "\n", "        final_features = selected_features\n", "\n", "    \n", "\n", "    # 2.3 \u7b80\u5316\u7684VIF\u8fc7\u6ee4\uff08\u907f\u514d\u6570\u7ec4\u7c7b\u578b\u9519\u8bef\uff09\n", "\n", "    print(\"\\n  \u6267\u884c\u7b80\u5316VIF\u7b5b\u9009...\")\n", "\n", "    try:\n", "\n", "        # \u786e\u4fdd\u6570\u636e\u7c7b\u578b\u6b63\u786e\n", "\n", "        X_vif = X_final.copy()\n", "\n", "        \n", "\n", "        # \u5982\u679c\u7279\u5f81\u6570\u592a\u591a\uff0c\u5148\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\n", "\n", "        if len(X_vif.columns) > 15:\n", "\n", "            # \u57fa\u4e8e\u4e0e\u76ee\u6807\u53d8\u91cf\u7684\u76f8\u5173\u6027\u9009\u62e9\u524d15\u4e2a\u7279\u5f81\n", "\n", "            correlations = X_vif.corrwith(y).abs().sort_values(ascending=False)\n", "\n", "            top_features = correlations.head(15).index.tolist()\n", "\n", "            X_final = X_vif[top_features]\n", "\n", "            print(f\"    \u57fa\u4e8e\u76f8\u5173\u6027\u9009\u62e9\u524d15\u4e2a\u7279\u5f81\")\n", "\n", "        else:\n", "\n", "            X_final = X_vif.copy()\n", "\n", "        \n", "\n", "        print(f\"    \u6700\u7ec8\u7279\u5f81\u6570: {len(X_final.columns)}\")\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  VIF\u7b5b\u9009\u5931\u8d25\uff0c\u4f7f\u7528\u539f\u59cb\u7279\u5f81: {e}\")\n", "\n", "        if len(X_final.columns) > 10:\n", "\n", "            # \u57fa\u4e8e\u65b9\u5dee\u9009\u62e9\u524d10\u4e2a\u7279\u5f81\u4f5c\u4e3a\u540e\u5907\n", "\n", "            variances = X_final.var().sort_values(ascending=False)\n", "\n", "            top_variance_features = variances.head(10).index.tolist()\n", "\n", "            X_final = X_final[top_variance_features]\n", "\n", "            print(f\"    \u4f7f\u7528\u65b9\u5dee\u6700\u5927\u768410\u4e2a\u7279\u5f81\")\n", "\n", "    \n", "\n", "    print(f\"  \u6700\u7ec8\u7279\u5f81: {list(X_final.columns)}\")\n", "\n", "    \n", "\n", "    # 3. \u6570\u636e\u5206\u5272\uff08\u5206\u5c42\u62bd\u6837\uff09\n", "\n", "    print(\"\\n3. \u6570\u636e\u5206\u5272...\")\n", "\n", "    \n", "\n", "    # \u91cd\u65b0\u5bf9\u9f50\u7d22\u5f15\n", "\n", "    y_aligned = y.loc[X_final.index]\n", "\n", "    \n", "\n", "    # \u57fa\u4e8e\u76ee\u6807\u53d8\u91cf\u5206\u4f4d\u6570\u8fdb\u884c\u5206\u5c42\n", "\n", "    try:\n", "\n", "        y_bins = pd.qcut(y_aligned, q=4, labels=False, duplicates='drop')\n", "\n", "        X_train, X_test, y_train, y_test = train_test_split(\n", "\n", "            X_final, y_aligned, test_size=0.2, random_state=42, stratify=y_bins\n", "\n", "        )\n", "\n", "        print(\"  \u4f7f\u7528\u5206\u5c42\u62bd\u6837\")\n", "\n", "    except:\n", "\n", "        X_train, X_test, y_train, y_test = train_test_split(\n", "\n", "            X_final, y_aligned, test_size=0.2, random_state=42\n", "\n", "        )\n", "\n", "        print(\"  \u4f7f\u7528\u968f\u673a\u62bd\u6837\")\n", "\n", "    \n", "\n", "    print(f\"  \u8bad\u7ec3\u96c6: {X_train.shape[0]} \u6837\u672c\")\n", "\n", "    print(f\"  \u6d4b\u8bd5\u96c6: {X_test.shape[0]} \u6837\u672c\")\n", "\n", "    \n", "\n", "    # 4. \u7b80\u5316\u7684XGBoost\u5efa\u6a21\uff08\u907f\u514d\u8fc7\u5ea6\u590d\u6742\u5316\uff09\n", "\n", "    print(\"\\n4. XGBoost\u5efa\u6a21...\")\n", "\n", "    \n", "\n", "    # \u57fa\u7840\u6a21\u578b\uff08\u4f7f\u7528\u76f8\u5bf9\u7b80\u5355\u7684\u53c2\u6570\u4ee5\u907f\u514d\u8fc7\u62df\u5408\uff09\n", "\n", "    xgb_model = xgb.XGBRegressor(\n", "\n", "        n_estimators=100,\n", "\n", "        max_depth=5,\n", "\n", "        learning_rate=0.1,\n", "\n", "        subsample=0.8,\n", "\n", "        colsample_bytree=0.8,\n", "\n", "        random_state=42,\n", "\n", "        n_jobs=-1\n", "\n", "    )\n", "\n", "    \n", "\n", "    # \u8bad\u7ec3\u6a21\u578b\n", "\n", "    xgb_model.fit(X_train, y_train)\n", "\n", "    \n", "\n", "    # 5. \u6a21\u578b\u8bc4\u4f30\n", "\n", "    print(\"\\n5. \u6a21\u578b\u8bc4\u4f30...\")\n", "\n", "    \n", "\n", "    # \u8bad\u7ec3\u96c6\u9884\u6d4b\n", "\n", "    y_train_pred = xgb_model.predict(X_train)\n", "\n", "    train_r2 = r2_score(y_train, y_train_pred)\n", "\n", "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n", "\n", "    train_mae = mean_absolute_error(y_train, y_train_pred)\n", "\n", "    \n", "\n", "    # \u6d4b\u8bd5\u96c6\u9884\u6d4b\n", "\n", "    y_test_pred = xgb_model.predict(X_test)\n", "\n", "    test_r2 = r2_score(y_test, y_test_pred)\n", "\n", "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n", "\n", "    test_mae = mean_absolute_error(y_test, y_test_pred)\n", "\n", "    \n", "\n", "    print(f\"  \u8bad\u7ec3\u96c6 - R\u00b2: {train_r2:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n", "\n", "    print(f\"  \u6d4b\u8bd5\u96c6 - R\u00b2: {test_r2:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n", "\n", "    \n", "\n", "    # \u68c0\u67e5\u8fc7\u62df\u5408\n", "\n", "    overfit_indicator = train_r2 - test_r2\n", "\n", "    print(f\"  \u8fc7\u62df\u5408\u6307\u6807: {overfit_indicator:.4f}\")\n", "\n", "    \n", "\n", "    if overfit_indicator > 0.2:\n", "\n", "        print(\"  \u26a0\ufe0f \u53ef\u80fd\u5b58\u5728\u8fc7\u62df\u5408\")\n", "\n", "    elif test_r2 < 0:\n", "\n", "        print(\"  \u26a0\ufe0f \u6a21\u578b\u6027\u80fd\u5f88\u5dee\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\")\n", "\n", "    else:\n", "\n", "        print(\"  \u2713 \u6a21\u578b\u6027\u80fd\u5408\u7406\")\n", "\n", "    \n", "\n", "    # 6. \u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\n", "\n", "    print(\"\\n6. \u7279\u5f81\u91cd\u8981\u6027\u5206\u6790...\")\n", "\n", "    \n", "\n", "    feature_importance = xgb_model.feature_importances_\n", "\n", "    importance_df = pd.DataFrame({\n", "\n", "        'feature': X_final.columns,\n", "\n", "        'importance': feature_importance\n", "\n", "    }).sort_values('importance', ascending=False)\n", "\n", "    \n", "\n", "    print(\"  \u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f:\")\n", "\n", "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n", "\n", "        print(f\"    {i+1:2d}. {row['feature']:20s}: {row['importance']:.4f}\")\n", "\n", "    \n", "\n", "    # 7. \u7b80\u5316\u7684\u53ef\u89c6\u5316\n", "\n", "    print(\"\\n7. \u751f\u6210\u57fa\u7840\u53ef\u89c6\u5316...\")\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n", "\n", "    \n", "\n", "    # 7.1 \u9884\u6d4bvs\u5b9e\u9645\n", "\n", "    axes[0, 0].scatter(y_train, y_train_pred, alpha=0.6, label=f'Train (R\u00b2={train_r2:.3f})', color='blue')\n", "\n", "    axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, label=f'Test (R\u00b2={test_r2:.3f})', color='red')\n", "\n", "    \n", "\n", "    # \u5b8c\u7f8e\u9884\u6d4b\u7ebf\n", "\n", "    min_val = min(y_aligned.min(), min(y_train_pred.min(), y_test_pred.min()))\n", "\n", "    max_val = max(y_aligned.max(), max(y_train_pred.max(), y_test_pred.max()))\n", "\n", "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, alpha=0.8)\n", "\n", "    \n", "\n", "    axes[0, 0].set_xlabel('Actual Salinity')\n", "\n", "    axes[0, 0].set_ylabel('Predicted Salinity')\n", "\n", "    axes[0, 0].set_title('Predicted vs Actual')\n", "\n", "    axes[0, 0].legend()\n", "\n", "    axes[0, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.2 \u6b8b\u5dee\u56fe\n", "\n", "    train_residuals = y_train - y_train_pred\n", "\n", "    test_residuals = y_test - y_test_pred\n", "\n", "    \n", "\n", "    axes[0, 1].scatter(y_train_pred, train_residuals, alpha=0.6, label='Train', color='blue')\n", "\n", "    axes[0, 1].scatter(y_test_pred, test_residuals, alpha=0.6, label='Test', color='red')\n", "\n", "    axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.8)\n", "\n", "    \n", "\n", "    axes[0, 1].set_xlabel('Predicted Salinity')\n", "\n", "    axes[0, 1].set_ylabel('Residuals')\n", "\n", "    axes[0, 1].set_title('Residuals Plot')\n", "\n", "    axes[0, 1].legend()\n", "\n", "    axes[0, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.3 \u7279\u5f81\u91cd\u8981\u6027\n", "\n", "    top_features = importance_df.head(min(10, len(importance_df)))\n", "\n", "    axes[1, 0].barh(range(len(top_features)), top_features['importance'])\n", "\n", "    axes[1, 0].set_yticks(range(len(top_features)))\n", "\n", "    axes[1, 0].set_yticklabels(top_features['feature'])\n", "\n", "    axes[1, 0].set_xlabel('Feature Importance')\n", "\n", "    axes[1, 0].set_title('Top Feature Importances')\n", "\n", "    axes[1, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # 7.4 \u9884\u6d4b\u5206\u5e03\u5bf9\u6bd4\n", "\n", "    axes[1, 1].hist(y_train, bins=15, alpha=0.5, label='Actual Train', color='blue', density=True)\n", "\n", "    axes[1, 1].hist(y_train_pred, bins=15, alpha=0.5, label='Predicted Train', color='lightblue', density=True)\n", "\n", "    axes[1, 1].hist(y_test, bins=15, alpha=0.5, label='Actual Test', color='red', density=True)\n", "\n", "    axes[1, 1].hist(y_test_pred, bins=15, alpha=0.5, label='Predicted Test', color='lightcoral', density=True)\n", "\n", "    \n", "\n", "    axes[1, 1].set_xlabel('Salinity Value')\n", "\n", "    axes[1, 1].set_ylabel('Density')\n", "\n", "    axes[1, 1].set_title('Distribution Comparison')\n", "\n", "    axes[1, 1].legend()\n", "\n", "    axes[1, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    plt.savefig(os.path.join(output_dir, 'xgboost_modeling_results_fixed.png'), \n", "\n", "                dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # 8. \u4fdd\u5b58\u7ed3\u679c\n", "\n", "    print(\"\\n8. \u4fdd\u5b58\u7ed3\u679c...\")\n", "\n", "    \n", "\n", "    results = {\n", "\n", "        'model_type': 'XGBoost_Fixed',\n", "\n", "        'train_r2': train_r2,\n", "\n", "        'test_r2': test_r2,\n", "\n", "        'train_rmse': train_rmse,\n", "\n", "        'test_rmse': test_rmse,\n", "\n", "        'train_mae': train_mae,\n", "\n", "        'test_mae': test_mae,\n", "\n", "        'overfit_indicator': overfit_indicator,\n", "\n", "        'n_features': len(X_final.columns),\n", "\n", "        'feature_names': list(X_final.columns),\n", "\n", "        'n_samples': len(X_final)\n", "\n", "    }\n", "\n", "    \n", "\n", "    print(f\"\u2713 \u5efa\u6a21\u5b8c\u6210\")\n", "\n", "    \n", "\n", "    return xgb_model, X_final.columns.tolist(), results\n", "\n", "\n", "\n", "# ==================== \u4fee\u590d\u540e\u7684\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790\u51fd\u6570 ====================\n", "\n", "\n", "\n", "def spatial_autocorrelation_analysis_fixed(df, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u4fee\u590d\u540e\u7684\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790 - \u517c\u5bb9\u65b0\u7248pysal\n", "\n", "    \"\"\"\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e8c\u6b65\uff1a\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790\uff08\u4fee\u590d\u7248\uff09\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    try:\n", "\n", "        import libpysal as lps\n", "\n", "        from esda.moran import Moran, Moran_Local\n", "\n", "        from sklearn.neighbors import NearestNeighbors\n", "\n", "    except ImportError:\n", "\n", "        print(\"\u26a0\ufe0f \u9700\u8981\u5b89\u88c5 pysal: pip install libpysal esda splot\")\n", "\n", "        return None, None\n", "\n", "    \n", "\n", "    # 1. \u51c6\u5907\u7a7a\u95f4\u6570\u636e\n", "\n", "    print(\"\\n1. \u51c6\u5907\u7a7a\u95f4\u6570\u636e...\")\n", "\n", "    \n", "\n", "    # \u83b7\u53d6\u5750\u6807\u548c\u76d0\u5ea6\u6570\u636e\n", "\n", "    coords = df[['longitude', 'latitude']].values\n", "\n", "    salinity = df['salinity'].values\n", "\n", "    \n", "\n", "    print(f\"  \u6837\u672c\u70b9\u6570\u91cf: {len(coords)}\")\n", "\n", "    print(f\"  \u5750\u6807\u8303\u56f4: \u7ecf\u5ea6[{coords[:, 0].min():.4f}, {coords[:, 0].max():.4f}]\")\n", "\n", "    print(f\"            \u7eac\u5ea6[{coords[:, 1].min():.4f}, {coords[:, 1].max():.4f}]\")\n", "\n", "    \n", "\n", "    # 2. \u6784\u5efa\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\n", "\n", "    print(\"\\n2. \u6784\u5efa\u7a7a\u95f4\u6743\u91cd\u77e9\u9635...\")\n", "\n", "    \n", "\n", "    # \u4f7f\u7528KNN\u6743\u91cd\uff08\u66f4\u7a33\u5b9a\uff09\n", "\n", "    try:\n", "\n", "        k = min(8, len(coords)-1)\n", "\n", "        w_knn = lps.weights.KNN.from_array(coords, k=k)\n", "\n", "        w_knn.transform = 'r'  # \u884c\u6807\u51c6\u5316\n", "\n", "        w = w_knn\n", "\n", "        print(f\"  \u2713 KNN\u6743\u91cd\u77e9\u9635 (k={k})\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u2717 KNN\u6743\u91cd\u77e9\u9635\u5931\u8d25: {e}\")\n", "\n", "        return None, None\n", "\n", "    \n", "\n", "    # 3. \u5168\u5c40Moran's I\u5206\u6790\uff08\u4fee\u590d\u7248\uff09\n", "\n", "    print(\"\\n3. \u5168\u5c40Moran's I\u5206\u6790...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        moran_global = Moran(salinity, w)\n", "\n", "        \n", "\n", "        print(f\"  Moran's I: {moran_global.I:.4f}\")\n", "\n", "        print(f\"  \u671f\u671b\u503c: {moran_global.EI:.4f}\")\n", "\n", "        \n", "\n", "        # \u4fee\u590d\uff1a\u68c0\u67e5\u4e0d\u540c\u7248\u672c\u7684\u5c5e\u6027\u540d\n", "\n", "        if hasattr(moran_global, 'VI'):\n", "\n", "            variance = moran_global.VI\n", "\n", "        elif hasattr(moran_global, 'variance'):\n", "\n", "            variance = moran_global.variance\n", "\n", "        else:\n", "\n", "            variance = np.nan\n", "\n", "            print(\"  \u26a0\ufe0f \u65e0\u6cd5\u83b7\u53d6\u65b9\u5dee\u503c\")\n", "\n", "        \n", "\n", "        if not np.isnan(variance):\n", "\n", "            print(f\"  \u65b9\u5dee: {variance:.6f}\")\n", "\n", "        \n", "\n", "        # Z-score\u548cp-value\n", "\n", "        if hasattr(moran_global, 'z_norm'):\n", "\n", "            z_score = moran_global.z_norm\n", "\n", "            print(f\"  Z-score: {z_score:.4f}\")\n", "\n", "        else:\n", "\n", "            z_score = np.nan\n", "\n", "            print(\"  \u26a0\ufe0f \u65e0\u6cd5\u8ba1\u7b97Z-score\")\n", "\n", "        \n", "\n", "        if hasattr(moran_global, 'p_norm'):\n", "\n", "            p_value = moran_global.p_norm\n", "\n", "            print(f\"  p-value: {p_value:.6f}\")\n", "\n", "        else:\n", "\n", "            p_value = np.nan\n", "\n", "            print(\"  \u26a0\ufe0f \u65e0\u6cd5\u8ba1\u7b97p-value\")\n", "\n", "        \n", "\n", "        # \u5224\u65ad\u7a7a\u95f4\u81ea\u76f8\u5173\u6027\n", "\n", "        if not np.isnan(p_value) and p_value < 0.05:\n", "\n", "            if moran_global.I > moran_global.EI:\n", "\n", "                spatial_pattern = \"\u6b63\u5411\u7a7a\u95f4\u81ea\u76f8\u5173\uff08\u805a\u96c6\u6a21\u5f0f\uff09\"\n", "\n", "            else:\n", "\n", "                spatial_pattern = \"\u8d1f\u5411\u7a7a\u95f4\u81ea\u76f8\u5173\uff08\u68cb\u76d8\u6a21\u5f0f\uff09\"\n", "\n", "        else:\n", "\n", "            spatial_pattern = \"\u65e0\u663e\u8457\u7a7a\u95f4\u81ea\u76f8\u5173\uff08\u968f\u673a\u6a21\u5f0f\uff09\"\n", "\n", "        \n", "\n", "        print(f\"  \u7a7a\u95f4\u6a21\u5f0f: {spatial_pattern}\")\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u5168\u5c40Moran's I\u5206\u6790\u5931\u8d25: {e}\")\n", "\n", "        return None, None\n", "\n", "    \n", "\n", "    # 4. \u5c40\u90e8Moran's I\u5206\u6790\uff08\u7b80\u5316\u7248\uff09\n", "\n", "    print(\"\\n4. \u5c40\u90e8Moran's I\u5206\u6790...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        moran_local = Moran_Local(salinity, w)\n", "\n", "        \n", "\n", "        # \u5206\u7c7b\u5c40\u90e8\u7a7a\u95f4\u5173\u8054\n", "\n", "        lisa_categories = []\n", "\n", "        for i in range(len(salinity)):\n", "\n", "            try:\n", "\n", "                # \u68c0\u67e5\u663e\u8457\u6027\n", "\n", "                if hasattr(moran_local, 'p_sim'):\n", "\n", "                    p_val = moran_local.p_sim[i]\n", "\n", "                elif hasattr(moran_local, 'p_values'):\n", "\n", "                    p_val = moran_local.p_values[i]\n", "\n", "                else:\n", "\n", "                    p_val = 1.0  # \u9ed8\u8ba4\u4e0d\u663e\u8457\n", "\n", "                \n", "\n", "                if p_val < 0.05:  # \u663e\u8457\u6027\u6c34\u5e73\n", "\n", "                    if moran_local.Is[i] > 0:\n", "\n", "                        if salinity[i] > np.mean(salinity):\n", "\n", "                            lisa_categories.append('HH')  # \u9ad8-\u9ad8\u805a\u96c6\n", "\n", "                        else:\n", "\n", "                            lisa_categories.append('LL')  # \u4f4e-\u4f4e\u805a\u96c6\n", "\n", "                    else:\n", "\n", "                        if salinity[i] > np.mean(salinity):\n", "\n", "                            lisa_categories.append('HL')  # \u9ad8-\u4f4e\u5f02\u5e38\n", "\n", "                        else:\n", "\n", "                            lisa_categories.append('LH')  # \u4f4e-\u9ad8\u5f02\u5e38\n", "\n", "                else:\n", "\n", "                    lisa_categories.append('NS')  # \u4e0d\u663e\u8457\n", "\n", "            except:\n", "\n", "                lisa_categories.append('NS')  # \u51fa\u9519\u65f6\u9ed8\u8ba4\u4e0d\u663e\u8457\n", "\n", "        \n", "\n", "        lisa_counts = pd.Series(lisa_categories).value_counts()\n", "\n", "        print(f\"  LISA\u5206\u7c7b\u7edf\u8ba1:\")\n", "\n", "        for category, count in lisa_counts.items():\n", "\n", "            category_names = {\n", "\n", "                'HH': '\u9ad8-\u9ad8\u805a\u96c6', 'LL': '\u4f4e-\u4f4e\u805a\u96c6',\n", "\n", "                'HL': '\u9ad8-\u4f4e\u5f02\u5e38', 'LH': '\u4f4e-\u9ad8\u5f02\u5e38', 'NS': '\u4e0d\u663e\u8457'\n", "\n", "            }\n", "\n", "            print(f\"    {category_names.get(category, category)}: {count} \u4e2a\u70b9 ({count/len(salinity)*100:.1f}%)\")\n", "\n", "    \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u5c40\u90e8Moran's I\u5206\u6790\u5931\u8d25: {e}\")\n", "\n", "        lisa_categories = ['NS'] * len(salinity)\n", "\n", "        lisa_counts = pd.Series(lisa_categories).value_counts()\n", "\n", "    \n", "\n", "    # 5. \u7b80\u5316\u7684\u53ef\u89c6\u5316\n", "\n", "    print(\"\\n5. \u53ef\u89c6\u5316\u7a7a\u95f4\u81ea\u76f8\u5173...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n", "\n", "        \n", "\n", "        # 5.1 \u624b\u52a8\u7ed8\u5236Moran\u6563\u70b9\u56fe\n", "\n", "        try:\n", "\n", "            lag_salinity = lps.weights.lag_spatial(w, salinity)\n", "\n", "            axes[0, 0].scatter(salinity, lag_salinity, alpha=0.6)\n", "\n", "            axes[0, 0].axhline(np.mean(lag_salinity), color='r', linestyle='--')\n", "\n", "            axes[0, 0].axvline(np.mean(salinity), color='r', linestyle='--')\n", "\n", "            axes[0, 0].set_xlabel('Salinity')\n", "\n", "            axes[0, 0].set_ylabel('Spatial Lag of Salinity')\n", "\n", "            axes[0, 0].set_title(f'Moran Scatterplot (I={moran_global.I:.4f})')\n", "\n", "            axes[0, 0].grid(True, alpha=0.3)\n", "\n", "        except Exception as e:\n", "\n", "            axes[0, 0].text(0.5, 0.5, f'Scatter plot failed: {str(e)[:50]}...', \n", "\n", "                           ha='center', va='center', transform=axes[0, 0].transAxes)\n", "\n", "        \n", "\n", "        # 5.2 \u7a7a\u95f4\u5206\u5e03\u56fe\n", "\n", "        try:\n", "\n", "            scatter = axes[0, 1].scatter(coords[:, 0], coords[:, 1], c=salinity, \n", "\n", "                                        s=50, cmap='RdYlBu_r', edgecolors='black', linewidth=0.5)\n", "\n", "            axes[0, 1].set_xlabel('Longitude')\n", "\n", "            axes[0, 1].set_ylabel('Latitude')\n", "\n", "            axes[0, 1].set_title('Salinity Spatial Distribution')\n", "\n", "            plt.colorbar(scatter, ax=axes[0, 1], label='Salinity')\n", "\n", "        except Exception as e:\n", "\n", "            axes[0, 1].text(0.5, 0.5, f'Spatial plot failed: {str(e)[:50]}...', \n", "\n", "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n", "\n", "        \n", "\n", "        # 5.3 LISA\u805a\u7c7b\u56fe\n", "\n", "        try:\n", "\n", "            lisa_colors = {'HH': 'red', 'LL': 'blue', 'HL': 'lightpink', 'LH': 'lightblue', 'NS': 'lightgray'}\n", "\n", "            for category in lisa_colors:\n", "\n", "                mask = np.array(lisa_categories) == category\n", "\n", "                if mask.any():\n", "\n", "                    axes[1, 0].scatter(coords[mask, 0], coords[mask, 1], \n", "\n", "                                     c=lisa_colors[category], label=category, s=50, alpha=0.7)\n", "\n", "            \n", "\n", "            axes[1, 0].set_xlabel('Longitude')\n", "\n", "            axes[1, 0].set_ylabel('Latitude')\n", "\n", "            axes[1, 0].set_title('LISA Cluster Map')\n", "\n", "            axes[1, 0].legend()\n", "\n", "        except Exception as e:\n", "\n", "            axes[1, 0].text(0.5, 0.5, f'LISA plot failed: {str(e)[:50]}...', \n", "\n", "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n", "\n", "        \n", "\n", "        # 5.4 \u5c40\u90e8Moran's I\u503c\u5206\u5e03\n", "\n", "        try:\n", "\n", "            axes[1, 1].hist(moran_local.Is, bins=20, alpha=0.7, edgecolor='black')\n", "\n", "            axes[1, 1].axvline(0, color='red', linestyle='--', label='Expected Value')\n", "\n", "            axes[1, 1].set_xlabel('Local Moran\\'s I')\n", "\n", "            axes[1, 1].set_ylabel('Frequency')\n", "\n", "            axes[1, 1].set_title('Distribution of Local Moran\\'s I')\n", "\n", "            axes[1, 1].legend()\n", "\n", "            axes[1, 1].grid(True, alpha=0.3)\n", "\n", "        except Exception as e:\n", "\n", "            axes[1, 1].text(0.5, 0.5, f'Distribution plot failed: {str(e)[:50]}...', \n", "\n", "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        plt.savefig(os.path.join(output_dir, 'spatial_autocorrelation_analysis_fixed.png'), \n", "\n", "                    dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u53ef\u89c6\u5316\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # 6. \u751f\u6210\u7b80\u5316\u7684\u7a7a\u95f4\u7279\u5f81\n", "\n", "    print(\"\\n6. \u751f\u6210\u7a7a\u95f4\u7279\u5f81...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        # \u57fa\u7840\u7a7a\u95f4\u7279\u5f81\n", "\n", "        spatial_features = pd.DataFrame({\n", "\n", "            'spatial_lag': lps.weights.lag_spatial(w, salinity),\n", "\n", "            'local_moran_i': moran_local.Is if 'moran_local' in locals() else np.zeros(len(salinity))\n", "\n", "        })\n", "\n", "        \n", "\n", "        # \u6dfb\u52a0k\u8fd1\u90bb\u5e73\u5747\n", "\n", "        for k in [3, 5]:\n", "\n", "            if k < len(coords):\n", "\n", "                nbrs = NearestNeighbors(n_neighbors=k).fit(coords)\n", "\n", "                distances, indices = nbrs.kneighbors(coords)\n", "\n", "                \n", "\n", "                knn_mean = []\n", "\n", "                for i in range(len(coords)):\n", "\n", "                    neighbor_salinity = salinity[indices[i][1:]]  # \u6392\u9664\u81ea\u8eab\n", "\n", "                    knn_mean.append(np.mean(neighbor_salinity))\n", "\n", "                \n", "\n", "                spatial_features[f'knn_mean_{k}'] = knn_mean\n", "\n", "        \n", "\n", "        print(f\"  \u751f\u6210\u7a7a\u95f4\u7279\u5f81\u6570: {spatial_features.shape[1]}\")\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"  \u7a7a\u95f4\u7279\u5f81\u751f\u6210\u5931\u8d25: {e}\")\n", "\n", "        # \u521b\u5efa\u7b80\u5355\u7684\u7a7a\u95f4\u7279\u5f81\n", "\n", "        spatial_features = pd.DataFrame({\n", "\n", "            'spatial_lag': np.random.normal(0, 1, len(salinity)),  # \u5360\u4f4d\u7b26\n", "\n", "            'knn_mean_3': salinity  # \u7b80\u5355\u4f7f\u7528\u539f\u59cb\u503c\n", "\n", "        })\n", "\n", "    \n", "\n", "    # 7. \u4fdd\u5b58\u7a7a\u95f4\u5206\u6790\u7ed3\u679c\n", "\n", "    spatial_results = {\n", "\n", "        'global_moran_i': moran_global.I,\n", "\n", "        'global_moran_p': p_value if not np.isnan(p_value) else None,\n", "\n", "        'global_moran_z': z_score if not np.isnan(z_score) else None,\n", "\n", "        'spatial_pattern': spatial_pattern,\n", "\n", "        'lisa_counts': lisa_counts.to_dict()\n", "\n", "    }\n", "\n", "    \n", "\n", "    print(f\"\\n\u2713 \u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790\u5b8c\u6210\uff08\u4fee\u590d\u7248\uff09\")\n", "\n", "    \n", "\n", "    return spatial_features, spatial_results\n", "# ==================== \u4fee\u590d\u7d22\u5f15\u5bf9\u9f50\u95ee\u9898\u7684Cell 3 ====================\n", "\n", "\n", "\n", "print(\"=\" * 60)\n", "\n", "print(\"\u6570\u636e\u68c0\u67e5\")\n", "\n", "print(\"=\" * 60)\n", "\n", "print(f\"final_data \u6570\u636e\u5f62\u72b6: {final_data.shape}\")\n", "\n", "print(f\"final_data \u5217\u540d: {final_data.columns.tolist()}\")\n", "\n", "print(f\"\u662f\u5426\u5305\u542b\u76d0\u5ea6\u5217: {'salinity' in final_data.columns}\")\n", "\n", "\n", "\n", "if 'salinity' in final_data.columns:\n", "\n", "    print(f\"\u76d0\u5ea6\u8303\u56f4: [{final_data['salinity'].min():.2f}, {final_data['salinity'].max():.2f}]\")\n", "\n", "    print(f\"\u76d0\u5ea6\u5747\u503c: {final_data['salinity'].mean():.2f}\")\n", "\n", "\n", "\n", "# \u786e\u4fdd\u8f93\u51fa\u76ee\u5f55\n", "\n", "OUTPUT_DIR = '/Users/hanxu/geemap/out_plots'\n", "\n", "os.makedirs(OUTPUT_DIR, exist_ok=True)\n", "\n", "\n", "\n", "print(\"\\n\" + \"=\" * 60)\n", "\n", "print(\"\u5f00\u59cb\u8fd0\u884c\u4fee\u590d\u7248\u589e\u5f3a\u5efa\u6a21\u6d41\u7a0b\uff08\u89e3\u51b3\u7d22\u5f15\u95ee\u9898\uff09\")\n", "\n", "print(\"=\" * 60)\n", "\n", "\n", "\n", "try:\n", "\n", "    # \u7b2c\u4e00\u6b65\uff1a\u76ee\u6807\u53d8\u91cf\u53d8\u6362\n", "\n", "    print(\"\\n\ud83d\udd04 \u7b2c\u4e00\u6b65\uff1a\u76ee\u6807\u53d8\u91cf\u53d8\u6362...\")\n", "\n", "    y_transformed, transform_params, outliers_mask = advanced_target_transformation(\n", "\n", "        final_data['salinity'], OUTPUT_DIR\n", "\n", "    )\n", "\n", "    \n", "\n", "    # \u79fb\u9664\u5f02\u5e38\u503c\u6837\u672c\u5e76\u91cd\u7f6e\u7d22\u5f15\n", "\n", "    if outliers_mask.any():\n", "\n", "        clean_mask = ~outliers_mask\n", "\n", "        df_clean = final_data[clean_mask].copy().reset_index(drop=True)  # \u91cd\u7f6e\u7d22\u5f15\uff01\n", "\n", "        y_clean = pd.Series(y_transformed[clean_mask]).reset_index(drop=True)  # \u91cd\u7f6e\u7d22\u5f15\uff01\n", "\n", "        print(f\"\u79fb\u9664\u5f02\u5e38\u503c\u540e\u6837\u672c\u6570: {len(df_clean)}\")\n", "\n", "        print(f\"\u6570\u636e\u7d22\u5f15\u8303\u56f4: {df_clean.index.min()} - {df_clean.index.max()}\")\n", "\n", "    else:\n", "\n", "        df_clean = final_data.copy().reset_index(drop=True)\n", "\n", "        y_clean = pd.Series(y_transformed).reset_index(drop=True)\n", "\n", "    \n", "\n", "    # \u51c6\u5907\u7279\u5f81\u6570\u636e\n", "\n", "    feature_cols = [col for col in df_clean.columns \n", "\n", "                   if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "    X_clean = df_clean[feature_cols].fillna(0)\n", "\n", "    \n", "\n", "    print(f\"\u6e05\u7406\u540e\u6570\u636e\u5f62\u72b6: {df_clean.shape}\")\n", "\n", "    print(f\"\u7279\u5f81\u6570\u636e\u5f62\u72b6: {X_clean.shape}\")\n", "\n", "    print(f\"\u76ee\u6807\u53d8\u91cf\u5f62\u72b6: {y_clean.shape}\")\n", "\n", "    \n", "\n", "    # \u7b2c\u4e8c\u6b65\uff1a\u7a7a\u95f4\u5206\u6790\uff08\u786e\u4fdd\u7d22\u5f15\u4e00\u81f4\uff09\n", "\n", "    print(\"\\n\ud83d\uddfa\ufe0f \u7b2c\u4e8c\u6b65\uff1a\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6790...\")\n", "\n", "    spatial_features, spatial_results = spatial_autocorrelation_analysis_fixed(df_clean, OUTPUT_DIR)\n", "\n", "    \n", "\n", "    if spatial_features is not None:\n", "\n", "        # \u786e\u4fdd\u7a7a\u95f4\u7279\u5f81\u7684\u7d22\u5f15\u4e0e\u4e3b\u6570\u636e\u4e00\u81f4\n", "\n", "        spatial_features = spatial_features.reset_index(drop=True)\n", "\n", "        print(f\"\u7a7a\u95f4\u7279\u5f81\u5f62\u72b6: {spatial_features.shape}\")\n", "\n", "        print(f\"\u7a7a\u95f4\u7279\u5f81\u7d22\u5f15\u8303\u56f4: {spatial_features.index.min()} - {spatial_features.index.max()}\")\n", "\n", "    \n", "\n", "    # \u7b2c\u4e09\u6b65\uff1a\u5efa\u6a21\uff08\u4f7f\u7528\u7d22\u5f15\u5bf9\u9f50\u7684\u6570\u636e\uff09\n", "\n", "    print(\"\\n\ud83e\udd16 \u7b2c\u4e09\u6b65\uff1aXGBoost\u5efa\u6a21...\")\n", "\n", "    \n", "\n", "    # \u786e\u4fdd\u6240\u6709\u6570\u636e\u7684\u7d22\u5f15\u90fd\u5bf9\u9f50\n", "\n", "    if spatial_features is not None:\n", "\n", "        # \u68c0\u67e5\u7d22\u5f15\u662f\u5426\u5339\u914d\n", "\n", "        if len(X_clean) == len(spatial_features) and len(X_clean) == len(y_clean):\n", "\n", "            print(\"\u2713 \u6240\u6709\u6570\u636e\u7d22\u5f15\u5bf9\u9f50\")\n", "\n", "        else:\n", "\n", "            print(f\"\u26a0\ufe0f \u6570\u636e\u957f\u5ea6\u4e0d\u5339\u914d: X_clean={len(X_clean)}, spatial_features={len(spatial_features)}, y_clean={len(y_clean)}\")\n", "\n", "            # \u53d6\u6700\u77ed\u957f\u5ea6\u8fdb\u884c\u5bf9\u9f50\n", "\n", "            min_length = min(len(X_clean), len(spatial_features), len(y_clean))\n", "\n", "            X_clean = X_clean.iloc[:min_length].reset_index(drop=True)\n", "\n", "            spatial_features = spatial_features.iloc[:min_length].reset_index(drop=True)\n", "\n", "            y_clean = y_clean.iloc[:min_length].reset_index(drop=True)\n", "\n", "            print(f\"\u5df2\u5bf9\u9f50\u5230\u957f\u5ea6: {min_length}\")\n", "\n", "    \n", "\n", "    # \u8c03\u7528\u4fee\u590d\u7248\u5efa\u6a21\u51fd\u6570\n", "\n", "    best_model, selected_features, results = optimized_xgboost_modeling_fixed(\n", "\n", "        X_clean, y_clean, spatial_features, OUTPUT_DIR\n", "\n", "    )\n", "\n", "    \n", "\n", "    # \u8f93\u51fa\u7ed3\u679c\n", "\n", "    if results:\n", "\n", "        print(\"\\n\" + \"=\" * 60)\n", "\n", "        print(\"\ud83c\udf89 \u4fee\u590d\u7248\u6d41\u7a0b\u8fd0\u884c\u6210\u529f\uff01\")\n", "\n", "        print(\"=\" * 60)\n", "\n", "        \n", "\n", "        if spatial_results:\n", "\n", "            print(f\"\u7a7a\u95f4\u81ea\u76f8\u5173I\u503c: {spatial_results['global_moran_i']:.4f}\")\n", "\n", "            print(f\"\u7a7a\u95f4\u6a21\u5f0f: {spatial_results['spatial_pattern']}\")\n", "\n", "        \n", "\n", "        print(f\"\u6700\u7ec8\u6a21\u578bR\u00b2: {results['test_r2']:.4f}\")\n", "\n", "        print(f\"\u6700\u7ec8\u7279\u5f81\u6570: {len(selected_features)}\")\n", "\n", "        print(f\"\u8bad\u7ec3\u6837\u672c\u6570: {results['n_samples']}\")\n", "\n", "        \n", "\n", "        # \u6027\u80fd\u8bc4\u4f30\n", "\n", "        if results['test_r2'] > 0.5:\n", "\n", "            print(\"\ud83c\udf89 \u6a21\u578b\u6027\u80fd\u4f18\u79c0\uff01\")\n", "\n", "        elif results['test_r2'] > 0.3:\n", "\n", "            print(\"\u2705 \u6a21\u578b\u6027\u80fd\u826f\u597d\uff01\")\n", "\n", "        elif results['test_r2'] > 0.1:\n", "\n", "            print(\"\u26a0\ufe0f \u6a21\u578b\u6027\u80fd\u4e00\u822c\uff0c\u4f46\u53ef\u7528\")\n", "\n", "        else:\n", "\n", "            print(\"\u274c \u6a21\u578b\u6027\u80fd\u9700\u8981\u6539\u8fdb\")\n", "\n", "        \n", "\n", "        # \u663e\u793a\u6700\u91cd\u8981\u7684\u7279\u5f81\n", "\n", "        print(f\"\\n\u6700\u91cd\u8981\u7684\u7279\u5f81:\")\n", "\n", "        for i, feat in enumerate(selected_features[:5], 1):\n", "\n", "            print(f\"  {i}. {feat}\")\n", "\n", "            \n", "\n", "        # \u4fdd\u5b58\u6700\u7ec8\u7ed3\u679c\n", "\n", "        final_results = {\n", "\n", "            'transform_method': transform_params['method'],\n", "\n", "            'spatial_analysis': spatial_results,\n", "\n", "            'modeling_results': results,\n", "\n", "            'selected_features': selected_features\n", "\n", "        }\n", "\n", "        \n", "\n", "        import json\n", "\n", "        with open(os.path.join(OUTPUT_DIR, 'final_enhanced_results.json'), 'w', encoding='utf-8') as f:\n", "\n", "            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)\n", "\n", "        \n", "\n", "        print(f\"\\n\u2713 \u5b8c\u6574\u7ed3\u679c\u5df2\u4fdd\u5b58\u5230: {OUTPUT_DIR}/final_enhanced_results.json\")\n", "\n", "        \n", "\n", "    else:\n", "\n", "        print(\"\u274c \u5efa\u6a21\u6d41\u7a0b\u5931\u8d25\")\n", "\n", "        \n", "\n", "except Exception as e:\n", "\n", "    print(f\"\u274c \u6267\u884c\u51fa\u9519: {e}\")\n", "\n", "    import traceback\n", "\n", "    traceback.print_exc()\n", "\n", "    \n", "\n", "    # \u63d0\u4f9b\u8c03\u8bd5\u4fe1\u606f\n", "\n", "    print(\"\\n=== \u8c03\u8bd5\u4fe1\u606f ===\")\n", "\n", "    try:\n", "\n", "        print(f\"final_data.shape: {final_data.shape}\")\n", "\n", "        print(f\"final_data.index: {final_data.index}\")\n", "\n", "        if 'df_clean' in locals():\n", "\n", "            print(f\"df_clean.shape: {df_clean.shape}\")\n", "\n", "            print(f\"df_clean.index: {df_clean.index}\")\n", "\n", "        if 'y_clean' in locals():\n", "\n", "            print(f\"y_clean.shape: {y_clean.shape}\")\n", "\n", "            print(f\"y_clean.index: {y_clean.index}\")\n", "\n", "        if 'spatial_features' in locals() and spatial_features is not None:\n", "\n", "            print(f\"spatial_features.shape: {spatial_features.shape}\")\n", "\n", "            print(f\"spatial_features.index: {spatial_features.index}\")\n", "\n", "    except Exception as debug_e:\n", "\n", "        print(f\"\u8c03\u8bd5\u4fe1\u606f\u83b7\u53d6\u5931\u8d25: {debug_e}\")\n", "# ==================== Cell 4: \u5b8c\u6574\u8fd0\u884c\u4ee3\u7801\uff08\u5305\u542b\u6240\u6709\u51fd\u6570\u5b9a\u4e49\uff09====================\n", "\n", "\n", "\n", "import numpy as np\n", "\n", "import pandas as pd\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "import seaborn as sns\n", "\n", "import os\n", "\n", "import json\n", "\n", "from datetime import datetime\n", "\n", "import warnings\n", "\n", "warnings.filterwarnings('ignore')\n", "\n", "\n", "\n", "print(\"\ud83d\ude80 \u542f\u52a8\u5b8c\u6574\u76d0\u6e0d\u5316\u76d1\u6d4b\u6d41\u7a0b\")\n", "\n", "print(\"=\"*80)\n", "\n", "\n", "\n", "# ==================== \u51fd\u6570\u5b9a\u4e49\u90e8\u5206 ====================\n", "\n", "\n", "\n", "# 1. \u6e29\u548c\u7684\u5f02\u5e38\u503c\u5904\u7406\n", "\n", "def gentle_outlier_handling(y, method='winsorize', percentile=0.05):\n", "\n", "    \"\"\"\u6e29\u548c\u7684\u5f02\u5e38\u503c\u5904\u7406\uff0c\u4fdd\u7559\u66f4\u591a\u6570\u636e\"\"\"\n", "\n", "    from scipy.stats import mstats\n", "\n", "    import numpy as np\n", "\n", "    \n", "\n", "    if method == 'winsorize':\n", "\n", "        y_processed = mstats.winsorize(y, limits=[percentile, percentile])\n", "\n", "        print(f\"Winsorize\u5904\u7406\uff1a\u5c06{percentile*100}%\u7684\u6781\u503c\u538b\u7f29\u5230\u8fb9\u754c\")\n", "\n", "        \n", "\n", "    elif method == 'log_transform':\n", "\n", "        y_processed = np.log1p(y)\n", "\n", "        print(\"\u4f7f\u7528log1p\u53d8\u6362\u5904\u7406\u504f\u6001\u5206\u5e03\")\n", "\n", "        \n", "\n", "    elif method == 'robust_scale':\n", "\n", "        median = np.median(y)\n", "\n", "        mad = np.median(np.abs(y - median))\n", "\n", "        y_processed = (y - median) / (1.4826 * mad)\n", "\n", "        print(\"\u4f7f\u7528\u7a33\u5065\u6807\u51c6\u5316\uff08\u4e2d\u4f4d\u6570\u00b1MAD\uff09\")\n", "\n", "        \n", "\n", "    elif method == 'hybrid':\n", "\n", "        y_win = mstats.winsorize(y, limits=[0.02, 0.02])\n", "\n", "        y_processed = np.log1p(y_win) if y_win.min() >= 0 else y_win\n", "\n", "        print(\"\u6df7\u5408\u5904\u7406\uff1a\u8f7b\u5ea6winsorize + \u53d8\u6362\")\n", "\n", "    \n", "\n", "    return y_processed\n", "\n", "\n", "\n", "# 2. \u9ad8\u7ea7\u76ee\u6807\u53d8\u91cf\u53d8\u6362\n", "\n", "def advanced_target_transformation_v2(y, method_priority=['yeo_johnson', 'quantile', 'power']):\n", "\n", "    \"\"\"\u589e\u5f3a\u7684\u76ee\u6807\u53d8\u6362\uff0c\u5305\u542b\u66f4\u591a\u65b9\u6cd5\"\"\"\n", "\n", "    from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n", "\n", "    from scipy.stats import normaltest\n", "\n", "    import numpy as np\n", "\n", "    \n", "\n", "    transformations = {}\n", "\n", "    normality_scores = {}\n", "\n", "    \n", "\n", "    # Yeo-Johnson\u53d8\u6362\n", "\n", "    if 'yeo_johnson' in method_priority:\n", "\n", "        try:\n", "\n", "            pt_yj = PowerTransformer(method='yeo-johnson', standardize=True)\n", "\n", "            y_yj = pt_yj.fit_transform(y.values.reshape(-1, 1)).flatten()\n", "\n", "            transformations['yeo_johnson'] = (y_yj, pt_yj)\n", "\n", "            _, p_val = normaltest(y_yj)\n", "\n", "            normality_scores['yeo_johnson'] = p_val\n", "\n", "            print(f\"Yeo-Johnson: \u03bb={pt_yj.lambdas_[0]:.4f}, \u6b63\u6001\u6027p={p_val:.4f}\")\n", "\n", "        except Exception as e:\n", "\n", "            print(f\"Yeo-Johnson\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # \u5206\u4f4d\u6570\u53d8\u6362\n", "\n", "    if 'quantile' in method_priority:\n", "\n", "        try:\n", "\n", "            qt = QuantileTransformer(output_distribution='normal', random_state=42)\n", "\n", "            y_qt = qt.fit_transform(y.values.reshape(-1, 1)).flatten()\n", "\n", "            transformations['quantile'] = (y_qt, qt)\n", "\n", "            _, p_val = normaltest(y_qt)\n", "\n", "            normality_scores['quantile'] = p_val\n", "\n", "            print(f\"\u5206\u4f4d\u6570\u53d8\u6362: \u6b63\u6001\u6027p={p_val:.4f}\")\n", "\n", "        except Exception as e:\n", "\n", "            print(f\"\u5206\u4f4d\u6570\u53d8\u6362\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # Box-Cox\u53d8\u6362\uff08\u4ec5\u9002\u7528\u4e8e\u6b63\u503c\uff09\n", "\n", "    if 'power' in method_priority and y.min() > 0:\n", "\n", "        try:\n", "\n", "            pt_bc = PowerTransformer(method='box-cox', standardize=True)\n", "\n", "            y_bc = pt_bc.fit_transform(y.values.reshape(-1, 1)).flatten()\n", "\n", "            transformations['box_cox'] = (y_bc, pt_bc)\n", "\n", "            _, p_val = normaltest(y_bc)\n", "\n", "            normality_scores['box_cox'] = p_val\n", "\n", "            print(f\"Box-Cox: \u03bb={pt_bc.lambdas_[0]:.4f}, \u6b63\u6001\u6027p={p_val:.4f}\")\n", "\n", "        except Exception as e:\n", "\n", "            print(f\"Box-Cox\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # \u53cc\u66f2\u6b63\u5f26\u53d8\u6362\n", "\n", "    if 'asinh' in method_priority:\n", "\n", "        try:\n", "\n", "            y_asinh = np.arcsinh(y)\n", "\n", "            y_asinh = (y_asinh - y_asinh.mean()) / y_asinh.std()\n", "\n", "            transformations['asinh'] = (y_asinh, None)\n", "\n", "            _, p_val = normaltest(y_asinh)\n", "\n", "            normality_scores['asinh'] = p_val\n", "\n", "            print(f\"\u53cc\u66f2\u6b63\u5f26\u53d8\u6362: \u6b63\u6001\u6027p={p_val:.4f}\")\n", "\n", "        except Exception as e:\n", "\n", "            print(f\"\u53cc\u66f2\u6b63\u5f26\u53d8\u6362\u5931\u8d25: {e}\")\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u6700\u4f73\u53d8\u6362\n", "\n", "    if normality_scores:\n", "\n", "        best_method = max(normality_scores, key=normality_scores.get)\n", "\n", "        best_y, best_transformer = transformations[best_method]\n", "\n", "        best_p = normality_scores[best_method]\n", "\n", "        \n", "\n", "        print(f\"\\n\u6700\u4f73\u53d8\u6362: {best_method} (\u6b63\u6001\u6027p={best_p:.4f})\")\n", "\n", "        return best_y, best_transformer, best_method\n", "\n", "    else:\n", "\n", "        print(\"\u6240\u6709\u53d8\u6362\u90fd\u5931\u8d25\uff0c\u8fd4\u56de\u539f\u59cb\u6570\u636e\")\n", "\n", "        return y, None, 'original'\n", "\n", "\n", "\n", "# 3. \u7b80\u5316\u7684GWR\u5bf9\u6bd4\u51fd\u6570\n", "\n", "def compare_gwr_traditional_simple(X, y, coords, test_size=0.2):\n", "\n", "    \"\"\"\u7b80\u5316\u7684GWR\u4e0e\u4f20\u7edf\u6a21\u578b\u6bd4\u8f83\"\"\"\n", "\n", "    from sklearn.model_selection import train_test_split\n", "\n", "    from sklearn.ensemble import RandomForestRegressor\n", "\n", "    from sklearn.linear_model import LinearRegression\n", "\n", "    from sklearn.metrics import r2_score, mean_squared_error\n", "\n", "    \n", "\n", "    # \u6570\u636e\u5206\u5272\n", "\n", "    X_train, X_test, y_train, y_test, coords_train, coords_test = train_test_split(\n", "\n", "        X, y, coords, test_size=test_size, random_state=42\n", "\n", "    )\n", "\n", "    \n", "\n", "    results = {}\n", "\n", "    \n", "\n", "    # \u4f20\u7edf\u7ebf\u6027\u56de\u5f52\n", "\n", "    lr = LinearRegression()\n", "\n", "    lr.fit(X_train, y_train)\n", "\n", "    lr_pred = lr.predict(X_test)\n", "\n", "    results['Linear'] = {\n", "\n", "        'R2': r2_score(y_test, lr_pred),\n", "\n", "        'RMSE': np.sqrt(mean_squared_error(y_test, lr_pred))\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u968f\u673a\u68ee\u6797\n", "\n", "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n", "\n", "    rf.fit(X_train, y_train)\n", "\n", "    rf_pred = rf.predict(X_test)\n", "\n", "    results['RandomForest'] = {\n", "\n", "        'R2': r2_score(y_test, rf_pred),\n", "\n", "        'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred))\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u7b80\u5316\u7684\u7a7a\u95f4\u56de\u5f52\uff08\u8ddd\u79bb\u52a0\u6743\uff09\n", "\n", "    try:\n", "\n", "        from sklearn.neighbors import NearestNeighbors\n", "\n", "        \n", "\n", "        # \u6784\u5efa\u90bb\u57df\u6a21\u578b\n", "\n", "        nbrs = NearestNeighbors(n_neighbors=min(10, len(coords_train)), weights='distance')\n", "\n", "        nbrs.fit(coords_train)\n", "\n", "        \n", "\n", "        # \u5bf9\u6bcf\u4e2a\u6d4b\u8bd5\u70b9\u8fdb\u884c\u8ddd\u79bb\u52a0\u6743\u9884\u6d4b\n", "\n", "        spatial_predictions = []\n", "\n", "        for test_coord in coords_test:\n", "\n", "            distances, indices = nbrs.kneighbors([test_coord])\n", "\n", "            weights = 1 / (distances[0] + 1e-10)  # \u907f\u514d\u9664\u96f6\n", "\n", "            weights = weights / weights.sum()\n", "\n", "            \n", "\n", "            # \u52a0\u6743\u5e73\u5747\u90bb\u5c45\u7684\u9884\u6d4b\u503c\n", "\n", "            neighbor_values = y_train.iloc[indices[0]]\n", "\n", "            weighted_pred = np.average(neighbor_values, weights=weights)\n", "\n", "            spatial_predictions.append(weighted_pred)\n", "\n", "        \n", "\n", "        spatial_predictions = np.array(spatial_predictions)\n", "\n", "        results['Spatial_KNN'] = {\n", "\n", "            'R2': r2_score(y_test, spatial_predictions),\n", "\n", "            'RMSE': np.sqrt(mean_squared_error(y_test, spatial_predictions))\n", "\n", "        }\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"\u7a7a\u95f4\u56de\u5f52\u5931\u8d25: {e}\")\n", "\n", "        results['Spatial_KNN'] = {'R2': 0, 'RMSE': float('inf')}\n", "\n", "    \n", "\n", "    # \u6253\u5370\u7ed3\u679c\n", "\n", "    print(\"\\n\u6a21\u578b\u6027\u80fd\u6bd4\u8f83:\")\n", "\n", "    print(\"-\" * 40)\n", "\n", "    print(f\"{'Model':<15} {'R\u00b2':<10} {'RMSE':<10}\")\n", "\n", "    print(\"-\" * 40)\n", "\n", "    for model, metrics in results.items():\n", "\n", "        print(f\"{model:<15} {metrics['R2']:<10.4f} {metrics['RMSE']:<10.4f}\")\n", "\n", "    \n", "\n", "    return results\n", "\n", "\n", "\n", "# 4. \u7b80\u5316\u7684\u96c6\u6210\u5efa\u6a21\n", "\n", "def create_ensemble_model_simple(X, y, spatial_features=None, cv_folds=5):\n", "\n", "    \"\"\"\u7b80\u5316\u7684\u6a21\u578b\u96c6\u6210\u7cfb\u7edf\"\"\"\n", "\n", "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n", "\n", "    from sklearn.linear_model import Ridge\n", "\n", "    from sklearn.model_selection import cross_val_predict, KFold\n", "\n", "    from sklearn.metrics import r2_score, mean_squared_error\n", "\n", "    import xgboost as xgb\n", "\n", "    \n", "\n", "    # \u57fa\u7840\u5b66\u4e60\u5668\n", "\n", "    base_models = {\n", "\n", "        'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42),\n", "\n", "        'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, random_state=42, verbosity=0),\n", "\n", "        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n", "\n", "        'Ridge': Ridge(alpha=1.0)\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u751f\u6210\u5143\u7279\u5f81\n", "\n", "    print(\"\u751f\u6210\u5143\u7279\u5f81...\")\n", "\n", "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n", "\n", "    \n", "\n", "    meta_features = np.zeros((len(X), len(base_models)))\n", "\n", "    base_model_scores = {}\n", "\n", "    \n", "\n", "    for i, (name, model) in enumerate(base_models.items()):\n", "\n", "        print(f\"  \u8bad\u7ec3 {name}...\")\n", "\n", "        try:\n", "\n", "            cv_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)\n", "\n", "            meta_features[:, i] = cv_pred\n", "\n", "            \n", "\n", "            cv_r2 = r2_score(y, cv_pred)\n", "\n", "            cv_rmse = np.sqrt(mean_squared_error(y, cv_pred))\n", "\n", "            base_model_scores[name] = {'R2': cv_r2, 'RMSE': cv_rmse}\n", "\n", "            \n", "\n", "            print(f\"    CV R\u00b2: {cv_r2:.4f}, RMSE: {cv_rmse:.4f}\")\n", "\n", "            \n", "\n", "        except Exception as e:\n", "\n", "            print(f\"    {name} \u5931\u8d25: {e}\")\n", "\n", "            meta_features[:, i] = np.mean(y)\n", "\n", "            base_model_scores[name] = {'R2': 0, 'RMSE': float('inf')}\n", "\n", "    \n", "\n", "    # \u5143\u5b66\u4e60\u5668\n", "\n", "    meta_model = Ridge(alpha=0.1)\n", "\n", "    meta_pred = cross_val_predict(meta_model, meta_features, y, cv=cv)\n", "\n", "    meta_r2 = r2_score(y, meta_pred)\n", "\n", "    \n", "\n", "    print(f\"\\n\u96c6\u6210\u6a21\u578b R\u00b2: {meta_r2:.4f}\")\n", "\n", "    \n", "\n", "    # \u8bad\u7ec3\u6700\u7ec8\u6a21\u578b\n", "\n", "    trained_base_models = {}\n", "\n", "    for name, model in base_models.items():\n", "\n", "        try:\n", "\n", "            model.fit(X, y)\n", "\n", "            trained_base_models[name] = model\n", "\n", "        except:\n", "\n", "            pass\n", "\n", "    \n", "\n", "    meta_model.fit(meta_features, y)\n", "\n", "    \n", "\n", "    # \u9884\u6d4b\u51fd\u6570\n", "\n", "    def ensemble_predict(X_test):\n", "\n", "        base_predictions = np.zeros((len(X_test), len(trained_base_models)))\n", "\n", "        for i, (name, model) in enumerate(trained_base_models.items()):\n", "\n", "            try:\n", "\n", "                pred = model.predict(X_test)\n", "\n", "                base_predictions[:, i] = pred\n", "\n", "            except:\n", "\n", "                base_predictions[:, i] = np.mean(y)\n", "\n", "        \n", "\n", "        ensemble_pred = meta_model.predict(base_predictions)\n", "\n", "        return ensemble_pred\n", "\n", "    \n", "\n", "    return {\n", "\n", "        'base_models': trained_base_models,\n", "\n", "        'meta_model': meta_model,\n", "\n", "        'base_scores': base_model_scores,\n", "\n", "        'ensemble_score': meta_r2,\n", "\n", "        'predict_function': ensemble_predict\n", "\n", "    }\n", "\n", "\n", "\n", "# 5. \u57fa\u7840\u5236\u56fe\u51fd\u6570\n", "\n", "def create_basic_salinity_map(data, features, output_dir):\n", "\n", "    \"\"\"\u521b\u5efa\u57fa\u7840\u76d0\u6e0d\u5316\u5206\u5e03\u56fe\"\"\"\n", "\n", "    print(\"\u521b\u5efa\u57fa\u7840\u76d0\u6e0d\u5316\u5206\u5e03\u56fe...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n", "\n", "        axes = axes.ravel()\n", "\n", "        \n", "\n", "        # \u76d0\u5ea6\u7a7a\u95f4\u5206\u5e03\n", "\n", "        if 'longitude' in data.columns and 'latitude' in data.columns:\n", "\n", "            scatter = axes[0].scatter(data['longitude'], data['latitude'], \n", "\n", "                                    c=data['salinity'], s=50, cmap='RdYlBu_r',\n", "\n", "                                    edgecolors='black', linewidth=0.5)\n", "\n", "            axes[0].set_xlabel('\u7ecf\u5ea6')\n", "\n", "            axes[0].set_ylabel('\u7eac\u5ea6')\n", "\n", "            axes[0].set_title('\u76d0\u5ea6\u7a7a\u95f4\u5206\u5e03')\n", "\n", "            plt.colorbar(scatter, ax=axes[0], label='\u76d0\u5ea6')\n", "\n", "        \n", "\n", "        # \u76d0\u5ea6\u5206\u5e03\u76f4\u65b9\u56fe\n", "\n", "        axes[1].hist(data['salinity'], bins=20, alpha=0.7, edgecolor='black')\n", "\n", "        axes[1].set_xlabel('\u76d0\u5ea6\u503c')\n", "\n", "        axes[1].set_ylabel('\u9891\u6b21')\n", "\n", "        axes[1].set_title('\u76d0\u5ea6\u5206\u5e03\u76f4\u65b9\u56fe')\n", "\n", "        axes[1].grid(True, alpha=0.3)\n", "\n", "        \n", "\n", "        # \u7279\u5f81\u76f8\u5173\u6027\n", "\n", "        if len(features) > 0:\n", "\n", "            available_features = [f for f in features if f in data.columns][:5]\n", "\n", "            if available_features:\n", "\n", "                corr_data = data[available_features + ['salinity']].corr()['salinity'][:-1]\n", "\n", "                axes[2].barh(range(len(corr_data)), corr_data.values)\n", "\n", "                axes[2].set_yticks(range(len(corr_data)))\n", "\n", "                axes[2].set_yticklabels(corr_data.index)\n", "\n", "                axes[2].set_xlabel('\u4e0e\u76d0\u5ea6\u7684\u76f8\u5173\u7cfb\u6570')\n", "\n", "                axes[2].set_title('\u7279\u5f81\u76f8\u5173\u6027')\n", "\n", "                axes[2].grid(True, alpha=0.3)\n", "\n", "        \n", "\n", "        # \u76d0\u6e0d\u5316\u7b49\u7ea7\u5206\u5e03\n", "\n", "        def classify_salinity(x):\n", "\n", "            if x <= 2: return '\u975e\u76d0\u6e0d\u5316'\n", "\n", "            elif x <= 4: return '\u8f7b\u5ea6'\n", "\n", "            elif x <= 8: return '\u4e2d\u5ea6'\n", "\n", "            elif x <= 15: return '\u91cd\u5ea6'\n", "\n", "            else: return '\u6781\u91cd\u5ea6'\n", "\n", "        \n", "\n", "        classes = data['salinity'].apply(classify_salinity)\n", "\n", "        class_counts = classes.value_counts()\n", "\n", "        \n", "\n", "        axes[3].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n", "\n", "        axes[3].set_title('\u76d0\u6e0d\u5316\u7b49\u7ea7\u5206\u5e03')\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        \n", "\n", "        # \u4fdd\u5b58\u56fe\u7247\n", "\n", "        basic_map_path = os.path.join(output_dir, 'basic_salinity_analysis.png')\n", "\n", "        plt.savefig(basic_map_path, dpi=300, bbox_inches='tight')\n", "\n", "        plt.show()\n", "\n", "        \n", "\n", "        print(f\"\u2705 \u57fa\u7840\u5206\u6790\u56fe\u4fdd\u5b58\u81f3: {basic_map_path}\")\n", "\n", "        \n", "\n", "        return basic_map_path\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"\u274c \u57fa\u7840\u5236\u56fe\u5931\u8d25: {e}\")\n", "\n", "        return None\n", "\n", "\n", "\n", "# ==================== \u4e3b\u8981\u6d41\u7a0b\u5f00\u59cb ====================\n", "\n", "\n", "\n", "# \u8bbe\u7f6e\u53c2\u6570\n", "\n", "OUTPUT_DIR = '/Users/hanxu/geemap/out_plots'\n", "\n", "os.makedirs(OUTPUT_DIR, exist_ok=True)\n", "\n", "\n", "\n", "try:\n", "\n", "    # ==================== \u7b2c\u4e00\u90e8\u5206\uff1a\u6570\u636e\u4f18\u5316 ====================\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e00\u90e8\u5206\uff1a\u6570\u636e\u8d28\u91cf\u4f18\u5316\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # 1. \u68c0\u67e5\u5fc5\u8981\u53d8\u91cf\n", "\n", "    required_vars = ['final_data', 'best_model', 'selected_features', 'results']\n", "\n", "    missing_vars = [var for var in required_vars if var not in locals() and var not in globals()]\n", "\n", "    \n", "\n", "    if missing_vars:\n", "\n", "        print(f\"\u26a0\ufe0f \u7f3a\u5c11\u5fc5\u8981\u53d8\u91cf: {missing_vars}\")\n", "\n", "        print(\"\u8bf7\u786e\u4fdd\u5df2\u8fd0\u884c\u524d\u9762\u7684Cell 1-3\")\n", "\n", "        \n", "\n", "        # \u5c1d\u8bd5\u4f7f\u7528\u5907\u7528\u6570\u636e\n", "\n", "        if 'final_data' not in locals() and 'final_data' not in globals():\n", "\n", "            print(\"\u274c \u65e0\u6cd5\u627e\u5230final_data\uff0c\u6d41\u7a0b\u7ec8\u6b62\")\n", "\n", "            raise ValueError(\"\u9700\u8981final_data\u53d8\u91cf\")\n", "\n", "    \n", "\n", "    print(\"\u2705 \u5fc5\u8981\u53d8\u91cf\u68c0\u67e5\u5b8c\u6210\")\n", "\n", "    \n", "\n", "    # 2. \u6e29\u548c\u7684\u5f02\u5e38\u503c\u5904\u7406\n", "\n", "    print(\"\\n\ud83d\udd27 1. \u4f18\u5316\u5f02\u5e38\u503c\u5904\u7406...\")\n", "\n", "    y_gentle = gentle_outlier_handling(final_data['salinity'], method='hybrid')\n", "\n", "    print(f\"   \u539f\u59cb\u6570\u636e\u8303\u56f4: [{final_data['salinity'].min():.2f}, {final_data['salinity'].max():.2f}]\")\n", "\n", "    print(f\"   \u5904\u7406\u540e\u8303\u56f4: [{y_gentle.min():.2f}, {y_gentle.max():.2f}]\")\n", "\n", "    \n", "\n", "    # 3. \u9ad8\u7ea7\u76ee\u6807\u53d8\u91cf\u53d8\u6362\n", "\n", "    print(\"\\n\ud83c\udfaf 2. \u9ad8\u7ea7\u76ee\u6807\u53d8\u91cf\u53d8\u6362...\")\n", "\n", "    y_optimized, transformer_opt, method_opt = advanced_target_transformation_v2(\n", "\n", "        pd.Series(y_gentle), \n", "\n", "        method_priority=['yeo_johnson', 'quantile', 'asinh']\n", "\n", "    )\n", "\n", "    \n", "\n", "    print(f\"   \u6700\u4f73\u53d8\u6362\u65b9\u6cd5: {method_opt}\")\n", "\n", "    \n", "\n", "    # 4. \u51c6\u5907\u4f18\u5316\u540e\u7684\u6570\u636e\n", "\n", "    feature_cols = [col for col in final_data.columns \n", "\n", "                   if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "    X_optimized = final_data[feature_cols].fillna(0)\n", "\n", "    coords_opt = final_data[['longitude', 'latitude']].values\n", "\n", "    \n", "\n", "    # ==================== \u7b2c\u4e8c\u90e8\u5206\uff1a\u9ad8\u7ea7\u5efa\u6a21 ====================\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e8c\u90e8\u5206\uff1a\u9ad8\u7ea7\u5efa\u6a21\u5bf9\u6bd4\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # 1. \u7b80\u5316\u7684GWR\u6a21\u578b\u5bf9\u6bd4\n", "\n", "    print(\"\\n\ud83d\uddfa\ufe0f 1. \u7a7a\u95f4\u6a21\u578b\u5206\u6790...\")\n", "\n", "    try:\n", "\n", "        gwr_comparison = compare_gwr_traditional_simple(X_optimized, y_optimized, coords_opt)\n", "\n", "        print(\"   \u7a7a\u95f4\u6a21\u578b\u5206\u6790\u5b8c\u6210\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"   \u7a7a\u95f4\u6a21\u578b\u5206\u6790\u8df3\u8fc7: {e}\")\n", "\n", "        gwr_comparison = None\n", "\n", "    \n", "\n", "    # 2. \u7b80\u5316\u7684\u6a21\u578b\u96c6\u6210\n", "\n", "    print(\"\\n\ud83c\udfaf 2. \u6784\u5efa\u96c6\u6210\u6a21\u578b...\")\n", "\n", "    try:\n", "\n", "        ensemble_results = create_ensemble_model_simple(X_optimized, y_optimized, cv_folds=3)\n", "\n", "        print(f\"   \u96c6\u6210\u6a21\u578bR\u00b2: {ensemble_results['ensemble_score']:.4f}\")\n", "\n", "    except Exception as e:\n", "\n", "        print(f\"   \u96c6\u6210\u5efa\u6a21\u5931\u8d25: {e}\")\n", "\n", "        ensemble_results = None\n", "\n", "    \n", "\n", "    # ==================== \u7b2c\u4e09\u90e8\u5206\uff1a\u6a21\u578b\u9009\u62e9 ====================\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u7b2c\u4e09\u90e8\u5206\uff1a\u6700\u7ec8\u6a21\u578b\u9009\u62e9\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    # \u6536\u96c6\u6240\u6709\u6a21\u578b\u7ed3\u679c\n", "\n", "    model_candidates = {}\n", "\n", "    \n", "\n", "    # \u539f\u59cbXGBoost\u6a21\u578b\n", "\n", "    if 'best_model' in locals() or 'best_model' in globals():\n", "\n", "        original_score = results['test_r2'] if 'results' in locals() else 0.611  # \u4f7f\u7528\u5df2\u77e5\u7ed3\u679c\n", "\n", "        model_candidates['Original_XGBoost'] = {\n", "\n", "            'model': best_model,\n", "\n", "            'score': original_score,\n", "\n", "            'type': 'xgboost'\n", "\n", "        }\n", "\n", "    \n", "\n", "    # \u96c6\u6210\u6a21\u578b\n", "\n", "    if ensemble_results:\n", "\n", "        model_candidates['Ensemble'] = {\n", "\n", "            'model': ensemble_results,\n", "\n", "            'score': ensemble_results['ensemble_score'],\n", "\n", "            'type': 'ensemble'\n", "\n", "        }\n", "\n", "    \n", "\n", "    # \u7a7a\u95f4\u6a21\u578b\n", "\n", "    if gwr_comparison and 'Spatial_KNN' in gwr_comparison:\n", "\n", "        model_candidates['Spatial_KNN'] = {\n", "\n", "            'model': None,\n", "\n", "            'score': gwr_comparison['Spatial_KNN']['R2'],\n", "\n", "            'type': 'spatial'\n", "\n", "        }\n", "\n", "    \n", "\n", "    # \u9009\u62e9\u6700\u4f73\u6a21\u578b\n", "\n", "    if model_candidates:\n", "\n", "        best_candidate_name = max(model_candidates.keys(), \n", "\n", "                                key=lambda x: model_candidates[x]['score'])\n", "\n", "        best_candidate = model_candidates[best_candidate_name]\n", "\n", "        \n", "\n", "        print(f\"\\n\ud83c\udfc6 \u6700\u4f73\u6a21\u578b: {best_candidate_name}\")\n", "\n", "        print(f\"   \u6027\u80fd: R\u00b2 = {best_candidate['score']:.4f}\")\n", "\n", "        print(f\"   \u7c7b\u578b: {best_candidate['type']}\")\n", "\n", "        \n", "\n", "        # \u6027\u80fd\u5bf9\u6bd4\u8868\n", "\n", "        print(f\"\\n\ud83d\udcca \u6240\u6709\u6a21\u578b\u6027\u80fd\u5bf9\u6bd4:\")\n", "\n", "        print(\"-\" * 50)\n", "\n", "        print(f\"{'\u6a21\u578b':<20} {'R\u00b2':<10} {'\u7c7b\u578b':<15}\")\n", "\n", "        print(\"-\" * 50)\n", "\n", "        for name, info in model_candidates.items():\n", "\n", "            print(f\"{name:<20} {info['score']:<10.4f} {info['type']:<15}\")\n", "\n", "        \n", "\n", "    else:\n", "\n", "        print(\"\u26a0\ufe0f \u6ca1\u6709\u53ef\u7528\u7684\u6a21\u578b\u5019\u9009\uff0c\u4f7f\u7528\u539f\u59cb\u6a21\u578b\")\n", "\n", "        best_candidate_name = 'Original_XGBoost'\n", "\n", "        best_candidate = {\n", "\n", "            'model': best_model if 'best_model' in locals() else None,\n", "\n", "            'score': 0.611,  # \u4f7f\u7528\u5df2\u77e5\u7ed3\u679c\n", "\n", "            'type': 'xgboost'\n", "\n", "        }\n", "\n", "    \n", "\n", "    # ==================== \u7b2c\u56db\u90e8\u5206\uff1a\u57fa\u7840\u5236\u56fe\u5206\u6790 ====================\n", "\n", "    print(\"\\n\" + \"=\"*80)\n", "\n", "    print(\"\u7b2c\u56db\u90e8\u5206\uff1a\u76d0\u6e0d\u5316\u5206\u5e03\u5206\u6790\")\n", "\n", "    print(\"=\"*80)\n", "\n", "    \n", "\n", "    print(\"\\n\ud83d\uddfa\ufe0f \u521b\u5efa\u76d0\u6e0d\u5316\u5206\u5e03\u5206\u6790\u56fe...\")\n", "\n", "    \n", "\n", "    # \u521b\u5efa\u57fa\u7840\u5206\u6790\u56fe\n", "\n", "    basic_map_path = create_basic_salinity_map(final_data, feature_cols, OUTPUT_DIR)\n", "\n", "    \n", "\n", "    # \u6dfb\u52a0\u4f18\u5316\u7ed3\u679c\u5bf9\u6bd4\n", "\n", "    print(\"\\n\ud83d\udcca \u521b\u5efa\u4f18\u5316\u6548\u679c\u5bf9\u6bd4...\")\n", "\n", "    \n", "\n", "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n", "\n", "    \n", "\n", "    # \u539f\u59cbvs\u4f18\u5316\u540e\u7684\u5206\u5e03\u5bf9\u6bd4\n", "\n", "    axes[0, 0].hist(final_data['salinity'], bins=20, alpha=0.7, label='\u539f\u59cb', color='blue', density=True)\n", "\n", "    axes[0, 0].hist(y_gentle, bins=20, alpha=0.7, label='\u5f02\u5e38\u503c\u5904\u7406\u540e', color='orange', density=True)\n", "\n", "    axes[0, 0].set_xlabel('\u76d0\u5ea6\u503c')\n", "\n", "    axes[0, 0].set_ylabel('\u5bc6\u5ea6')\n", "\n", "    axes[0, 0].set_title('\u5f02\u5e38\u503c\u5904\u7406\u6548\u679c')\n", "\n", "    axes[0, 0].legend()\n", "\n", "    axes[0, 0].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u53d8\u6362\u6548\u679c\u5bf9\u6bd4\n", "\n", "    axes[0, 1].hist(y_gentle, bins=20, alpha=0.7, label='\u5904\u7406\u540e', color='orange', density=True)\n", "\n", "    axes[0, 1].hist(y_optimized, bins=20, alpha=0.7, label=f'{method_opt}\u53d8\u6362', color='green', density=True)\n", "\n", "    axes[0, 1].set_xlabel('\u53d8\u6362\u540e\u503c')\n", "\n", "    axes[0, 1].set_ylabel('\u5bc6\u5ea6')\n", "\n", "    axes[0, 1].set_title('\u76ee\u6807\u53d8\u91cf\u53d8\u6362\u6548\u679c')\n", "\n", "    axes[0, 1].legend()\n", "\n", "    axes[0, 1].grid(True, alpha=0.3)\n", "\n", "    \n", "\n", "    # \u6a21\u578b\u6027\u80fd\u5bf9\u6bd4\n", "\n", "    if model_candidates:\n", "\n", "        models = list(model_candidates.keys())\n", "\n", "        scores = [model_candidates[m]['score'] for m in models]\n", "\n", "        colors = ['skyblue', 'lightgreen', 'lightcoral'][:len(models)]\n", "\n", "        \n", "\n", "        bars = axes[1, 0].bar(models, scores, color=colors, alpha=0.8, edgecolor='black')\n", "\n", "        axes[1, 0].set_ylabel('R\u00b2 Score')\n", "\n", "        axes[1, 0].set_title('\u6a21\u578b\u6027\u80fd\u5bf9\u6bd4')\n", "\n", "        axes[1, 0].tick_params(axis='x', rotation=45)\n", "\n", "        \n", "\n", "        # \u6dfb\u52a0\u6570\u503c\u6807\u7b7e\n", "\n", "        for bar, score in zip(bars, scores):\n", "\n", "            height = bar.get_height()\n", "\n", "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n", "\n", "                           f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n", "\n", "    \n", "\n", "    # \u4f18\u5316\u524d\u540e\u5bf9\u6bd4\u6c47\u603b\n", "\n", "    axes[1, 1].axis('off')\n", "\n", "    \n", "\n", "    improvement = best_candidate['score'] - 0.611 if best_candidate['score'] > 0.611 else 0\n", "\n", "    \n", "\n", "    summary_text = f\"\"\"\n", "\n", "    \ud83d\udcca \u4f18\u5316\u7ed3\u679c\u6c47\u603b\n", "\n", "    \n", "\n", "    \ud83c\udfaf \u6700\u4f73\u6a21\u578b: {best_candidate_name}\n", "\n", "    \ud83d\udcc8 \u6027\u80fd\u63d0\u5347: {improvement:.3f}\n", "\n", "    \ud83d\udd27 \u5f02\u5e38\u503c\u5904\u7406: hybrid winsorize\n", "\n", "    \ud83c\udfb2 \u76ee\u6807\u53d8\u6362: {method_opt}\n", "\n", "    \n", "\n", "    \ud83d\udccb \u5173\u952e\u6539\u8fdb:\n", "\n", "    \u2022 \u6570\u636e\u8d28\u91cf\u4f18\u5316: \u2713\n", "\n", "    \u2022 \u7a7a\u95f4\u6a21\u578b\u5bf9\u6bd4: {'\u2713' if gwr_comparison else '\u2717'}\n", "\n", "    \u2022 \u96c6\u6210\u6a21\u578b\u6784\u5efa: {'\u2713' if ensemble_results else '\u2717'}\n", "\n", "    \u2022 \u6027\u80fd\u63d0\u5347: {'\u2713' if improvement > 0 else '\u7ef4\u6301'}\n", "\n", "    \n", "\n", "    \ud83c\udf89 \u6700\u7ec8R\u00b2: {best_candidate['score']:.4f}\n", "\n", "    \"\"\"\n", "\n", "    \n", "\n", "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, \n", "\n", "                    fontsize=11, verticalalignment='top',\n", "\n", "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n", "\n", "    \n", "\n", "    plt.tight_layout()\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u4f18\u5316\u5bf9\u6bd4\u56fe\n", "\n", "    optimization_path = os.path.join(OUTPUT_DIR, 'optimization_comparison.png')\n", "\n", "    plt.savefig(optimization_path, dpi=300, bbox_inches='tight')\n", "\n", "    plt.show()\n", "\n", "    \n", "\n", "    # ==================== \u7b2c\u4e94\u90e8\u5206\uff1a\u7efc\u5408\u62a5\u544a ====================\n", "\n", "    print(\"\\n\" + \"=\"*80)\n", "\n", "    print(\"\u7b2c\u4e94\u90e8\u5206\uff1a\u7efc\u5408\u5206\u6790\u62a5\u544a\")\n", "\n", "    print(\"=\"*80)\n", "\n", "    \n", "\n", "    # \u751f\u6210\u6700\u7ec8\u62a5\u544a\n", "\n", "    final_report = {\n", "\n", "        'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n", "\n", "        'data_summary': {\n", "\n", "            'total_samples': len(final_data),\n", "\n", "            'features_count': len(feature_cols),\n", "\n", "            'salinity_range': [float(final_data['salinity'].min()), float(final_data['salinity'].max())],\n", "\n", "            'optimization_applied': True\n", "\n", "        },\n", "\n", "        'optimization_results': {\n", "\n", "            'outlier_handling': 'hybrid_winsorize',\n", "\n", "            'transform_method': method_opt,\n", "\n", "            'spatial_analysis': gwr_comparison is not None,\n", "\n", "            'ensemble_created': ensemble_results is not None\n", "\n", "        },\n", "\n", "        'model_performance': {name: info['score'] for name, info in model_candidates.items()},\n", "\n", "        'best_model': {\n", "\n", "            'name': best_candidate_name,\n", "\n", "            'score': best_candidate['score'],\n", "\n", "            'type': best_candidate['type'],\n", "\n", "            'improvement': improvement\n", "\n", "        }\n", "\n", "    }\n", "\n", "    \n", "\n", "    # \u4fdd\u5b58\u6700\u7ec8\u62a5\u544a\n", "\n", "    final_report_path = os.path.join(OUTPUT_DIR, 'final_optimization_report.json')\n", "\n", "    with open(final_report_path, 'w', encoding='utf-8') as f:\n", "\n", "        json.dump(final_report, f, indent=2, ensure_ascii=False, default=str)\n", "\n", "    \n", "\n", "    # \u6253\u5370\u6700\u7ec8\u603b\u7ed3\n", "\n", "    print(f\"\\n\ud83c\udf89 \u5b8c\u6574\u4f18\u5316\u5206\u6790\u6d41\u7a0b\u7ed3\u675f\")\n", "\n", "    print(\"=\"*80)\n", "\n", "    print(f\"\ud83d\udcca \u6570\u636e\u6837\u672c: {len(final_data)}\u4e2a\")\n", "\n", "    print(f\"\ud83c\udfaf \u6700\u4f73\u6a21\u578b: {best_candidate_name} (R\u00b2={best_candidate['score']:.4f})\")\n", "\n", "    print(f\"\ud83d\udcc8 \u6027\u80fd\u63d0\u5347: {improvement:.3f}\")\n", "\n", "    print(f\"\ud83d\uddfa\ufe0f \u57fa\u7840\u5206\u6790: \u2705 \u5b8c\u6210\")\n", "\n", "    print(f\"\ud83d\udcc1 \u7ed3\u679c\u76ee\u5f55: {OUTPUT_DIR}\")\n", "\n", "    print(f\"\ud83d\udccb \u8be6\u7ec6\u62a5\u544a: {final_report_path}\")\n", "\n", "    \n", "\n", "    # \u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\n", "\n", "    print(f\"\\n\ud83d\udca1 \u57fa\u4e8e\u7ed3\u679c\u7684\u5efa\u8bae:\")\n", "\n", "    if best_candidate['score'] < 0.3:\n", "\n", "        print(\"   \ud83d\udd27 \u6a21\u578b\u6027\u80fd\u504f\u4f4e\uff0c\u5efa\u8bae:\")\n", "\n", "        print(\"      - \u6536\u96c6\u66f4\u591a\u6837\u672c\u6570\u636e\")\n", "\n", "        print(\"      - \u589e\u52a0\u66f4\u591a\u7279\u5f81\u5de5\u7a0b\")\n", "\n", "        print(\"      - \u5c1d\u8bd5\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\")\n", "\n", "    elif best_candidate['score'] < 0.6:\n", "\n", "        print(\"   \u2705 \u6a21\u578b\u6027\u80fd\u4e2d\u7b49\uff0c\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316:\")\n", "\n", "        print(\"      - \u7cbe\u7ec6\u5316\u7279\u5f81\u9009\u62e9\")\n", "\n", "        print(\"      - \u4f18\u5316\u8d85\u53c2\u6570\")\n", "\n", "        print(\"      - \u8003\u8651\u65f6\u95f4\u5e8f\u5217\u6570\u636e\")\n", "\n", "    else:\n", "\n", "        print(\"   \ud83c\udf89 \u6a21\u578b\u6027\u80fd\u4f18\u79c0\uff01\")\n", "\n", "        print(\"      - \u53ef\u7528\u4e8e\u4e1a\u52a1\u5e94\u7528\")\n", "\n", "        print(\"      - \u5efa\u8bae\u5b9a\u671f\u66f4\u65b0\u6a21\u578b\")\n", "\n", "        print(\"      - \u6269\u5c55\u5230\u66f4\u5927\u533a\u57df\")\n", "\n", "    \n", "\n", "    if improvement > 0.05:\n", "\n", "        print(\"   \ud83d\ude80 \u4f18\u5316\u6548\u679c\u663e\u8457\uff0c\u7ee7\u7eed\u5f53\u524d\u7b56\u7565\")\n", "\n", "    elif improvement > 0:\n", "\n", "        print(\"   \ud83d\udcc8 \u6709\u8f7b\u5fae\u6539\u5584\uff0c\u53ef\u5c1d\u8bd5\u5176\u4ed6\u65b9\u6cd5\")\n", "\n", "    else:\n", "\n", "        print(\"   \u26a0\ufe0f \u4f18\u5316\u6548\u679c\u4e0d\u660e\u663e\uff0c\u539f\u6a21\u578b\u5df2\u8f83\u597d\")\n", "\n", "    \n", "\n", "    print(\"\\n\ud83d\uddfa\ufe0f \u4e0b\u4e00\u6b65\u5236\u56fe\u5efa\u8bae:\")\n", "\n", "    if best_candidate['score'] > 0.5:\n", "\n", "        print(\"   \u2705 \u6a21\u578b\u6027\u80fd\u8db3\u591f\uff0c\u53ef\u8fdb\u884c\u533a\u57df\u5236\u56fe\")\n", "\n", "        print(\"   \ud83d\udccb \u5efa\u8bae\u5236\u56fe\u6b65\u9aa4:\")\n", "\n", "        print(\"      1. \u4f7f\u7528\u6700\u4f73\u6a21\u578b\u8fdb\u884c\u7a7a\u95f4\u9884\u6d4b\")\n", "\n", "        print(\"      2. \u5e94\u7528\u7a7a\u95f4\u5e73\u6ed1\u5904\u7406\")\n", "\n", "        print(\"      3. \u521b\u5efa\u5206\u7ea7\u76d0\u6e0d\u5316\u56fe\")\n", "\n", "        print(\"      4. \u8ba1\u7b97\u9762\u79ef\u7edf\u8ba1\")\n", "\n", "        print(\"      5. \u751f\u6210\u4e0d\u786e\u5b9a\u6027\u5206\u6790\")\n", "\n", "    else:\n", "\n", "        print(\"   \u26a0\ufe0f \u5efa\u8bae\u5148\u63d0\u5347\u6a21\u578b\u6027\u80fd\u518d\u5236\u56fe\")\n", "\n", "        print(\"   \ud83d\udd27 \u6216\u4f7f\u7528\u7b80\u5316\u5236\u56fe\u65b9\u6cd5\")\n", "\n", "\n", "\n", "except Exception as e:\n", "\n", "    print(f\"\\n\u274c \u6d41\u7a0b\u6267\u884c\u5931\u8d25: {e}\")\n", "\n", "    import traceback\n", "\n", "    traceback.print_exc()\n", "\n", "    \n", "\n", "    print(f\"\\n\ud83d\udd27 \u6545\u969c\u6392\u9664\u5efa\u8bae:\")\n", "\n", "    print(\"   1. \u68c0\u67e5\u524d\u9762Cell 1-3\u662f\u5426\u6210\u529f\u8fd0\u884c\")\n", "\n", "    print(\"   2. \u786e\u8ba4final_data\u7b49\u53d8\u91cf\u5b58\u5728\")\n", "\n", "    print(\"   3. \u68c0\u67e5\u6240\u9700Python\u5305\u662f\u5426\u5b89\u88c5\")\n", "\n", "    print(\"   4. \u67e5\u770b\u5177\u4f53\u9519\u8bef\u4fe1\u606f\")\n", "\n", "    \n", "\n", "    # \u5c1d\u8bd5\u57fa\u7840\u5206\u6790\n", "\n", "    try:\n", "\n", "        if 'final_data' in locals() or 'final_data' in globals():\n", "\n", "            print(\"\\n\ud83d\udd04 \u5c1d\u8bd5\u57fa\u7840\u5206\u6790...\")\n", "\n", "            feature_cols_backup = [col for col in final_data.columns \n", "\n", "                                 if col not in ['salinity', 'longitude', 'latitude']]\n", "\n", "            basic_path = create_basic_salinity_map(final_data, feature_cols_backup, OUTPUT_DIR)\n", "\n", "            if basic_path:\n", "\n", "                print(\"\u2705 \u57fa\u7840\u5206\u6790\u56fe\u521b\u5efa\u6210\u529f\")\n", "\n", "    except Exception as e2:\n", "\n", "        print(f\"   \u57fa\u7840\u5206\u6790\u4e5f\u5931\u8d25: {e2}\")\n", "\n", "\n", "\n", "finally:\n", "\n", "    print(f\"\\n\ud83d\udcc1 \u6240\u6709\u8f93\u51fa\u6587\u4ef6\u4fdd\u5b58\u5728: {OUTPUT_DIR}\")\n", "\n", "    print(\"\ud83c\udfaf \u4f18\u5316\u5206\u6790\u5b8c\u6210\uff01\")\n", "\n", "\n", "\n", "# ==================== \u7b80\u5316\u7684\u5236\u56fe\u9884\u89c8\u51fd\u6570 ====================\n", "\n", "\n", "\n", "def preview_salinity_distribution(data, output_dir):\n", "\n", "    \"\"\"\n", "\n", "    \u9884\u89c8\u76d0\u6e0d\u5316\u7a7a\u95f4\u5206\u5e03\uff08\u7b80\u5316\u7248\u5236\u56fe\uff09\n", "\n", "    \"\"\"\n", "\n", "    if 'longitude' not in data.columns or 'latitude' not in data.columns:\n", "\n", "        print(\"\u26a0\ufe0f \u7f3a\u5c11\u7a7a\u95f4\u5750\u6807\uff0c\u65e0\u6cd5\u521b\u5efa\u7a7a\u95f4\u5206\u5e03\u56fe\")\n", "\n", "        return None\n", "\n", "    \n", "\n", "    print(\"\\n\ud83d\uddfa\ufe0f \u521b\u5efa\u76d0\u6e0d\u5316\u7a7a\u95f4\u5206\u5e03\u9884\u89c8...\")\n", "\n", "    \n", "\n", "    try:\n", "\n", "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n", "\n", "        \n", "\n", "        # 1. \u539f\u59cb\u76d0\u5ea6\u5206\u5e03\n", "\n", "        scatter1 = axes[0].scatter(data['longitude'], data['latitude'], \n", "\n", "                                 c=data['salinity'], s=60, cmap='RdYlBu_r',\n", "\n", "                                 edgecolors='black', linewidth=0.5, alpha=0.8)\n", "\n", "        axes[0].set_xlabel('\u7ecf\u5ea6', fontsize=12)\n", "\n", "        axes[0].set_ylabel('\u7eac\u5ea6', fontsize=12)\n", "\n", "        axes[0].set_title('\u5b9e\u6d4b\u76d0\u5ea6\u7a7a\u95f4\u5206\u5e03', fontsize=14, fontweight='bold')\n", "\n", "        cbar1 = plt.colorbar(scatter1, ax=axes[0], shrink=0.8)\n", "\n", "        cbar1.set_label('\u76d0\u5ea6\u503c', fontsize=11)\n", "\n", "        axes[0].grid(True, alpha=0.3)\n", "\n", "        \n", "\n", "        # 2. \u5206\u7ea7\u76d0\u6e0d\u5316\u5206\u5e03\n", "\n", "        def classify_salinity_simple(x):\n", "\n", "            if x <= 2: return 1  # \u975e\u76d0\u6e0d\u5316\n", "\n", "            elif x <= 4: return 2  # \u8f7b\u5ea6\n", "\n", "            elif x <= 8: return 3  # \u4e2d\u5ea6  \n", "\n", "            elif x <= 15: return 4  # \u91cd\u5ea6\n", "\n", "            else: return 5  # \u6781\u91cd\u5ea6\n", "\n", "        \n", "\n", "        classes = data['salinity'].apply(classify_salinity_simple)\n", "\n", "        class_colors = ['#2d5a27', '#78c679', '#c2e699', '#fd8d3c', '#bd0026']\n", "\n", "        class_names = ['\u975e\u76d0\u6e0d\u5316', '\u8f7b\u5ea6', '\u4e2d\u5ea6', '\u91cd\u5ea6', '\u6781\u91cd\u5ea6']\n", "\n", "        \n", "\n", "        scatter2 = axes[1].scatter(data['longitude'], data['latitude'], \n", "\n", "                                 c=classes, s=60, cmap='RdYlGn_r',\n", "\n", "                                 edgecolors='black', linewidth=0.5, alpha=0.8)\n", "\n", "        axes[1].set_xlabel('\u7ecf\u5ea6', fontsize=12)\n", "\n", "        axes[1].set_ylabel('\u7eac\u5ea6', fontsize=12)\n", "\n", "        axes[1].set_title('\u76d0\u6e0d\u5316\u7b49\u7ea7\u5206\u5e03', fontsize=14, fontweight='bold')\n", "\n", "        \n", "\n", "        # \u6dfb\u52a0\u56fe\u4f8b\n", "\n", "        import matplotlib.patches as mpatches\n", "\n", "        legend_elements = [mpatches.Patch(color=class_colors[i], label=class_names[i]) \n", "\n", "                          for i in range(len(class_names))]\n", "\n", "        axes[1].legend(handles=legend_elements, loc='upper right', fontsize=10)\n", "\n", "        axes[1].grid(True, alpha=0.3)\n", "\n", "        \n", "\n", "        # 3. \u7edf\u8ba1\u6c47\u603b\n", "\n", "        class_stats = pd.Series(classes).value_counts().sort_index()\n", "\n", "        total_samples = len(classes)\n", "\n", "        \n", "\n", "        # \u997c\u56fe\n", "\n", "        valid_classes = class_stats[class_stats > 0]\n", "\n", "        colors_subset = [class_colors[i-1] for i in valid_classes.index]\n", "\n", "        labels_subset = [class_names[i-1] for i in valid_classes.index]\n", "\n", "        \n", "\n", "        wedges, texts, autotexts = axes[2].pie(valid_classes.values, \n", "\n", "                                              labels=labels_subset,\n", "\n", "                                              colors=colors_subset,\n", "\n", "                                              autopct='%1.1f%%',\n", "\n", "                                              startangle=90)\n", "\n", "        axes[2].set_title('\u76d0\u6e0d\u5316\u7b49\u7ea7\u5360\u6bd4', fontsize=14, fontweight='bold')\n", "\n", "        \n", "\n", "        # \u7f8e\u5316\u6587\u5b57\n", "\n", "        for autotext in autotexts:\n", "\n", "            autotext.set_color('white')\n", "\n", "            autotext.set_fontweight('bold')\n", "\n", "        \n", "\n", "        plt.tight_layout()\n", "\n", "        \n", "\n", "        # \u4fdd\u5b58\u56fe\u7247\n", "\n", "        preview_path = os.path.join(output_dir, 'salinity_distribution_preview.png')\n", "\n", "        plt.savefig(preview_path, dpi=300, bbox_inches='tight', facecolor='white')\n", "\n", "        plt.show()\n", "\n", "        \n", "\n", "        # \u6253\u5370\u7edf\u8ba1\u4fe1\u606f\n", "\n", "        print(\"\\n\ud83d\udcca \u76d0\u6e0d\u5316\u5206\u5e03\u7edf\u8ba1:\")\n", "\n", "        print(\"-\" * 40)\n", "\n", "        for i, count in class_stats.items():\n", "\n", "            if count > 0:\n", "\n", "                percentage = count / total_samples * 100\n", "\n", "                print(f\"{class_names[i-1]:>8s}: {count:>3d} \u4e2a\u70b9 ({percentage:>5.1f}%)\")\n", "\n", "        \n", "\n", "        # \u8ba1\u7b97\u76d0\u6e0d\u5316\u7387\n", "\n", "        salinized_count = class_stats.iloc[1:].sum()  # \u6392\u9664\u975e\u76d0\u6e0d\u5316\n", "\n", "        salinization_rate = salinized_count / total_samples * 100\n", "\n", "        \n", "\n", "        print(f\"\\n\ud83c\udfaf \u76d0\u6e0d\u5316\u8bc4\u4f30:\")\n", "\n", "        print(f\"   \u603b\u6837\u672c\u6570: {total_samples}\")\n", "\n", "        print(f\"   \u76d0\u6e0d\u5316\u6837\u672c: {salinized_count}\")\n", "\n", "        print(f\"   \u76d0\u6e0d\u5316\u7387: {salinization_rate:.1f}%\")\n", "\n", "        \n", "\n", "        if salinization_rate > 60:\n", "\n", "            print(\"   \u26a0\ufe0f \u9ad8\u76d0\u6e0d\u5316\u533a\u57df - \u9700\u8981\u91cd\u70b9\u5173\u6ce8\")\n", "\n", "        elif salinization_rate > 30:\n", "\n", "            print(\"   \u26a0\ufe0f \u4e2d\u7b49\u76d0\u6e0d\u5316\u533a\u57df - \u9700\u8981\u76d1\u63a7\")\n", "\n", "        else:\n", "\n", "            print(\"   \u2705 \u4f4e\u76d0\u6e0d\u5316\u533a\u57df - \u76f8\u5bf9\u5065\u5eb7\")\n", "\n", "        \n", "\n", "        print(f\"\\n\u2705 \u7a7a\u95f4\u5206\u5e03\u9884\u89c8\u56fe\u4fdd\u5b58\u81f3: {preview_path}\")\n", "\n", "        \n", "\n", "        return preview_path\n", "\n", "        \n", "\n", "    except Exception as e:\n", "\n", "        print(f\"\u274c \u7a7a\u95f4\u5206\u5e03\u9884\u89c8\u5931\u8d25: {e}\")\n", "\n", "        return None\n", "\n", "\n", "\n", "# \u8c03\u7528\u9884\u89c8\u51fd\u6570\n", "\n", "if 'final_data' in locals() or 'final_data' in globals():\n", "\n", "    print(\"\\n\" + \"=\"*60)\n", "\n", "    print(\"\u9644\u52a0\uff1a\u76d0\u6e0d\u5316\u7a7a\u95f4\u5206\u5e03\u9884\u89c8\")\n", "\n", "    print(\"=\"*60)\n", "\n", "    \n", "\n", "    preview_path = preview_salinity_distribution(final_data, OUTPUT_DIR)\n", "\n", "    \n", "\n", "    if preview_path:\n", "\n", "        print(\"\\n\ud83c\udf89 \u7a7a\u95f4\u5206\u5e03\u9884\u89c8\u5b8c\u6210\uff01\")\n", "\n", "        print(\"\u8fd9\u4e3a\u540e\u7eed\u7684\u5b8c\u6574\u5236\u56fe\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\")\n", "\n", "else:\n", "\n", "    print(\"\\n\u26a0\ufe0f \u65e0\u6cd5\u521b\u5efa\u7a7a\u95f4\u5206\u5e03\u9884\u89c8\uff08\u7f3a\u5c11final_data\uff09\")\n", "\n", "\n", "\n", "print(\"\\n\ud83d\ude80 Cell 4 \u4f18\u5316\u5206\u6790\u6d41\u7a0b\u5168\u90e8\u5b8c\u6210\uff01\")\n", "\n", "print(\"=\"*80)\n"], "outputs": [], "execution_count": null}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}
